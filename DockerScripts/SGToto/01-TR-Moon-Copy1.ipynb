{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cores:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n",
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  return _inspect.getargspec(target)\n",
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:4900: ResourceWarning: unclosed file <_io.TextIOWrapper name='/Users/uqapp/.keras/keras.json' mode='r' encoding='UTF-8'>\n",
      "  _config = json.load(open(_config_path))\n",
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#!conda install -n mldds -c anaconda joblib\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "import multiprocessing\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "print(\"Cores: \", num_cores)\n",
    "\n",
    "import time\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto( device_count = {'GPU': 0 , 'CPU': num_cores} )\n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "#os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from MyTotoResearchv4 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getAllData(df):\n",
    "    drop_cols = ['T', 'D', 'N1','N2','N3','N4','N5','N6','N7','L','M','S','R','E','A','V' ,'J','U']\n",
    "    X = df.drop(drop_cols, axis=1)\n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_prediction(mrt, model, f, scaler=None, name='unnamed'):\n",
    "    def getAllData(df):\n",
    "        drop_cols = ['T', 'L','M','S','R','E','A','V' ,'J','U']\n",
    "        X = df.drop(drop_cols, axis=1)\n",
    "#        print(df.head())\n",
    "        use_cols = ['Ph','il','age','dist','adia','sundist','sunadia']\n",
    "        X = df[use_cols]\n",
    "        return X\n",
    "\n",
    "    test_data = mtr.get_test_data()\n",
    "    X = mtr.modified_dataset(getAllData(test_data)) #\n",
    "#    X = getAdjustedDataF(test_data,f)\n",
    "\n",
    "\n",
    "    if ( scaler == None ):\n",
    "        Z = X\n",
    "    else:\n",
    "        scaler.fit(X)\n",
    "        Z = scaler.transform(X)\n",
    "\n",
    "    predictions = model.predict(Z)\n",
    "\n",
    "    dfResult= pd.DataFrame(predictions, columns=['N1', 'N2', 'N3', 'N4', 'N5','N6', 'N7'])\n",
    "#    mtr.print_predictions(dfResult)\n",
    "\n",
    "    global df_predictions\n",
    "    global prev_r\n",
    "    r = mtr.getAccuracyCount(np.array(dfResult)) ;\n",
    "#    if ( r > prev_r ):\n",
    "#        df_predictions = []\n",
    "    df_predictions.append(dfResult)\n",
    "    g_all_pred.update({name : dfResult})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded MyTotoResearch algo_no:  1\n",
      "1521\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from keras.models import Input, Model\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, History\n",
    "import json as simplejson\n",
    "from keras import regularizers\n",
    "from sklearn import preprocessing\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, RandomForestClassifier, ExtraTreesRegressor, ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVR, NuSVC\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor, SGDClassifier, LogisticRegression, PassiveAggressiveClassifier, Perceptron, RidgeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression, Lasso, ElasticNet, Ridge, RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, SVR, LinearSVC\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "seed = 42\n",
    "\n",
    "mtr = MyTotoResearch(algo_no=1)\n",
    "lresult, df = mtr.load_totodata()\n",
    "\n",
    "df_predictions = []\n",
    "\n",
    "\n",
    "all_models = []\n",
    "\n",
    "#all_models.append(('SVCpoly01', SVC(kernel='poly', coef0=0.05, probability=True, degree=2, random_state=seed)))\n",
    "#all_models.append(('SVCrbf010', SVC(kernel='rbf', coef0=0.75, probability=True, degree=2, random_state=seed)))\n",
    "# all_models.append(('SVCrbf011', SVC(kernel='rbf', coef0=0.5, probability=True, degree=2, random_state=seed)))\n",
    "# all_models.append(('SVCrbf012', SVC(kernel='rbf', coef0=0.25, probability=True, degree=2, random_state=seed)))\n",
    "\n",
    "# all_models.append(('SVCrbf0103', SVC(kernel='rbf', coef0=0.75, probability=True, degree=3, random_state=seed)))\n",
    "# all_models.append(('SVCrbf0113', SVC(kernel='rbf', coef0=0.5, probability=True, degree=3, random_state=seed)))\n",
    "# all_models.append(('SVCrbf0123', SVC(kernel='rbf', coef0=0.25, probability=True, degree=3, random_state=seed)))\n",
    "\n",
    "\n",
    "#all_models.append(('SVCrbf020', SVC(kernel='sigmoid', coef0=0.75, probability=True, degree=2, random_state=seed)))\n",
    "# all_models.append(('SVCrbf021', SVC(kernel='sigmoid', coef0=0.5, probability=True, degree=2, random_state=seed)))\n",
    "# all_models.append(('SVCrbf022', SVC(kernel='sigmoid', coef0=0.25, probability=True, degree=2, random_state=seed)))\n",
    "\n",
    "# all_models.append(('SVCrbf0203', SVC(kernel='sigmoid', coef0=0.75, probability=True, degree=3, random_state=seed)))\n",
    "# all_models.append(('SVCrbf0213', SVC(kernel='sigmoid', coef0=0.5, probability=True, degree=3, random_state=seed)))\n",
    "# all_models.append(('SVCrbf0223', SVC(kernel='sigmoid', coef0=0.25, probability=True, degree=3, random_state=seed)))\n",
    "\n",
    "\n",
    "# all_models.append(('SVCrbf030', SVC(kernel='linear', coef0=0.75, probability=True, degree=2, random_state=seed)))\n",
    "# all_models.append(('SVCrbf031', SVC(kernel='linear', coef0=0.5, probability=True, degree=2, random_state=seed)))\n",
    "# all_models.append(('SVCrbf032', SVC(kernel='linear', coef0=0.25, probability=True, degree=2, random_state=seed)))\n",
    "\n",
    "# all_models.append(('SVCrbf0303', SVC(kernel='linear', coef0=0.75, probability=True, degree=3, random_state=seed)))\n",
    "# all_models.append(('SVCrbf0313', SVC(kernel='linear', coef0=0.5, probability=True, degree=3, random_state=seed)))\n",
    "# all_models.append(('SVCrbf0323', SVC(kernel='linear', coef0=0.25, probability=True, degree=3, random_state=seed)))\n",
    "\n",
    "\n",
    "\n",
    "# all_models.append(('LR', (LogisticRegression(random_state=seed))))\n",
    "\n",
    "#all_models.append(('KNNC', KNeighborsClassifier()))\n",
    "#all_models.append(('KNNR', KNeighborsRegressor()))\n",
    "#all_models.append(('RC', RidgeClassifier(random_state=seed)))\n",
    "# all_models.append(('LR', LogisticRegression(random_state=seed)))\n",
    "# all_models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "# all_models.append(('DTR', DecisionTreeRegressor()))\n",
    "# all_models.append(('ETR', ExtraTreesRegressor(n_estimators=5)))\n",
    "#all_models.append(('ETC', ExtraTreesClassifier(n_estimators=5)))\n",
    "# all_models.append(('EN', ElasticNet()))\n",
    "#all_models.append(('CART', DecisionTreeClassifier()))\n",
    "# all_models.append(('NB', GaussianNB()))\n",
    "# all_models.append(('Lasso', Lasso()))\n",
    "all_models.append(('GBR', GradientBoostingRegressor()))\n",
    "#all_models.append(('RFR5', RandomForestClassifier(n_estimators=5, n_jobs=5, random_state=seed)))\n",
    "# all_models.append(('RFR5', RandomForestClassifier(n_estimators=5, n_jobs=5, random_state=seed)))\n",
    "# all_models.append(('RFR3', RandomForestRegressor(n_estimators=3, n_jobs=5, random_state=seed)))\n",
    "# all_models.append(('SGDR', SGDRegressor(random_state=seed)))\n",
    "#all_models.append(('AdaB', AdaBoostClassifier(RandomForestClassifier(n_estimators=3))))\n",
    "#all_models.append(('MLPC', MLPClassifier(hidden_layer_sizes=(500,500,500), max_iter=2000, alpha=0.001, activation='tanh', learning_rate='adaptive', solver='sgd', verbose=0,  random_state=42,tol=0.000000001)))\n",
    "\n",
    "#92.6 accuracy\n",
    "#all_models.append(('MLPC', MLPClassifier(hidden_layer_sizes=(490,490,490,490,490,490,490), max_iter=500000, alpha=0.001, activation='relu', learning_rate='adaptive', solver='adam', verbose=10,  random_state=42,tol=0.000000001)))\n",
    "\n",
    "\n",
    "#92.45 accuracy\n",
    "#all_models.append(('MLPC', MLPClassifier(hidden_layer_sizes=(420,420,420,420,420,420,420), max_iter=500000, alpha=0.001, activation='relu', learning_rate='adaptive', solver='adam', verbose=10,  random_state=42,tol=0.000000001)))\n",
    "\n",
    "#96.22 accuracy\n",
    "#all_models.append(('MLPC', MLPClassifier(hidden_layer_sizes=(420,420,420,420,420,420,420), max_iter=500000, alpha=0.0001, activation='relu', learning_rate='adaptive', solver='sgd', verbose=2,  random_state=42,tol=0.000000001)))\n",
    "\n",
    "#94.xx\n",
    "#all_models.append(('MLPC', MLPClassifier(hidden_layer_sizes=(420,420,420,420,420,420,420), max_iter=500000, alpha=0.0001, activation='tanh', learning_rate='adaptive', solver='sgd', verbose=2,  random_state=42,tol=0.000000001)))\n",
    "\n",
    "#94.33\n",
    "#all_models.append(('MLPC', MLPClassifier(hidden_layer_sizes=(512,256,128,64,32,16), max_iter=30000, alpha=0.001, activation='relu', learning_rate='adaptive', solver='adam', verbose=2,  random_state=42,tol=0.000000001)))\n",
    "\n",
    "#92\n",
    "#all_models.append(('MLPC', MLPClassifier(hidden_layer_sizes=(512,512,256,128,64,32,16), max_iter=30000, alpha=0.01, activation='relu', learning_rate='adaptive', solver='adam', verbose=2,  random_state=42,tol=0.000000001)))\n",
    "\n",
    "#96.22\n",
    "#all_models.append(('MLPC', MLPClassifier(hidden_layer_sizes=(512,512,512,512,512,256,128,64,32,16), max_iter=30000, alpha=0.01, activation='relu', learning_rate='adaptive', solver='adam', verbose=2,  random_state=42,tol=0.000000001)))\n",
    "all_models.append(('MLPC', MLPClassifier(hidden_layer_sizes=(512,512,256,256,256,256,256,128,64,32,16), max_iter=30000, alpha=0.0001, activation='relu', learning_rate='adaptive', solver='adam', verbose=2,  random_state=42,tol=0.000000001)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove zeros from set...\n",
      "Remove zeros from set...\n",
      "[[1, 2], [0, 0], [2, 6, 7, 0]]\n",
      "[2 4 7]\n",
      "Idx to delete  [0, 2]\n",
      "[array([1, 2]), array([], dtype=int64), array([2, 6, 7])]\n",
      "[2 4 7]\n",
      "[True, False, True]\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "y_true = [[1,2],[0,0],[2,6,7,0]]\n",
    "y_pred = np.array([2,4,7])\n",
    "\n",
    "\n",
    "#print(y_pred[y_pred != 0])\n",
    "y_true_excluding_zeros = [np.array(v)[np.array(v)!=0] for v in y_true]\n",
    "\n",
    "y_idx = []\n",
    "for idx, v in enumerate(y_true):\n",
    "    if v.__contains__(2):\n",
    "        print(\"Remove zeros from set...\")\n",
    "        y_idx.append(idx)\n",
    "\n",
    "print(y_true)\n",
    "print(y_pred)\n",
    "print('Idx to delete ', y_idx)\n",
    "y_true = y_true_excluding_zeros\n",
    "\n",
    "#y_true = np.delete(y_true,y_idx,axis=0)\n",
    "#y_pred = np.delete(y_pred,y_idx)\n",
    "print(y_true)\n",
    "print(y_pred)\n",
    "matched_index = [t.__contains__(p) for (t,p) in zip(y_true, y_pred)]\n",
    "print(matched_index)\n",
    "print(sum(matched_index)/len(matched_index))\n",
    "\n",
    "\n",
    "#matched_values = [reduce(np.intersect1d, (p, a)) for (p,a) in zip(y_true, y_pred)]\n",
    "#print(matched_values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.scorer import make_scorer\n",
    "def multi_targets_scorer_function(y_true, y_pred, sample_weight):    \n",
    "    y_true = mtr.getTargets()\n",
    "    y_true_excluding_zeros = [np.array(v)[np.array(v)!=0] for v in y_true]\n",
    "    matched_index = [t.__contains__(p) for (t,p) in zip(y_true_excluding_zeros, y_pred)]\n",
    "    print(sum(matched_index)/len(y_true_excluding_zeros), ' ', sum(sample_weight))\n",
    "    return sum(matched_index)/len(y_true_excluding_zeros)\n",
    "\n",
    "#score_params = {\"sample_weight\": sample_weight_frame}\n",
    "multi_targets_scorer = make_scorer(multi_targets_scorer_function, greater_is_better=True)\n",
    "#, **score_params)\n",
    "\n",
    "i = 0 ;\n",
    "iBest = 0 ;\n",
    "best_model = None\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "def scorer(estimator, X, y, sample_weight=None):\n",
    "    y_true = Y_TRUTH #mtr.getTargets()\n",
    "    y_pred = estimator.predict(X); \n",
    "    global i\n",
    "    if ( i < 3):\n",
    "        print(estimator.get_params())\n",
    "    i += 1\n",
    "    y_true_excluding_zeros = [np.array(v)[np.array(v)!=0] for v in y_true]\n",
    "    matched_index = [t.__contains__(p) for (t,p) in zip(y_true_excluding_zeros, y_pred)]\n",
    "    if ( sample_weight ):\n",
    "            print(sum(matched_index)/len(matched_index), ' ', sum(sample_weight))\n",
    "    iAccuracy = 100*sum(matched_index)/len(y_true_excluding_zeros)\n",
    "    global iBest, best_model\n",
    "    if(iBest < iAccuracy):\n",
    "        iBest = iAccuracy\n",
    "        best_model = deepcopy(estimator)\n",
    "        print('Imporved: ', iBest)\n",
    "    return iAccuracy\n",
    "\n",
    "#     extra = np.array([d['extra'] for d in X])\n",
    "#     return -((estimator.predict(X) - y)**2 * extra).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC, C = 1, gamma = 10\n",
    "\n",
    "\n",
    "def get_best_model ( estimator ):\n",
    "    global best_model, iBest\n",
    "    global mtr\n",
    "    iBest = 0 ;\n",
    "    best_model = None\n",
    "    for i in range(2,7):\n",
    "        print('Processing N = ', i)\n",
    "        estimator.fit(Z, mtr.getTarget(i))\n",
    "    if ( type(best_model) == type(None)):\n",
    "        return estimator\n",
    "    print(best_model)\n",
    "    return deepcopy(best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded MyTotoResearch algo_no:  1\n",
      "1521\n",
      "Processing N =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing N =  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing N =  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing N =  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing N =  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing N =  2\n",
      "Processing N =  3\n",
      "Processing N =  4\n",
      "Processing N =  5\n",
      "Processing N =  6\n",
      "Processing N =  2\n",
      "{'alpha': 0.9, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'ls', 'max_depth': 4, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 3, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'presort': 'auto', 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "{'alpha': 0.9, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'ls', 'max_depth': 4, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 3, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'presort': 'auto', 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "{'alpha': 0.9, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'ls', 'max_depth': 4, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 3, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'presort': 'auto', 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "Processing N =  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing N =  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing N =  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing N =  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.svm import SVR, NuSVC\n",
    "\n",
    "mtr = MyTotoResearch(algo_no=1)\n",
    "lresult, df = mtr.load_totodata()\n",
    "\n",
    "X = mtr.modified_dataset(getAllData(df)) #\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "Z = scaler.transform(X)\n",
    "#print(df.describe())\n",
    "all_models = {}\n",
    "\n",
    "Y_TRUTH = mtr.getTargets()\n",
    "\n",
    "models = []\n",
    "\n",
    "models.append(get_best_model (GridSearchCV(LinearSVC(), \n",
    "                    param_grid=dict(tol=[1e-0,1e-1,1e-2,1e-3,1e-4]), n_jobs=6, scoring = scorer)))\n",
    "\n",
    "#models.append(get_best_model (GridSearchCV(MultinomialNB(), param_grid=dict(alpha=np.linspace(0,2,20)[1:]), n_jobs=6, scoring = scorer)))\n",
    "\n",
    "\n",
    "# k_range = list(range(1, 49))\n",
    "# knn_param_grid = dict(n_neighbors=k_range)\n",
    "# models.append(get_best_model ( GridSearchCV(KNeighborsClassifier(),\n",
    "#                        cv=None,  param_grid=knn_param_grid, n_jobs=6, scoring = scorer)))\n",
    "\n",
    "\n",
    "models.append(get_best_model(RidgeClassifier()))\n",
    "gbr_param_grid={'n_estimators':[100], \n",
    "            'learning_rate': [0.1, 0.05, 0.02, 0.01], \n",
    "            'max_depth':[4,6], \n",
    "            'min_samples_leaf':[3,5,9,17] } \n",
    "            \n",
    "models.append(get_best_model(GridSearchCV(GradientBoostingRegressor(),\n",
    "                       cv=None,  param_grid=gbr_param_grid, scoring = scorer)))\n",
    "\n",
    "\n",
    "# models.append(get_best_model ( GridSearchCV(SVC(kernel='rbf', gamma=0.1),\n",
    "#                     scoring=scorer, n_jobs=6, cv=None,  param_grid={\"C\": [1e0, 1e1, 1e2, 1e3],\n",
    "#                                \"gamma\": np.logspace(-2, 2, 5)}) ))\n",
    "\n",
    "# ada_boost_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "#           \"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "#           \"n_estimators\": [1, 2]\n",
    "#          }\n",
    "\n",
    "# DTC = DecisionTreeClassifier(random_state = 42, max_features = \"auto\", class_weight = \"balanced\",max_depth = None)\n",
    "# ABC = AdaBoostClassifier(base_estimator = DTC)  \n",
    "# models.append(get_best_model ( GridSearchCV(ABC,\n",
    "#                        cv=None,  param_grid=ada_boost_param_grid, scoring = scorer)))\n",
    "\n",
    "\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
      "       estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0),\n",
      "       fit_params=None, iid='warn', n_jobs=6,\n",
      "       param_grid={'tol': [1.0, 0.1, 0.01, 0.001, 0.0001]},\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
      "       scoring=<function scorer at 0x1a37ee52f0>, verbose=0)\n",
      "Imporved:  14.464168310322156\n",
      " Result  Accuracy:  14.464168310322156\n",
      "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "        max_iter=None, normalize=False, random_state=None, solver='auto',\n",
      "        tol=0.001)\n",
      "Imporved:  16.23931623931624\n",
      " Result  Accuracy:  16.23931623931624\n",
      "GridSearchCV(cv=None, error_score='raise-deprecating',\n",
      "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=1,\n",
      "             min_sampl...=None, subsample=1.0, tol=0.0001,\n",
      "             validation_fraction=0.1, verbose=0, warm_start=False),\n",
      "       fit_params=None, iid='warn', n_jobs=None,\n",
      "       param_grid={'n_estimators': [100], 'learning_rate': [0.1, 0.05, 0.02, 0.01], 'max_depth': [4, 6], 'min_samples_leaf': [3, 5, 9, 17]},\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
      "       scoring=<function scorer at 0x1a37ee52f0>, verbose=0)\n",
      " Result  Accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "# for i in range (1, 8):\n",
    "#     model = models[i-1]\n",
    "#     y_pred = model.predict(Z)\n",
    "#     print(\" Result N\", i, \"  Accuracy: \", model.score(Z, mtr.getTarget(i)))\n",
    "for model in models:\n",
    "    y_pred = model.predict(Z)\n",
    "    print(model)\n",
    "    print(\" Result  Accuracy: \", scorer(model, Z, mtr.getTarget(1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded MyTotoResearch algo_no:  1\n",
      "1521\n",
      "Accuracy:  37.735849056603776\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20180514   [17 24 29 45 46 49  5]   [38 33]   []\n",
      "20180517   [ 7 21 25 29 35 37 13]   [37 35]   [35 37]\n",
      "20180521   [ 8 10 16 30 37 44 17]   [37 35]   [37]\n",
      "20180524   [11 25 26 34 36 42 16]   [39 34]   [34]\n",
      "20180528   [ 5  9 27 28 30 44  2]   [39 35]   []\n",
      "20180531   [11 13 24 26 47 49 33]   [39 37]   []\n",
      "20180604   [20 22 31 37 43 45 27]   [41 33]   []\n",
      "20180607   [12 20 29 31 37 39 42]   [41 34]   []\n",
      "20180611   [16 25 30 37 44 49 34]   [38 34]   [34]\n",
      "20180614   [ 4 29 31 35 42 48  1]   [37 34]   []\n",
      "20180618   [11 15 22 23 26 43 25]   [37 34]   []\n",
      "20180621   [ 4  6 15 24 30 35 46]   [41 35]   [35]\n",
      "20180625   [ 2  5 25 38 44 48  9]   [39 36]   []\n",
      "20180628   [ 2  7 22 27 40 47 48]   [39 39]   []\n",
      "20180702   [12 13 26 33 35 38 23]   [41 36]   []\n",
      "20180705   [ 8 11 28 30 32 34 39]   [41 35]   []\n",
      "20180709   [ 6 23 31 38 39 43 33]   [41 35]   []\n",
      "20180712   [ 4 15 25 32 40 41 10]   [37 39]   []\n",
      "20180716   [ 4  8 19 24 32 47 22]   [37 33]   []\n",
      "20180719   [13 14 23 35 37 46 45]   [41 35]   [35]\n",
      "20180723   [ 2 23 26 28 39 40 12]   [39 36]   [39]\n",
      "20180726   [ 1  9 13 17 28 40 37]   [39 35]   []\n",
      "20180730   [ 8 10 19 20 41 43  7]   [39 34]   []\n",
      "20180802   [ 1 10 15 27 41 46 35]   [41 36]   [41]\n",
      "20180806   [ 7 18 20 27 36 40 15]   [41 34]   []\n",
      "20180809   [13 16 20 23 39 42 28]   [37 34]   []\n",
      "20180813   [ 1  3  6 16 22 36 17]   [37 35]   []\n",
      "20180816   [22 23 25 32 33 36 20]   [37 35]   []\n",
      "20180820   [ 9 10 25 38 40 42  2]   [41 35]   []\n",
      "20180823   [ 2  3 23 30 39 41 19]   [39 37]   [39]\n",
      "20180827   [ 5  6 16 24 26 29 38]   [39 36]   []\n",
      "20180830   [ 3  9 27 29 31 40 46]   [39 35]   []\n",
      "20180903   [ 4  5 13 18 39 40  3]   [41 34]   []\n",
      "20180906   [ 2 15 17 20 23 30 45]   [37 32]   []\n",
      "20180910   [ 2  6  9 15 40 43 18]   [37 34]   []\n",
      "20180913   [ 6 16 17 40 44 48 34]   [37 39]   []\n",
      "20180917   [16 21 22 24 25 27  1]   [41 33]   []\n",
      "20180920   [ 5 12 18 30 32 38 22]   [41 35]   []\n",
      "20180924   [ 6  8 17 24 29 47 34]   [39 36]   []\n",
      "20180927   [ 2 25 29 33 42 45 20]   [39 34]   []\n",
      "20181001   [11 15 23 24 32 40 43]   [40 34]   [40]\n",
      "20181004   [ 5 12 23 32 37 42 43]   [40 37]   [37]\n",
      "20181008   [17 18 23 39 43 49  2]   [38 34]   []\n",
      "20181011   [ 1 16 18 24 29 46 35]   [37 32]   []\n",
      "20181015   [ 1  4 24 32 35 48 20]   [41 34]   []\n",
      "20181018   [ 5 14 17 31 46 48 47]   [41 37]   []\n",
      "20181022   [ 5 22 24 40 43 48  2]   [39 36]   []\n",
      "20181025   [ 7  8 13 15 35 48 30]   [39 35]   [35]\n",
      "20181029   [ 2  6 10 20 28 31 30]   [40 35]   []\n",
      "20181101   [ 6 27 28 41 44 48 15]   [40 34]   []\n",
      "20181105   [ 3  8 14 28 43 49 26]   [38 35]   []\n",
      "20181108   [ 8 13 16 26 28 38 46]   [36 34]   []\n",
      "20181112   [ 4 12 21 34 41 47 33]   [41 35]   [41]\n",
      "20181115  Predicted:  [41 34]  \n",
      "20181119  Predicted:  [36 36]  \n",
      "20181122  Predicted:  [42 35]  \n",
      "20181126  Predicted:  [42 35]  \n",
      "20181129  Predicted:  [40 36]  \n",
      "20181203  Predicted:  [40 35]  \n",
      "20181206  Predicted:  [38 35]  \n",
      "20181210  Predicted:  [36 31]  \n",
      "20181213  Predicted:  [36 36]  \n",
      "20181217  Predicted:  [36 36]  \n",
      "20181220  Predicted:  [42 35]  \n",
      "20181224  Predicted:  [42 36]  \n",
      "20181227  Predicted:  [42 33]  \n",
      "20181231  Predicted:  [40 36]  \n",
      "    P1  P2  P3\n",
      "0   39  38  33\n",
      "1   39  37  35\n",
      "2   39  37  35\n",
      "3   39  39  34\n",
      "4   34  39  35\n",
      "5   34  39  37\n",
      "6   34  41  33\n",
      "7   39  41  34\n",
      "8   39  38  34\n",
      "9   39  37  34\n",
      "10  39  37  34\n",
      "11  39  41  35\n",
      "12  34  39  36\n",
      "13  34  39  39\n",
      "14  34  41  36\n",
      "15  39  41  35\n",
      "16  39  41  35\n",
      "17  39  37  39\n",
      "18  39  37  33\n",
      "19  39  41  35\n",
      "20  34  39  36\n",
      "21  34  39  35\n",
      "22  34  39  34\n",
      "23  39  41  36\n",
      "24  39  41  34\n",
      "25  39  37  34\n",
      "26  39  37  35\n",
      "27  39  37  35\n",
      "28  34  41  35\n",
      "29  34  39  37\n",
      "..  ..  ..  ..\n",
      "37  34  41  35\n",
      "38  34  39  36\n",
      "39  34  39  34\n",
      "40  39  40  34\n",
      "41  39  40  37\n",
      "42  39  38  34\n",
      "43  41  37  32\n",
      "44  34  41  34\n",
      "45  34  41  37\n",
      "46  34  39  36\n",
      "47  34  39  35\n",
      "48  34  40  35\n",
      "49  39  40  34\n",
      "50  31  38  35\n",
      "51  34  36  34\n",
      "52  34  41  35\n",
      "53  34  41  34\n",
      "54  34  36  36\n",
      "55  34  42  35\n",
      "56  34  42  35\n",
      "57  34  40  36\n",
      "58  34  40  35\n",
      "59  34  38  35\n",
      "60  34  36  31\n",
      "61  34  36  36\n",
      "62  34  36  36\n",
      "63  34  42  35\n",
      "64  34  42  36\n",
      "65  34  42  33\n",
      "66  34  40  36\n",
      "\n",
      "[67 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "q_models = []\n",
    "\n",
    "\n",
    "mtr = MyTotoResearch(algo_no=1)\n",
    "lresult, df = mtr.load_totodata()\n",
    "\n",
    "X = mtr.modified_dataset(getAllData(df)) #\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "Z = scaler.transform(X)\n",
    "\n",
    "\n",
    "# q_model = GaussianNB()\n",
    "# q_model = GradientBoostingRegressor()\n",
    "\n",
    "# for i in range(1, 8):\n",
    "#     q_model.fit(Z, mtr.getTarget(i))\n",
    "#     q_models.append(deepcopy(q_model))\n",
    "\n",
    "y_true = mtr.getTargets()\n",
    "    \n",
    "def getAllData4Prediction(df):\n",
    "    drop_cols = ['T', 'L','M','S','R','E','A','V' ,'J','U']\n",
    "    X = df.drop(drop_cols, axis=1)\n",
    "    return X\n",
    "\n",
    "df_predicted = mtr.get_test_data()\n",
    "newZ = scaler.transform(getAllData4Prediction(df_predicted))\n",
    "\n",
    "df_top_seven = pd.DataFrame()\n",
    "\n",
    "# for i in range (1, 8):\n",
    "#     model = q_models[i-1]\n",
    "#     s = 'P' + str(i)\n",
    "#     df_top_seven[s] = model.predict(newZ).astype(int)\n",
    "\n",
    "i = 1\n",
    "for model in models:\n",
    "    y_pred = model.predict(Z)\n",
    "    s = 'P' + str(i)\n",
    "    i = i +1\n",
    "    df_top_seven[s] = model.predict(newZ).astype(int)\n",
    "#    print(df_top_seven[s])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#print(df_top_seven)\n",
    "\n",
    "r = mtr.getAccuracyCount(np.array(df_top_seven)) ;\n",
    "print ( \"Accuracy: \",  r)\n",
    "#dict_accuracy.update({s: r})\n",
    "mtr.plot_matched_counts(df_top_seven.values)\n",
    "mtr.print_predictions(df_top_seven)\n",
    "\n",
    "print(df_top_seven)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-400d9849b0e0>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-400d9849b0e0>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    date\tnum1\tnum2\tnum3\tnum4\tnum5\tnum6\tadditional\u001b[0m\n\u001b[0m        \t   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "  date\tnum1\tnum2\tnum3\tnum4\tnum5\tnum6\tadditional\n",
    "Mon, 03 Dec 2018\t5\t6\t7\t29\t37\t38\t23\n",
    "Thu, 29 Nov 2018\t4\t11\t19\t25\t40\t44\t1\n",
    "Mon, 26 Nov 2018\t16\t22\t28\t31\t38\t46\t33\n",
    "Mon, 19 Nov 2018\t2\t9\t14\t36\t46\t48\t5\n",
    "Thu, 15 Nov 2018\t21\t25\t27\t36\t39\t44\t18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = mtr.getTargets()\n",
    "    \n",
    "def getAllData4Prediction(df):\n",
    "    drop_cols = ['T', 'L','M','S','R','E','A','V' ,'J','U']\n",
    "    X = df.drop(drop_cols, axis=1)\n",
    "    return X\n",
    "\n",
    "df_predicted = mtr.get_test_data()\n",
    "newZ = scaler.transform(getAllData4Prediction(df_predicted))\n",
    "\n",
    "df_top_seven = pd.DataFrame()\n",
    "\n",
    "for i in range (1, 8):\n",
    "    model = models[i-1]\n",
    "    s = 'P' + str(i)\n",
    "    df_top_seven[s] = model.predict(newZ)\n",
    "\n",
    "    \n",
    "#print(df_top_seven)\n",
    "\n",
    "r = mtr.getAccuracyCount(np.array(df_top_seven)) ;\n",
    "print ( \"Accuracy: \",  r)\n",
    "#dict_accuracy.update({s: r})\n",
    "mtr.plot_matched_counts(df_top_seven.values)\n",
    "mtr.print_predictions(df_top_seven)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date\tnum1\tnum2\tnum3\tnum4\tnum5\tnum6\tadditional\n",
    "Mon, 03 Dec 2018\t5\t6\t7\t29\t37\t38\t23\n",
    "Thu, 29 Nov 2018\t4\t11\t19\t25\t40\t44\t1\n",
    "Mon, 26 Nov 2018\t16\t22\t28\t31\t38\t46\t33\n",
    "Mon, 19 Nov 2018\t2\t9\t14\t36\t46\t48\t5\n",
    "Thu, 15 Nov 2018\t21\t25\t27\t36\t39\t44\t18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scorer(best_model, Z, mtr.getTarget(1)))\n",
    "print(best_model)\n",
    "\n",
    "#model = SVC(kernel='rbf', gamma=100.0, C=10.0)\n",
    "#model.fit(Z,mtr.getTarget(5))\n",
    "\n",
    "model = best_model\n",
    "\n",
    "y_pred = model.predict(Z)\n",
    "print(scorer(model,Z, mtr.getTarget(5)))\n",
    "#print(model)\n",
    "#print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = mtr.getTargets()\n",
    "iTotalMatched = 0\n",
    "#for (t,p) in zip(y_true, y_pred):\n",
    "matched_index = [t.__contains__(p) for (t,p) in zip(y_true, y_pred)]\n",
    "print(\"Accuracy: \", 100*sum(matched_index)/len(y_true))\n",
    "\n",
    "for (t,p) in zip(y_true, y_pred):\n",
    "    print(t, ' ', p, ' ', t.__contains__(p) )\n",
    "    \n",
    "\n",
    "    \n",
    "def getAllData4Prediction(df):\n",
    "    drop_cols = ['T', 'L','M','S','R','E','A','V' ,'J','U']\n",
    "    X = df.drop(drop_cols, axis=1)\n",
    "    return X\n",
    "\n",
    "df = mtr.get_test_data()\n",
    "newZ = scaler.transform(getAllData4Prediction(df))\n",
    "\n",
    "\n",
    "y_pred = model.predict(newZ)\n",
    "print(y_pred)\n",
    "\n",
    "df['Predicted'] = y_pred\n",
    "\n",
    "df_output = pd.DataFrame()\n",
    "df_output['T'] = df['T']\n",
    "\n",
    "df_output['Predicted'] = y_pred\n",
    "\n",
    "print(df_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('prediction_data.csv')\n",
    "df['Predicted'] = diagnosis_encoder.inverse_transform(y_pred)\n",
    "\n",
    "print()\n",
    "print(df.loc[:,['id', 'diagnosis','Predicted']])\n",
    "\n",
    "\n",
    "\n",
    "print(svr.score(Z, mtr.getTarget(4)))\n",
    "\n",
    "print(svr.best_score_)\n",
    "print(svr.cv_results_)\n",
    "#score(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each model in turn\n",
    "from sklearn import model_selection\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "\n",
    "g_all_pred = {}\n",
    "\n",
    "X = mtr.modified_dataset(getAllData(df)) #\n",
    "f = 1.0 #365/27.58\n",
    "#    X = getAdjustedDataF(df,f)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "#scaler = RobustScaler()\n",
    "scaler.fit(X)\n",
    "Z = scaler.transform(X)\n",
    "\n",
    "for name, model in all_models:\n",
    "    \n",
    "    \n",
    "#    scaler = None\n",
    "#    Z = X\n",
    "\n",
    "#     kfold = model_selection.KFold(n_splits=3, random_state=seed)\n",
    "#     cv_results = model_selection.cross_val_score(model, Z, mtr.getTarget(3), cv=kfold, scoring=scoring)\n",
    "#     results.append(cv_results)\n",
    "#     names.append(name)\n",
    "#     msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "#     print(msg)\n",
    "    \n",
    "    oClassifier = MultiOutputClassifier(model, n_jobs=num_cores-2)\n",
    "    oClassifier.fit(Z, mtr.getTargets()) \n",
    "    print(oClassifier)\n",
    "    s = oClassifier.score(Z, mtr.getTargets())\n",
    "    if(oClassifier.score(Z, mtr.getTargets()) == 1.0):\n",
    "        print( name, ' ', str(f), ' ', str(s))\n",
    "    store_prediction(mtr, oClassifier, f, scaler=scaler, name=name)\n",
    "    start = time.clock()\n",
    "    print(str(f), \" Time taken: \", (time.clock() - start),  \" \")\n",
    "\n",
    "# for n in range(len(df_predictions)):\n",
    "#     print( mtr.getAccuracyCount(np.array(df_predictions[n])))\n",
    "#     mtr.print_predictions(df_predictions[n])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# boxplot algorithm comparison\n",
    "# fig = plt.figure()\n",
    "# fig.suptitle('Algorithm Comparison')\n",
    "# ax = fig.add_subplot(111)\n",
    "# plt.boxplot(results)\n",
    "# ax.set_xticklabels(names)\n",
    "# plt.show()\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from itertools import combinations\n",
    "import operator \n",
    "from itertools import islice\n",
    "\n",
    "name_ = []\n",
    "\n",
    "lst = [name for name, model in all_models]\n",
    "iBestIndex = -1\n",
    "iBestN = []\n",
    "#print(\"List \", lst)\n",
    "top_n = 12\n",
    "\n",
    "\n",
    "dict_accuracy = {}\n",
    "for z in range(5, 0,-1):\n",
    "    a = [list(x) for x in itertools.combinations(lst, z) if len(x) > 1 ] \n",
    "#    print(a)\n",
    "\n",
    "    for xx in a:\n",
    "        test_pred = []\n",
    "        s = ''\n",
    "        combine_prediction(xx, test_pred)\n",
    "#        print(s)\n",
    "\n",
    "        #print(len(test_pred))\n",
    "\n",
    "        all_pred = [] ;\n",
    "        for i in range(len(test_pred)):\n",
    "            if ( i == 0 ):\n",
    "                all_pred = test_pred[i]\n",
    "            else:\n",
    "                all_pred = np.column_stack((all_pred, test_pred[i]) )\n",
    "\n",
    "        top_seven = []\n",
    "        for i in range(len(all_pred)):\n",
    "            unique, counts = np.unique(all_pred[i], return_counts=True)\n",
    "            x = dict(zip(unique, counts))\n",
    "            sorted_x = sorted(x.items(), key=operator.itemgetter(1), reverse=True) # sorted by value\n",
    "            l = list(islice([int(x) for x,y in sorted_x],top_n))\n",
    "            while ( len(l) < top_n ):\n",
    "                l.append(-1)\n",
    "\n",
    "            top_seven.append(l)\n",
    "            \n",
    "\n",
    "#        print(len(top_seven))\n",
    "#         if(len(top_seven[0]) < top_n ):\n",
    "#             print(\"*** Caught \", )\n",
    "        columns = ['N'+str(i+1) for i in range(len(top_seven[0]))]\n",
    "#        print(columns)\n",
    "        df_top_seven = pd.DataFrame(top_seven, columns=columns)\n",
    "        r = mtr.getAccuracyCount(np.array(df_top_seven)) ;\n",
    "        matched, weighted_match = mtr.print_weighted_numbers(df_top_seven.values)\n",
    "        r = sum(weighted_match)\n",
    "\n",
    "        dict_accuracy.update({s: r})\n",
    "\n",
    "t_accuracy = sorted(dict_accuracy.items(),key=operator.itemgetter(1), reverse=True)\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched, weighted_match = mtr.print_weighted_numbers(df_top_seven.values)\n",
    "print(matched)\n",
    "print(weighted_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 7\n",
    "print(t_accuracy[:n])\n",
    "\n",
    "a = [x[0].split('_') for x in t_accuracy[:n] ] \n",
    "print(a)\n",
    "for xx in a:\n",
    "    test_pred = []\n",
    "    s = ''\n",
    "    combine_prediction(xx, test_pred)\n",
    "    all_pred = [] ;\n",
    "    for i in range(len(test_pred)):\n",
    "        if ( i == 0 ):\n",
    "            all_pred = test_pred[i]\n",
    "        else:\n",
    "            all_pred = np.column_stack((all_pred, test_pred[i]) )\n",
    "\n",
    "    top_seven = []\n",
    "    for i in range(len(all_pred)):\n",
    "        unique, counts = np.unique(all_pred[i], return_counts=True)\n",
    "        x = dict(zip(unique, counts))\n",
    "        sorted_x = sorted(x.items(), key=operator.itemgetter(1), reverse=True) # sorted by value\n",
    "        l = list(islice([int(x) for x,y in sorted_x],top_n))\n",
    "        while ( len(l) < top_n ):\n",
    "          l.append(-1)\n",
    "        top_seven.append(l)\n",
    "\n",
    "\n",
    "    columns = ['N'+str(i+1) for i in range(len(top_seven[0]))]\n",
    "    df_top_seven = pd.DataFrame(top_seven, columns=columns)\n",
    "    r = mtr.getAccuracyCount(np.array(df_top_seven)) ;\n",
    "    print ( \"Accuracy: \",  r)\n",
    "    dict_accuracy.update({s: r})\n",
    "    mtr.plot_matched_counts(df_top_seven.values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Nov 26\n",
    "# 16 22 28 31 38 46 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep track of all results\n",
    "#df_predictions = []\n",
    "\n",
    "#print(df_predictions)\n",
    "#mtr = MyTotoResearch(algo_no=1)\n",
    "def getAllData(df):\n",
    "#     drop_cols = ['T', 'L','M','S','R','E','A','V' ,'J','U','K']\n",
    "#     X = df.drop(drop_cols, axis=1)\n",
    "\n",
    "    use_cols = ['Ph','il','age','dist','adia','sundist','sunadia']\n",
    "    X = df[use_cols]\n",
    "    return X\n",
    "\n",
    "lresult, df = mtr.load_totodata()\n",
    "\n",
    "test_data = mtr.get_test_data()\n",
    "X = mtr.modified_dataset(getAllData(test_data)) #\n",
    "\n",
    "print(len(df_predictions))\n",
    "for n in range(len(df_predictions)):\n",
    "    print( mtr.getAccuracyCount(np.array(df_predictions[n])))\n",
    "    mtr.print_predictions(df_predictions[n])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cores:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  return _inspect.getargspec(target)\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:4900: ResourceWarning: unclosed file <_io.TextIOWrapper name='/home/jovyan/.keras/keras.json' mode='r' encoding='UTF-8'>\n",
      "  _config = json.load(open(_config_path))\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#Predict smallest Number\n",
    "\n",
    "#!conda install -n mldds -c anaconda joblib\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "import multiprocessing\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "print(\"Cores: \", num_cores)\n",
    "\n",
    "import time\n",
    "import keras\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "# import tensorflow as tf\n",
    "# config = tf.ConfigProto( device_count = {'GPU': 0 , 'CPU': num_cores} )\n",
    "# sess = tf.Session(config=config) \n",
    "# keras.backend.set_session(sess)\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from MyTotoResearchv4 import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllData(df):\n",
    "    drop_cols = ['T', 'D', 'N1','N2','N3','N4','N5','N6','N7','L','M','S','R','E','A','V' ,'J','U']\n",
    "    drop_cols = ['D', 'N1','N2','N3','N4','N5','N6','N7', 'Ph', 'il', 'age', 'dist', 'adia', 'sundist', 'sunadia' ]\n",
    "\n",
    "#     Ph         1521 non-null float64\n",
    "# il         1521 non-null float64\n",
    "# age        1521 non-null float64\n",
    "# dist       1521 non-null float64\n",
    "# adia       1521 non-null float64\n",
    "# sundist    1521 non-null float64\n",
    "# sunadia    1521 non-null float64\n",
    "\n",
    "#    drop_cols = ['T', 'D', 'M','S','R','E','A','V' ,'J','U']\n",
    "\n",
    "\n",
    "    X = df.drop(drop_cols, axis=1)\n",
    "    \n",
    "    X = df[['T','L','M','S','R','E','A','V']]\n",
    "#    X = df[[ 'T','L','M','S','il', 'age', 'dist', 'adia', 'sundist', 'sunadia']]\n",
    "\n",
    "#     df1 = df[['N1','N2','N3','N4','N5','N6','N7']]\n",
    "#     X['smallest'] = df1.min(axis=1)\n",
    "#     X['biggest'] = df1.max(axis=1)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def getSmallestN(df):\n",
    "    df1 = df[['N1','N2','N3','N4','N5','N6','N7']]\n",
    "    y = pd.DataFrame(index=df.index)\n",
    "    y['SN'] = df1.min(axis=1)\n",
    "    return y ;\n",
    "\n",
    "def getBiggestN(df):\n",
    "    df1 = df[['N1','N2','N3','N4','N5','N6','N7']]\n",
    "    y = pd.DataFrame(index=df.index)\n",
    "    y['LN'] = df1.max(axis=1)\n",
    "    return y ;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded MyTotoResearch algo_no:  1\n",
      "1521\n",
      "      N1  N2  N3  N4  N5  N6  N7         D\n",
      "0      3  10  18  21  29  36  42  20040212\n",
      "1      6   8  22  39  40  41  45  20040216\n",
      "2      5   8  15  17  21  35  36  20040219\n",
      "3      6  20  21  23  28  38  39  20040223\n",
      "4      3  10  16  25  26  40  44  20040226\n",
      "5     14  20  22  25  27  28  29  20040301\n",
      "6      2   6   7  25  40  41  43  20040304\n",
      "7      2  17  18  19  21  42  44  20040308\n",
      "8     10  18  24  26  34  36  43  20040311\n",
      "9      1   9  13  16  32  35  45  20040315\n",
      "10     6  17  21  31  40  42  43  20040318\n",
      "11    16  21  22  23  27  28  35  20040322\n",
      "12     6  13  18  20  37  38  43  20040325\n",
      "13     1   2   6   7  19  24  32  20040329\n",
      "14     3   9  27  28  31  39  40  20040401\n",
      "15     4   7  12  13  15  16  33  20040405\n",
      "16     4  11  25  30  33  36  39  20040408\n",
      "17     2   8  10  16  29  32  37  20040412\n",
      "18     2   3   9  19  26  32  34  20040415\n",
      "19    10  15  18  26  36  39  45  20040419\n",
      "20     5   6   8  14  23  26  37  20040422\n",
      "21     7  15  18  24  25  37  41  20040426\n",
      "22     6  10  18  22  31  32  43  20040429\n",
      "23     3   4   9  12  20  33  37  20040503\n",
      "24     5  11  20  23  27  30  34  20040506\n",
      "25     2   6   8  16  19  21  41  20040510\n",
      "26    11  13  16  21  24  35  45  20040513\n",
      "27     2   6  10  13  17  21  28  20040517\n",
      "28    17  20  23  25  32  36  45  20040520\n",
      "29     6   7  16  21  33  40  44  20040524\n",
      "...   ..  ..  ..  ..  ..  ..  ..       ...\n",
      "1491   5   6  16  24  26  29  38  20180827\n",
      "1492   2   3  19  23  30  39  41  20180823\n",
      "1493   2   9  10  25  38  40  42  20180820\n",
      "1494  20  22  23  25  32  33  36  20180816\n",
      "1495   1   3   6  16  17  22  36  20180813\n",
      "1496  13  16  20  23  28  39  42  20180809\n",
      "1497   7  15  18  20  27  36  40  20180806\n",
      "1498   1  10  15  27  35  41  46  20180802\n",
      "1499   7   8  10  19  20  41  43  20180730\n",
      "1500   1   9  13  17  28  37  40  20180726\n",
      "1501   2  12  23  26  28  39  40  20180723\n",
      "1502  13  14  23  35  37  45  46  20180719\n",
      "1503   4   8  19  22  24  32  47  20180716\n",
      "1504   4  10  15  25  32  40  41  20180712\n",
      "1505   6  23  31  33  38  39  43  20180709\n",
      "1506   8  11  28  30  32  34  39  20180705\n",
      "1507  12  13  23  26  33  35  38  20180702\n",
      "1508   2   3   9  25  38  44  48  20180625\n",
      "1509   4   6  15  24  30  35  46  20180621\n",
      "1510  11  15  22  23  25  26  43  20180618\n",
      "1511   1   4  29  31  35  42  48  20180614\n",
      "1512  16  25  30  34  37  44  49  20180611\n",
      "1513  12  20  29  31  37  39  42  20180607\n",
      "1514  20  22  27  31  37  43  45  20180604\n",
      "1515  11  13  24  26  33  47  49  20180531\n",
      "1516   2   5   9  27  28  30  44  20180528\n",
      "1517  11  16  25  26  34  36  42  20180524\n",
      "1518   8  10  16  17  30  37  44  20180521\n",
      "1519   7  13  21  25  29  35  37  20180517\n",
      "1520   5  17  24  29  45  46  49  20180514\n",
      "\n",
      "[1521 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "\n",
    "mtr = MyTotoResearch(algo_no=1)\n",
    "lresult, df = mtr.load_totodata()\n",
    "\n",
    "X = mtr.modified_dataset(getAllData(df)) #\n",
    "\n",
    "#X\n",
    "\n",
    "#getSmallestN(df)\n",
    "#getBiggestN(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result = {}\n",
    "def store_model_result(sEstimator, N, r):\n",
    "    if ( sEstimator not in model_result ):\n",
    "        model_result[sEstimator] = pd.DataFrame(index=df.index) \n",
    "    m_df = model_result[sEstimator] ;\n",
    "    m_df[N] = r\n",
    "    return m_df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "data = [[1,2,3],[4,5,6],[7,8,9],[10,11,23]]\n",
    "def helperMethod(oldCombination, row, col):\n",
    "    newCombination = oldCombination + str(data[row][col]);\n",
    "    if (row == len(data) - 1):\n",
    "        print(newCombination);\n",
    "    if (row < len(data) - 1):\n",
    "        helperMethod(newCombination, row + 1, 0);\n",
    "    if (col < len(data[row]) - 1):\n",
    "        helperMethod(oldCombination, row, col + 1);\n",
    "#    print(row,col,oldCombination)\n",
    "#     if ( row == 1 and oldCombination == \"\" ):\n",
    "#         return ;\n",
    "\n",
    "#helperMethod(\"\",0, 0);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['2', '4', '7', '10'], ['2', '4', '7', '11'], ['2', '4', '7', '23'], ['2', '4', '8', '10'], ['2', '4', '8', '11'], ['2', '4', '8', '23'], ['2', '4', '9', '10'], ['2', '4', '9', '11'], ['2', '4', '9', '23'], ['2', '5', '7', '10'], ['2', '5', '7', '11'], ['2', '5', '7', '23'], ['2', '5', '8', '10'], ['2', '5', '8', '11'], ['2', '5', '8', '23'], ['2', '5', '9', '10'], ['2', '5', '9', '11'], ['2', '5', '9', '23'], ['2', '6', '7', '10'], ['2', '6', '7', '11'], ['2', '6', '7', '23'], ['2', '6', '8', '10'], ['2', '6', '8', '11'], ['2', '6', '8', '23'], ['2', '6', '9', '10'], ['2', '6', '9', '11'], ['2', '6', '9', '23'], ['3', '4', '7', '10'], ['3', '4', '7', '11'], ['3', '4', '7', '23'], ['3', '4', '8', '10'], ['3', '4', '8', '11'], ['3', '4', '8', '23'], ['3', '4', '9', '10'], ['3', '4', '9', '11'], ['3', '4', '9', '23'], ['3', '5', '7', '10'], ['3', '5', '7', '11'], ['3', '5', '7', '23'], ['3', '5', '8', '10'], ['3', '5', '8', '11'], ['3', '5', '8', '23'], ['3', '5', '9', '10'], ['3', '5', '9', '11'], ['3', '5', '9', '23'], ['3', '6', '7', '10'], ['3', '6', '7', '11'], ['3', '6', '7', '23'], ['3', '6', '8', '10'], ['3', '6', '8', '11'], ['3', '6', '8', '23'], ['3', '6', '9', '10'], ['3', '6', '9', '11'], ['3', '6', '9', '23']]\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "data = [[1,2,3],[4,5,6],[7,8,9],[10,11,23]]\n",
    "\n",
    "data_comb = []\n",
    "def getYColumn(oldCombination, row, col):\n",
    "#     if ( len(oldCombination) > 0 ):\n",
    "#         oldCombination = oldCombination + \",\"\n",
    "    newCombination = oldCombination + str(data[row][col]) + \",\"\n",
    "    if (row == len(data) - 1):\n",
    "#        print(newCombination.split(\",\"));\n",
    "        data_comb.append(newCombination[0:-1].split(\",\")) ;\n",
    "    if (row < len(data) - 1):\n",
    "        getYColumn(newCombination, row + 1, 0);\n",
    "    if (col < len(data[row]) - 1):\n",
    "        getYColumn(oldCombination, row, col + 1);\n",
    "\n",
    "getYColumn(\"\",0, 1);\n",
    "print(data_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "data = [[1,2,3],[4,5,6],[7,8,9],[10,11,23]]\n",
    "getYLimited_start_pos = 0\n",
    "#data_comb = []\n",
    "def getYLimited(data_comb, oldCombination, row, col, howmany):\n",
    "    global getYLimited_start_pos\n",
    "    newCombination = oldCombination + str(data[row][col]) + \",\"\n",
    "    if (row == len(data) - 1):\n",
    "        if ( getYLimited_start_pos <= 0 ):\n",
    "            data_comb.append(newCombination[0:-1].split(\",\"))\n",
    "        else:\n",
    "            getYLimited_start_pos = getYLimited_start_pos - 1\n",
    "    if ( len(data_comb) == howmany ):\n",
    "        return data_comb ;\n",
    "    if (row < len(data) - 1):\n",
    "        getYLimited(data_comb, newCombination, row + 1, 0, howmany);\n",
    "    if (col < len(data[row]) - 1):\n",
    "        getYLimited(data_comb, oldCombination, row, col + 1, howmany);\n",
    "\n",
    "# data_comb = []\n",
    "# getYLimited_start_pos = 0\n",
    "# getYLimited(\"\",0, 1, 11);\n",
    "# print(data_comb)\n",
    "# getYLimited_start_pos = 0\n",
    "# data_comb = []\n",
    "# max_comb = 5\n",
    "# getYLimited(\"\",0, 1, 5);\n",
    "# print(data_comb)\n",
    "# getYLimited_start_pos = 5\n",
    "# data_comb = []\n",
    "# getYLimited(\"\",0, 1, 5);\n",
    "# print(data_comb)\n",
    "# getYLimited_start_pos = 10\n",
    "# data_comb = []\n",
    "# getYLimited(\"\",0, 1, 3);\n",
    "# print(data_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#of Entries:  1521\n",
      "[[  3.  10.  18. ...,  29.  36.  42.]\n",
      " [  6.   8.  22. ...,  40.  41.  45.]\n",
      " [  5.   8.  15. ...,  21.  35.  36.]\n",
      " ..., \n",
      " [  8.  10.  16. ...,  30.  37.  44.]\n",
      " [  7.  13.  21. ...,  29.  35.  37.]\n",
      " [  5.  17.  24. ...,  45.  46.  49.]]\n",
      "1398000 "
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Lasso, SGDRegressor, SGDClassifier, LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor  \n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X.drop('T',axis=1))\n",
    "\n",
    "# X_scaled = scaler.transform(X.drop('T',axis=1))\n",
    "X_scaled = X\n",
    "\n",
    "l_encoder = LabelEncoder()\n",
    "result = pd.DataFrame(index=df.index)\n",
    "result['T'] = df['T']\n",
    "\n",
    "test_data = mtr.get_test_data()\n",
    "\n",
    "y_combination = pd.DataFrame(index=df.index)\n",
    "for i in range(1,8):\n",
    "     y_combination['N'+str(i)] = df['N'+str(i)]\n",
    "#y_combination.reset_index()\n",
    "data = y_combination.values\n",
    "check_size = len(data)\n",
    "print(\"#of Entries: \", check_size)\n",
    "print(data)\n",
    "\n",
    "dt = LinearRegression(fit_intercept=True) # 22.65% intersection\n",
    "#dt = AdaBoostRegressor(random_state=seed,base_estimator=dt_use,learning_rate=1)\n",
    "#dt = SVC(kernel='linear', C=1E10)\n",
    "\n",
    "\n",
    "#dt = DecisionTreeRegressor()\n",
    "#max_features='sqrt', splitter='random', min_samples_split=4, max_depth=3) ;\n",
    "max_comb = 2000\n",
    "model_result = {}\n",
    "bBest = 0.0\n",
    "getYLimited_start_pos = 0\n",
    "for idx in range(700,1000000):\n",
    "    data_comb = []\n",
    "    getYLimited_start_pos = (idx-1) * max_comb\n",
    "    getYLimited(data_comb,\"\",0, 0, max_comb);\n",
    "    print(((idx-1) * max_comb), end=' ')\n",
    "#     if ( len(data_comb[0]) < check_size ):\n",
    "#         print(\"ERROR generating combination. \", len(data_comb[0]), \" expected \", check_size)\n",
    "#         break \n",
    "    for y_c in range(0,len(data_comb)):\n",
    "        sEstimator = \"DT\" + str(y_c)\n",
    "        #     model = Pipeline([('poly', PolynomialFeatures(degree=deg)),\n",
    "        #                       ('linear', AdaBoostRegressor(random_state=seed,base_estimator=dt,learning_rate=1))])\n",
    "        model = dt ;\n",
    "        model.fit(X_scaled, data_comb[y_c])\n",
    "#        p = store_model_result(sEstimator, \"N\", model.predict(X_scaled).round())\n",
    "        p = model.predict(X_scaled).round()\n",
    "        m_df = pd.DataFrame(index=df.index) \n",
    "        m_df['N'] = p\n",
    "        p = m_df\n",
    "    \n",
    "#    print(p)\n",
    "#     dt = DecisionTreeRegressor()\n",
    "#     dt = DecisionTreeRegressor(max_features='sqrt', splitter='random', min_samples_split=4, max_depth=3) ;\n",
    "        bAccuracy = mtr.getAccuracyCount(np.array(p)) ;\n",
    "        if ( bBest < bAccuracy or bAccuracy > 98.0 ):\n",
    "            print ( str(idx) + \" \" + sEstimator + \" Accuracy: \",  bAccuracy)\n",
    "            bBest = bAccuracy\n",
    "#            mtr.plot_matched_counts(p.values)\n",
    "    #        mtr.print_predictions(result)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_printed = {}\n",
    "bBest = 0.0\n",
    "print ( \"BEST INTERSECTION\")\n",
    "for key1, value1 in model_result.items():\n",
    "    for key2, value2 in model_result.items():\n",
    "        if ( key1 != key2 ):\n",
    "            if ( (key1 + key2) not in already_printed ):\n",
    "                if ( (key2 + key1) not in already_printed ) :\n",
    "                    already_printed[key1+key2] = 'Y'\n",
    "                    p = mtr.getIntersection(value1.values, value2.values)\n",
    "                    bAccuracy = mtr.getAccuracyCount(np.array(p)) ;\n",
    "                    if ( bBest < bAccuracy ):\n",
    "                        print ( key1, key2, \" Accuracy: \",  bAccuracy)\n",
    "                        bBest = bAccuracy\n",
    "                        mtr.plot_matched_counts(p)\n",
    "#                        mtr.print_predictionsV2(np.array(p))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_printed = {}\n",
    "bBest = 0.0\n",
    "print ( \"BEST UNION\")\n",
    "for key1, value1 in model_result.items():\n",
    "    for key2, value2 in model_result.items():\n",
    "        if ( key1 != key2 ):\n",
    "            if ( (key1 + key2) not in already_printed ):\n",
    "                if ( (key2 + key1) not in already_printed ) :\n",
    "                    already_printed[key1+key2] = 'Y'\n",
    "                    p = [np.union1d(a,b) for a,b in zip(value1.values, value2.values)] #mtr.getIntersection(value1.values, value2.values)\n",
    "                    bAccuracy = mtr.getAccuracyCount(np.array(p)) ;\n",
    "                    if ( bBest < bAccuracy ):\n",
    "                        print ( key1, key2, \" Accuracy: \",  bAccuracy)\n",
    "                        bBest = bAccuracy\n",
    "                        mtr.plot_matched_counts(p)\n",
    "mtr.print_predictionsV2(np.array(p))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = result.columns #['N'+str(i)+'_P' for i in range(1,8)]\n",
    "print(result.columns)\n",
    "my_prediction = pd.DataFrame(result, columns=columns)\n",
    "\n",
    "print(my_prediction)\n",
    "for col in columns:\n",
    "    my_prediction[col] = my_prediction[col].apply(lambda x: int(x) if x == x else \"\")\n",
    "    \n",
    "#print(my_prediction)\n",
    "print ( \"Accuracy: \",  mtr.getAccuracyCount(np.array(my_prediction)))\n",
    "mtr.plot_matched_counts(my_prediction.values)\n",
    "#mtr.print_result(my_prediction)\n",
    "mtr.print_predictions(my_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.unique1d(a,b) for a,b in zip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = mtr.get_test_data()\n",
    "#X_test = mtr.modified_dataset(getAllData(test_data)) #\n",
    "X_test = test_data[X.columns]\n",
    "#scaler.fit(X_test)\n",
    "X_test_scaled = scaler.transform(X_test.drop('T',axis=1))\n",
    "#[X.columns])\n",
    "X_test_scaled = X_test\n",
    "#[X.columns]\n",
    "test_result = pd.DataFrame(index=X_test.index)\n",
    "test_result['T'] = test_data['T']\n",
    "\n",
    "for i in range(1,8):\n",
    "    test_result['N'+str(i)+'_P'] = model_store[i-1].predict(X_test_scaled).round()\n",
    "#     test_result['N'+str(i)+'_P0'] = test_result['N'+str(i)+'_P']-1\n",
    "#     test_result['N'+str(i)+'_P1'] = test_result['N'+str(i)+'_P']+1\n",
    "\n",
    "\n",
    "#mtr.print_result(test_result)\n",
    "mtr.print_predictions(test_result.astype(int))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_result = np.unique(result.values)\n",
    "print(result.values)\n",
    "predicted_result = [np.unique(a) for a in result.values]\n",
    "len(predicted_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

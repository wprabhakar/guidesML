{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cores:  12\n"
     ]
    }
   ],
   "source": [
    "#!conda install -n mldds -c anaconda joblib\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "import multiprocessing\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "print(\"Cores: \", num_cores)\n",
    "\n",
    "import time\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto( device_count = {'GPU': 0 , 'CPU': num_cores} )\n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:138: DeprecationWarning: invalid escape sequence \\s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import utils, preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from matplotlib.pyplot import figure\n",
    "from functools import reduce\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MyTotoResearch:\n",
    "    \n",
    "    @classmethod\n",
    "    def __init__(self, algo_no=0, inputPPFile='../input/PPv3.csv', inputTotoResult='../input/SGH.csv'):\n",
    "        self.algo_number = algo_no\n",
    "        print('Loaded MyTotoResearch algo_no: ', self.algo_number)\n",
    "\n",
    "    @classmethod\n",
    "    def load_totodata(self, inputPPFile='../input/PPv3.csv', inputTotoResult='../input/SGH.csv'):\n",
    "        pp = pd.read_csv(inputPPFile)\n",
    "        lr = pd.read_csv(inputTotoResult)\n",
    "        print(len(lr))\n",
    "        cols = ['D', 'N1','N2','N3','N4','N5','N6','N7']\n",
    "        lr = lr[cols]\n",
    "\n",
    "        #https://pandas.pydata.org/pandas-docs/stable/merging.html\n",
    "        df = pd.concat([pp, lr], axis=1, sort=False)\n",
    "        df = df.dropna()\n",
    "        df.reset_index().drop(['D'], axis=1)\n",
    "\n",
    "        print(df.shape)\n",
    "        counts = [df['N'+str(i)].value_counts() for i in range(1,8)]\n",
    "        for i in range(len(counts)):\n",
    "            df = df[~df['N'+str(i+1)].isin(counts[i][counts[i] <= 3].index)]\n",
    "        print('After removing numbers that have shown 3 or less times')\n",
    "        print(df.shape)\n",
    "\n",
    "        cols = ['N1','N2','N3','N4','N5','N6','N7']\n",
    "        lr = df[cols]\n",
    "\n",
    "        self.df = df\n",
    "        self.lresult = np.sort(lr.values[:, ::-1])\n",
    "        return self.lresult, self.df \n",
    "    \n",
    "    @classmethod\n",
    "    def modified_dataset ( self, dataset ):\n",
    "        self.dataset = dataset\n",
    "        return self.dataset\n",
    "    \n",
    "    @classmethod\n",
    "    def get_result_n(self, col_n):\n",
    "        aa = np.delete(self.lresult, np.s_[col_n:], axis=1)  \n",
    "        aa = np.delete(aa, np.s_[0:col_n-1], axis=1)  \n",
    "        return pd.DataFrame(aa, columns=list('N'))\n",
    "\n",
    "    @classmethod\n",
    "    def get_result_n_encoded(self, col_n):\n",
    "        aa = np.delete(self.lresult, np.s_[col_n:], axis=1)  \n",
    "        aa = np.delete(aa, np.s_[0:col_n-1], axis=1)  \n",
    "        # 1. INSTANTIATE\n",
    "        enc = preprocessing.OneHotEncoder()\n",
    "\n",
    "        # 2. FIT\n",
    "        enc.fit(aa)\n",
    "\n",
    "        # 3. Transform\n",
    "        onehotlabels = enc.transform(aa).toarray()\n",
    "        onehotlabels.shape\n",
    "        #print(onehotlabels)\n",
    "\n",
    "        #Convert 2d array to Dataframe\n",
    "        y = pd.DataFrame(aa, columns=list('N'))\n",
    "        y.head()\n",
    "        y = aa.astype(int).ravel()\n",
    "    #    print ( y )\n",
    "        return y\n",
    "        \n",
    "    @classmethod\n",
    "    def get_test_data(self, file_name = '../input/PPv3-Predict.csv' ):\n",
    "        self.data2Predict = pd.read_csv(file_name)\n",
    "        self.data2Predict.reset_index()\n",
    "        return self.data2Predict\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def plot_history(self, history):\n",
    "        loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
    "        val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
    "        acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n",
    "        val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n",
    "\n",
    "        if len(loss_list) == 0:\n",
    "            print('Loss is missing in history')\n",
    "            return \n",
    "\n",
    "        ## As loss always exists\n",
    "        epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
    "\n",
    "        ## Loss\n",
    "        figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "        plt.figure(1)\n",
    "        for l in loss_list:\n",
    "            plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "        for l in val_loss_list:\n",
    "            plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "\n",
    "        plt.title('Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        ## Accuracy\n",
    "        plt.figure(1)\n",
    "        for l in acc_list:\n",
    "            plt.plot(epochs, history.history[l], 'r', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "        for l in val_acc_list:    \n",
    "            plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "\n",
    "        plt.title('Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    @classmethod\n",
    "    def save_model(self, model, predict_number):\n",
    "        # serialize model to JSON\n",
    "        model_json = model.to_json()\n",
    "        with open(str(self.algo_number) + '_' + str(predict_number) + \"_model.json\", \"w\") as json_file:\n",
    "            json_file.write(simplejson.dumps(simplejson.loads(model_json), indent=4))\n",
    "\n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(str(self.algo_number) + '_' + str(predict_number) + \"_model.h5\")\n",
    "        print(\"Saved model to disk \", self.algo_number, \" Predict #N \", predict_number)\n",
    "\n",
    "    @classmethod\n",
    "    def print_result(self, predicted_values ):\n",
    "        test_df = pd.read_csv('../input/TestResult.csv', sep='\\s+', header=None, names=['D','N1','N2','N3','N4','N5','N6','N7'])\n",
    "        test_df['D'].replace(regex=True,inplace=True,to_replace=r'-',value=r'')\n",
    "        test_df['D'] = pd.to_numeric(test_df['D'])\n",
    "\n",
    "        cols = ['D', 'N1','N2','N3','N4','N5','N6','N7']\n",
    "        test_df = self.data2Predict.merge(test_df, left_on='T', right_on='D', how='inner')\n",
    "        test_df = test_df[cols]\n",
    "\n",
    "        tdfResult = predicted_values.drop(predicted_values.columns[0], axis=1) ;\n",
    "\n",
    "        actual_result = test_df[cols[1:]].values\n",
    "        predicted_result = tdfResult.values\n",
    "\n",
    "        matched = getIntersection(actual_result, predicted_result)\n",
    "\n",
    "        c = 0\n",
    "        for i in range(len(matched)):\n",
    "            print(int(self.data2Predict.loc[i]['T']), ' ', actual_result[i], ' ', predicted_result[i], ' ', matched[c])\n",
    "            c += 1\n",
    "        for i in range(c, len(predicted_result)):\n",
    "            print(int(self.data2Predict.loc[i]['T']), ' Predicted: ', predicted_result[i], ' ')\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(self, predict_number):\n",
    "        json_file = open(str(self.algo_number) + \"_\" + str(predict_number)+'_model.json', 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "        # load weights into new model\n",
    "        loaded_model.load_weights(str(self.algo_number) + \"_\" + str(predict_number)+\"_model.h5\")\n",
    "        print(\"Loaded model from disk \" + str(self.algo_number) + \"_\" + str(predict_number) + \"_model\" )\n",
    "        return loaded_model\n",
    "\n",
    "    @classmethod\n",
    "    def getTargets(self):\n",
    "        return np.array([self.get_result_n(i)['N'] for i in range(1,8)]).T\n",
    "\n",
    "    @classmethod\n",
    "    def getTarget(self, N):\n",
    "        return ([self.get_result_n(i)['N'] for i in range(1,8)])[N-1]\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def getIntersection(p1, p2):\n",
    "        return [reduce(np.intersect1d, (p.astype(int), a.astype(int))) for (p,a) in zip(p1, p2)]\n",
    "\n",
    "    @classmethod\n",
    "    def print_predictions(self, dfPredictions, result='../input/TestResult.csv'):\n",
    "        #load the test Toto Results files\n",
    "        test_df = pd.read_csv(result, sep='\\s+', header=None, names=['D','N1','N2','N3','N4','N5','N6','N7'])\n",
    "        test_df['D'].replace(regex=True,inplace=True,to_replace=r'-',value=r'')\n",
    "        test_df['D'] = pd.to_numeric(test_df['D'])\n",
    "\n",
    "        #Merge the Planet Position File with the Toto Results file\n",
    "        test_df = self.data2Predict.merge(test_df, left_on='T', right_on='D', how='inner')\n",
    "\n",
    "        #Extract only the Results for all dates\n",
    "        cols = ['D', 'N1','N2','N3','N4','N5','N6','N7']\n",
    "        test_df = test_df[cols]\n",
    "\n",
    "        tdfResult = dfPredictions.drop(dfPredictions.columns[0], axis=1) ;\n",
    "\n",
    "        actual_result = test_df[cols[1:]].values\n",
    "        predicted_result = tdfResult.values\n",
    "\n",
    "        matched = MyTotoResearch.getIntersection(actual_result, predicted_result)\n",
    "\n",
    "        c = 0\n",
    "        for i in range(len(matched)):\n",
    "            print(int(self.data2Predict.loc[i]['T']), ' ', actual_result[i], ' ', predicted_result[i], ' ', matched[c])\n",
    "            c += 1\n",
    "        for i in range(c, len(predicted_result)):\n",
    "            print(int(self.data2Predict.loc[i]['T']), ' Predicted: ', predicted_result[i], ' ')\n",
    "\n",
    "            \n",
    "def getAdjustedDataF(df,f):\n",
    "    #Use only Planet Positions Testing\n",
    "    cols = ['L','M','S', 'R','E','A','V' ,'J','U','K']\n",
    "    X = df[cols]\n",
    "    deg = f\n",
    "    \n",
    "#     X['S_3'] = X['S'] // (deg*3)\n",
    "#     X['L_3'] = X['L'] // (deg*3)\n",
    "#     X['M_3'] = X['M'] // (deg*3)\n",
    "#     X['R_3'] = X['R'] // (deg*3)\n",
    "#     X['E_3'] = X['E'] // (deg*3)\n",
    "#     X['A_3'] = X['A'] // (deg*3)\n",
    "#     X['V_3'] = X['V'] // (deg*3)\n",
    "#     X['J_3'] = X['J'] // (deg*3)\n",
    "#     X['U_3'] = X['U'] // (deg*3)\n",
    "\n",
    "\n",
    "#     X['S_2'] = X['S'] // (deg*2)\n",
    "#     X['L_2'] = X['L'] // (deg*2)\n",
    "#     X['M_2'] = X['M'] // (deg*2)\n",
    "#     X['R_2'] = X['R'] // (deg*2)\n",
    "#     X['E_2'] = X['E'] // (deg*2)\n",
    "#     X['A_2'] = X['A'] // (deg*2)\n",
    "#     X['V_2'] = X['V'] // (deg*2)\n",
    "#     X['J_2'] = X['J'] // (deg*2)\n",
    "#     X['U_2'] = X['U'] // (deg*2)\n",
    "\n",
    "    X['S_1'] = X['S'] // (deg)\n",
    "    X['L_1'] = X['L'] // (deg)\n",
    "    X['M_1'] = X['M'] // (deg)\n",
    "    X['R_1'] = X['R'] // (deg)\n",
    "    X['E_1'] = X['E'] // (deg)\n",
    "    X['A_1'] = X['A'] // (deg)\n",
    "    X['V_1'] = X['V'] // (deg)\n",
    "    X['J_1'] = X['J'] // (deg)\n",
    "    X['U_1'] = X['U'] // (deg)\n",
    "   \n",
    "    X = X.drop(cols, axis=1)\n",
    "    return X\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping \n",
    "\n",
    "class MyEarlyStopping(EarlyStopping):\n",
    "    def __init__(self, threshold, **kwargs):\n",
    "        super(MyEarlyStopping, self).__init__(**kwargs)\n",
    "        self.threshold = threshold # threshold for validation loss\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            warnings.warn(\n",
    "                'Early stopping conditioned on metric `%s` '\n",
    "                'which is not available. Available metrics are: %s' %\n",
    "                (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n",
    "            )\n",
    "            return\n",
    "\n",
    "        # implement your own logic here\n",
    "        if (current >= self.threshold):\n",
    "            self.stopped_epoch = epoch\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "class MyEarlyStoppingLoss(EarlyStopping):\n",
    "    def __init__(self, threshold, **kwargs):\n",
    "        super(MyEarlyStoppingLoss, self).__init__(**kwargs)\n",
    "        self.threshold = threshold # threshold for validation loss\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            warnings.warn(\n",
    "                'Early stopping conditioned on metric `%s` '\n",
    "                'which is not available. Available metrics are: %s' %\n",
    "                (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n",
    "            )\n",
    "            return\n",
    "\n",
    "        # implement your own logic here\n",
    "        if (current <= self.threshold):\n",
    "            self.stopped_epoch = epoch\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded MyTotoResearch algo_no:  1\n",
      "1521\n",
      "(1521, 59)\n",
      "After removing numbers that have shown 3 or less times\n",
      "(1486, 59)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walter/Software/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/walter/Software/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:41: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1486, 7)\n",
      "1.0\n",
      " Time taken:  0.00025700000014694524  \n"
     ]
    }
   ],
   "source": [
    "from keras.models import Input, Model\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, History\n",
    "import json as simplejson\n",
    "from keras import regularizers\n",
    "from sklearn import preprocessing\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDRegressor, SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "\n",
    "def getAllData(df):\n",
    "    drop_cols = ['T', 'D', 'N1','N2','N3','N4','N5','N6','N7','L','M','S','R','E','A','V' ,'J','U','K']\n",
    "    X = df.drop(drop_cols, axis=1)\n",
    "    return X\n",
    "\n",
    "\n",
    "#early_stopping = MyEarlyStopping ( threshold=.99999, monitor='acc', verbose=1 )\n",
    "#early_stopping_loss = MyEarlyStoppingLoss ( threshold=.005, monitor='loss', verbose=1 )\n",
    "\n",
    "#Deep Neuro Network\n",
    "mtr = MyTotoResearch(algo_no=1)\n",
    "lresult, df = mtr.load_totodata()\n",
    "X = mtr.modified_dataset(getAllData(df)) #\n",
    "#target = np.array([mtr.get_result_n(i)['N'] for i in range(1,8)]).T\n",
    "target = mtr.getTargets() ;\n",
    "#print(target)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "Z = scaler.transform(X)\n",
    "                 \n",
    "Z = X\n",
    "\n",
    "sgd = GaussianProcessClassifier()\n",
    "\n",
    "multi_target_sgd = MultiOutputClassifier(sgd)\n",
    "print(target.shape)\n",
    "multi_target_sgd.fit(Z, mtr.getTargets()) \n",
    "print(multi_target_sgd.score(Z, mtr.getTargets()))\n",
    "\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "#print(\"Accuracy:\",metrics.accuracy_score(mtr.getTargets(), y_pred))\n",
    "\n",
    "# multi_target_sgd = sgd\n",
    "# multi_target_sgd.fit(Z, mtr.getTarget(0)) \n",
    "# print(multi_target_sgd.score(Z, mtr.getTarget(0)))\n",
    "\n",
    "start = time.clock()\n",
    "#history = multi_target_sgd.fit(X, target) #@, epochs=12000, verbose=0) #, callbacks=[early_stopping]) #, callbacks=[early_stopping]) #, checkpoint])    \n",
    "print(\" Time taken: \", (time.clock() - start),  \" \")\n",
    "\n",
    "# n_iters = [100,2000,5000,10000]\n",
    "# scores = []\n",
    "# for n_iter in n_iters:\n",
    "#     model = SGDClassifier(loss=\"log\", alpha=0.001, penalty=\"l2\", max_iter=n_iter, tol=1e-6, random_state=42)\n",
    "#     model.fit(Z, mtr.getTarget(0))\n",
    "#     scores.append(model.score(Z, mtr.getTarget(0)))\n",
    "  \n",
    "# plt.title(\"Effect of n_iter\")\n",
    "# plt.xlabel(\"n_iter\")\n",
    "# plt.ylabel(\"score\")\n",
    "# plt.plot(n_iters, scores) \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walter/Software/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/walter/Software/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20180514   [17 24 29 45 46 49  5]   [16. 21. 22. 24. 25. 27.]   [24]\n",
      "20180517   [ 7 21 25 29 35 37 13]   [16. 17. 34. 40. 44. 48.]   []\n",
      "20180521   [ 8 10 16 30 37 44 17]   [ 6.  9. 15. 18. 40. 43.]   []\n",
      "20180524   [11 25 26 34 36 42 16]   [15. 17. 20. 23. 30. 45.]   []\n",
      "20180528   [ 5  9 27 28 30 44  2]   [ 4.  5. 13. 18. 39. 40.]   [5]\n",
      "20180531   [11 13 24 26 47 49 33]   [ 9. 27. 29. 31. 40. 46.]   []\n",
      "20180604   [20 22 31 37 43 45 27]   [ 6. 16. 24. 26. 29. 38.]   []\n",
      "20180607   [12 20 29 31 37 39 42]   [ 3. 19. 23. 30. 39. 41.]   [39]\n",
      "20180611   [16 25 30 37 44 49 34]   [ 9. 10. 25. 38. 40. 42.]   [25]\n",
      "20180614   [ 4 29 31 35 42 48  1]   [22. 23. 25. 32. 33. 36.]   []\n",
      "20180618   [11 15 22 23 26 43 25]   [ 3.  6. 16. 17. 22. 36.]   [22]\n",
      "20180621   [ 4  6 15 24 30 35 46]   [16. 20. 23. 28. 39. 42.]   []\n",
      "20180625   [ 2  5 25 38 44 48  9]   [15. 18. 20. 27. 36. 40.]   []\n",
      "20180628   [ 2  7 22 27 40 47 48]   [10. 15. 27. 35. 41. 46.]   [27]\n",
      "20180702   [12 13 26 33 35 38 23]   [ 8. 10. 19. 20. 41. 43.]   []\n",
      "20180705   [ 8 11 28 30 32 34 39]   [ 9. 13. 17. 28. 37. 40.]   [28]\n",
      "20180709   [ 6 23 31 38 39 43 33]   [12. 23. 26. 28. 39. 40.]   [23 39]\n",
      "20180712   [ 4 15 25 32 40 41 10]   [14. 23. 35. 37. 45. 46.]   []\n",
      "20180716   [ 4  8 19 24 32 47 22]   [ 8. 19. 22. 24. 32. 47.]   [ 8 19 22 24 32 47]\n",
      "20180719   [13 14 23 35 37 46 45]   [10. 15. 25. 32. 40. 41.]   []\n",
      "20180723   [ 2 23 26 28 39 40 12]   [23. 31. 33. 38. 39. 43.]   [23 39]\n",
      "20180726   [ 1  9 13 17 28 40 37]   [11. 28. 30. 32. 34. 39.]   [28]\n",
      "20180730   [ 8 10 19 20 41 43  7]   [13. 23. 26. 33. 35. 38.]   []\n",
      "20180802   [ 1 10 15 27 41 46 35]   [ 3.  9. 25. 38. 44. 48.]   []\n",
      "20180806   [ 7 18 20 27 36 40 15]   [ 6. 15. 24. 30. 35. 46.]   [15]\n",
      "20180809   [13 16 20 23 39 42 28]   [15. 22. 23. 25. 26. 43.]   [23]\n",
      "20180813   [ 1  3  6 16 22 36 17]   [ 4. 29. 31. 35. 42. 48.]   []\n",
      "20180816   [22 23 25 32 33 36 20]   [25. 30. 34. 37. 44. 49.]   [25]\n",
      "20180820   [ 9 10 25 38 40 42  2]   [20. 29. 31. 37. 39. 42.]   [42]\n",
      "20180823   [ 2  3 23 30 39 41 19]   [22. 27. 31. 37. 43. 45.]   []\n",
      "20180827   [ 5  6 16 24 26 29 38]   [13. 24. 26. 33. 47. 49.]   [24 26]\n",
      "20180830   [ 3  9 27 29 31 40 46]   [ 5.  9. 27. 28. 30. 44.]   [ 9 27]\n",
      "20180903   [ 4  5 13 18 39 40  3]   [16. 25. 26. 34. 36. 42.]   []\n",
      "20180906   [ 2 15 17 20 23 30 45]   [10. 16. 17. 30. 37. 44.]   [17 30]\n",
      "20180910   [ 2  6  9 15 40 43 18]   [13. 21. 25. 29. 35. 37.]   []\n",
      "20180913   [ 6 16 17 40 44 48 34]   [31. 38. 43. 47. 48. 49.]   [48]\n",
      "20180917   [16 21 22 24 25 27  1]   [31. 38. 43. 47. 48. 49.]   []\n",
      "20180920   [ 5 12 18 30 32 38 22]   [31. 38. 43. 47. 48. 49.]   [38]\n",
      "20180924   [ 6  8 17 24 29 47 34]   [31. 38. 43. 47. 48. 49.]   [47]\n",
      "20180927   [ 2 25 29 33 42 45 20]   [31. 38. 43. 47. 48. 49.]   []\n",
      "20181001   [11 15 23 24 32 40 43]   [31. 38. 43. 47. 48. 49.]   [43]\n",
      "20181004   [ 5 12 23 32 37 42 43]   [31. 38. 43. 47. 48. 49.]   [43]\n",
      "20181008   [17 18 23 39 43 49  2]   [31. 38. 43. 47. 48. 49.]   [43 49]\n",
      "20181011   [ 1 16 18 24 29 46 35]   [31. 38. 43. 47. 48. 49.]   []\n",
      "20181015   [ 1  4 24 32 35 48 20]   [31. 38. 43. 47. 48. 49.]   [48]\n",
      "20181018   [ 5 14 17 31 46 48 47]   [31. 38. 43. 47. 48. 49.]   [31 47 48]\n",
      "20181022   [ 5 22 24 40 43 48  2]   [31. 38. 43. 47. 48. 49.]   [43 48]\n",
      "20181025   [ 7  8 13 15 35 48 30]   [31. 38. 43. 47. 48. 49.]   [48]\n",
      "20181029   [ 2  6 10 20 28 31 30]   [31. 38. 43. 47. 48. 49.]   [31]\n",
      "20181101   [ 6 27 28 41 44 48 15]   [31. 38. 43. 47. 48. 49.]   [48]\n",
      "20181105   [ 3  8 14 28 43 49 26]   [31. 38. 43. 47. 48. 49.]   [43 49]\n",
      "20181108   [ 8 13 16 26 28 38 46]   [31. 38. 43. 47. 48. 49.]   [38]\n",
      "20181112   [ 4 12 21 34 41 47 33]   [31. 38. 43. 47. 48. 49.]   [47]\n",
      "20181115  Predicted:  [31. 38. 43. 47. 48. 49.]  \n",
      "20181119  Predicted:  [31. 38. 43. 47. 48. 49.]  \n",
      "20181122  Predicted:  [31. 38. 43. 47. 48. 49.]  \n",
      "20181126  Predicted:  [31. 38. 43. 47. 48. 49.]  \n",
      "20181129  Predicted:  [31. 38. 43. 47. 48. 49.]  \n",
      "20181203  Predicted:  [31. 38. 43. 47. 48. 49.]  \n",
      "20181206  Predicted:  [31. 38. 43. 47. 48. 49.]  \n",
      "20181210  Predicted:  [31. 38. 43. 47. 48. 49.]  \n",
      "20181213  Predicted:  [31. 38. 43. 47. 48. 49.]  \n",
      "20181217  Predicted:  [31. 38. 43. 47. 48. 49.]  \n",
      "20181220  Predicted:  [31. 38. 43. 47. 48. 49.]  \n",
      "20181224  Predicted:  [31. 38. 43. 47. 48. 49.]  \n",
      "20181227  Predicted:  [31. 38. 43. 47. 48. 49.]  \n",
      "20181231  Predicted:  [31. 38. 43. 47. 48. 49.]  \n"
     ]
    }
   ],
   "source": [
    "def getAllData(df):\n",
    "    drop_cols = ['T', 'L','M','S','R','E','A','V' ,'J','U','K']\n",
    "    X = df.drop(drop_cols, axis=1)\n",
    "    return X\n",
    "\n",
    "test_data = mtr.get_test_data()\n",
    "X = mtr.modified_dataset(getAllData(test_data)) #\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "Z = scaler.transform(X)\n",
    "\n",
    "Z = X\n",
    "#X = mtr.modified_dataset(getAdjustedDataF(test_data,f))\n",
    "predictions = multi_target_sgd.predict(Z)\n",
    "dfResult= pd.DataFrame(predictions, columns=['N1', 'N2', 'N3', 'N4', 'N5','N6', 'N7'])\n",
    "mtr.print_predictions(dfResult)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "20180514   [17 24 29 45 46 49  5]   [25.  5. 35. 19. 45. 28.]   [ 5 45]\n",
    "20180517   [ 7 21 25 29 35 37 13]   [22.  5. 26. 17. 22. 30.]   []\n",
    "20180521   [ 8 10 16 30 37 44 17]   [25.  5. 10. 19. 20. 30.]   [10 30]\n",
    "20180524   [11 25 26 34 36 42 16]   [22.  5. 10. 12. 45. 30.]   []\n",
    "20180528   [ 5  9 27 28 30 44  2]   [22.  5. 35. 41. 22. 28.]   [ 5 28]\n",
    "20180531   [11 13 24 26 47 49 33]   [22. 35. 35. 17. 22. 30.]   []\n",
    "20180604   [20 22 31 37 43 45 27]   [22. 31. 35. 20. 22. 26.]   [20 22 31]\n",
    "20180607   [12 20 29 31 37 39 42]   [24. 31. 35. 20. 16. 30.]   [20 31]\n",
    "20180611   [16 25 30 37 44 49 34]   [24. 36. 10. 12. 16. 30.]   [16 30]\n",
    "20180614   [ 4 29 31 35 42 48  1]   [22.  5. 10. 44. 22. 28.]   []\n",
    "20180618   [11 15 22 23 26 43 25]   [22. 36. 36. 17. 22. 27.]   [22]\n",
    "20180621   [ 4  6 15 24 30 35 46]   [22.  7. 37. 15. 19. 34.]   [15]\n",
    "20180625   [ 2  5 25 38 44 48  9]   [22.  7. 37. 43. 48. 30.]   [48]\n",
    "20180628   [ 2  7 22 27 40 47 48]   [22. 32. 37. 15. 19. 29.]   [22]\n",
    "20180702   [12 13 26 33 35 38 23]   [22. 33. 35. 41. 19. 28.]   [33 35]\n",
    "20180705   [ 8 11 28 30 32 34 39]   [24. 33. 35. 15. 16. 30.]   [30]\n",
    "20180709   [ 6 23 31 38 39 43 33]   [23. 28. 35. 16. 20. 30.]   [23]\n",
    "20180712   [ 4 15 25 32 40 41 10]   [22.  5. 35. 43. 45. 28.]   []\n",
    "20180716   [ 4  8 19 24 32 47 22]   [22. 30. 35. 16. 20. 30.]   [22]\n",
    "20180719   [13 14 23 35 37 46 45]   [22. 31. 35. 43. 20. 30.]   [35]\n",
    "20180723   [ 2 23 26 28 39 40 12]   [22. 31. 33. 16. 20. 28.]   [28]\n",
    "20180726   [ 1  9 13 17 28 40 37]   [25. 31. 12. 43. 20. 28.]   [28]\n",
    "20180730   [ 8 10 19 20 41 43  7]   [25.  5.  8. 12. 21. 28.]   [8]\n",
    "20180802   [ 1 10 15 27 41 46 35]   [23. 32. 35. 43. 20. 30.]   [35]\n",
    "20180806   [ 7 18 20 27 36 40 15]   [25.  5. 37. 43. 20. 47.]   [20]\n",
    "20180809   [13 16 20 23 39 42 28]   [23.  5. 12. 43. 48. 28.]   [23 28]\n",
    "20180813   [ 1  3  6 16 22 36 17]   [25.  5. 37. 43. 46. 30.]   []\n",
    "20180816   [22 23 25 32 33 36 20]   [25. 15. 10. 43. 19. 48.]   [25]\n",
    "20180820   [ 9 10 25 38 40 42  2]   [25. 29. 38. 43. 48. 30.]   [25 38]\n",
    "20180823   [ 2  3 23 30 39 41 19]   [22.  5. 35. 43. 48. 30.]   [30]\n",
    "20180827   [ 5  6 16 24 26 29 38]   [22.  5. 35. 43. 48. 30.]   [5]\n",
    "20180830   [ 3  9 27 29 31 40 46]   [23.  5. 12. 43. 20. 28.]   []\n",
    "20180903   [ 4  5 13 18 39 40  3]   [22.  5. 12. 43. 48. 30.]   [5]\n",
    "20180906   [ 2 15 17 20 23 30 45]   [23. 31. 36. 43. 48. 28.]   [23]\n",
    "20180910   [ 2  6  9 15 40 43 18]   [22.  5. 37. 43. 48. 28.]   [43]\n",
    "20180913   [ 6 16 17 40 44 48 34]   [23.  5. 37. 43. 48. 28.]   [48]\n",
    "20180917   [16 21 22 24 25 27  1]   [22.  5. 12. 43. 48. 28.]   [22]\n",
    "20180920   [ 5 12 18 30 32 38 22]   [23.  5. 37. 43. 46. 28.]   [5]\n",
    "20180924   [ 6  8 17 24 29 47 34]   [22.  5. 37. 43. 48. 28.]   []\n",
    "20180927   [ 2 25 29 33 42 45 20]   [22.  5. 37. 43. 48. 28.]   []\n",
    "20181001   [11 15 23 24 32 40 43]   [22.  5. 37. 43. 46. 28.]   [43]\n",
    "20181004   [ 5 12 23 32 37 42 43]   [23.  5. 37. 43. 20. 28.]   [ 5 23 37 43]\n",
    "20181008   [17 18 23 39 43 49  2]   [23.  5. 36. 43. 48. 28.]   [23 43]\n",
    "20181011   [ 1 16 18 24 29 46 35]   [ 2.  5. 36. 43. 22. 28.]   []\n",
    "20181015   [ 1  4 24 32 35 48 20]   [23. 31. 12. 43. 48. 28.]   [48]\n",
    "20181018   [ 5 14 17 31 46 48 47]   [23. 31. 12. 43. 48. 25.]   [31 48]\n",
    "20181022   [ 5 22 24 40 43 48  2]   [23. 31. 12. 43. 48. 28.]   [43 48]\n",
    "20181025   [ 7  8 13 15 35 48 30]   [28. 31. 12. 43. 48. 28.]   [48]\n",
    "20181029   [ 2  6 10 20 28 31 30]   [24.  5. 37. 43. 48. 28.]   [28]\n",
    "20181101   [ 6 27 28 41 44 48 15]   [24. 31. 38. 43. 46. 28.]   [28]\n",
    "20181105   [ 3  8 14 28 43 49 26]   [ 2.  5. 37. 43. 46. 28.]   [28 43]\n",
    "20181108   [ 8 13 16 26 28 38 46]   [23.  5. 37. 43. 48. 28.]   [28]\n",
    "20181112   [ 4 12 21 34 41 47 33]   [23. 31. 12. 43. 48. 28.]   [12]\n",
    "20181115  Predicted:  [23. 28. 37. 43. 48. 28.]  \n",
    "20181119  Predicted:  [23.  5. 37. 44. 46. 28.]  \n",
    "20181122  Predicted:  [23.  5. 37. 43. 48. 28.]  \n",
    "20181126  Predicted:  [23.  5. 37. 43. 22. 28.]  \n",
    "20181129  Predicted:  [21.  5. 36. 43. 22. 28.]  \n",
    "20181203  Predicted:  [23. 31. 37. 43. 46. 28.]  \n",
    "20181206  Predicted:  [23.  5. 37. 43. 48. 28.]  \n",
    "20181210  Predicted:  [23.  5. 37. 43. 48. 28.]  \n",
    "20181213  Predicted:  [23.  5. 37. 43. 48. 28.]  \n",
    "20181217  Predicted:  [23.  5. 37. 43. 46. 28.]  \n",
    "20181220  Predicted:  [22. 31. 37. 43. 46. 28.]  \n",
    "20181224  Predicted:  [22.  5. 37. 43. 46. 28.]  \n",
    "20181227  Predicted:  [22.  5. 37. 43. 48. 28.]  \n",
    "20181231  Predicted:  [22.  5. 38. 43. 46. 28.]  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

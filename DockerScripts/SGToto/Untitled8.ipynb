{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import rcParams\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Variable\n",
    "v_input_datafile = '../input/train.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data as Pandas dataframe\n",
    "# Optional Parametrs: header=None, names=col_names, sep='\\s+', na_values=['?']\n",
    "# col_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model year', 'origin', 'car name']\n",
    "input_df = pd.read_csv(v_input_datafile, )\n",
    "input_df.info()\n",
    "\n",
    "#Pick features\n",
    "features = df.columns[(df.columns != target) & \n",
    "                      (df.columns != 'day') &\n",
    "                      (df.columns != 'month')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop NA\n",
    "#df[df.isna().any(axis=1)]\n",
    "\n",
    "# df.isna().any(axis=1) returns True if a row has missing values\n",
    "#\n",
    "# mask = df.isna().any(axis=1)\n",
    "# df[mask] return only the True rows (masking)\n",
    "\n",
    "\n",
    "print( \"Before dropping NA: \", input_df.shape )\n",
    "df = input_df.dropna()\n",
    "print ( \"After dropping NA: \", df.shape )\n",
    "\n",
    "#Drop Columns\n",
    "drop_lst = ['casual', 'registered']\n",
    "df = df.drop(drop_lst, axis=1)\n",
    "df.head()\n",
    "\n",
    "#Convert date time\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df['dow'] = df['datetime'].dt.dayofweek\n",
    "df['year'] = df['datetime'].dt.year\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df['week'] = df['datetime'].dt.week\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['day'] = df['datetime'].dt.day\n",
    "df = df.set_index(df['datetime'])\n",
    "df = df.drop(labels='datetime', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each feature list down the unique values\n",
    "#df['Title'].value_counts()\n",
    "# df['horsepower']=pd.to_numeric(df['horsepower'], errors='coerce')  #Convert non numeric values to NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#side by side comparison of 2011 and 2012 data\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "names = ['1', '2', '3', '4']\n",
    "\n",
    "values = df['season'][df['year'] == 2011].value_counts()\n",
    "ax[0].bar(names, values)\n",
    "\n",
    "values = df['season'][df['year'] == 2012].value_counts()\n",
    "ax[1].bar(names, values)\n",
    "\n",
    "fig.suptitle('Seasons in 2011 & 2012');\n",
    "\n",
    "#Plot value Counts\n",
    "names = ['2011', '2012']\n",
    "values = df['year'].value_counts()\n",
    "plt.bar(names, values);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot both on same chart\n",
    "plt.hist(df['temp'][df['year'] == 2011], alpha=0.5, label='2011')\n",
    "plt.hist(df['temp'][df['year'] == 2012], alpha=0.5, label='2012')\n",
    "\n",
    "plt.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Data\n",
    "sns.pairplot(df, kind='reg')\n",
    "#Plot Histogram\n",
    "plt.hist(df['count']);\n",
    "\n",
    "\n",
    "count_log = np.log(df['count'])\n",
    "plt.hist(count_log);\n",
    "\n",
    "#To Normalize\n",
    "count_boxcox, _ = stats.boxcox(df['count'])\n",
    "count_boxcox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(x=df['temp'], y=df['count'])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(30,12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(df.corr(), annot=True, fmt='.2f', ax=ax)\n",
    "\n",
    "# integer: 'd'\n",
    "# floating point: 'f'\n",
    "# floating point with 3 decimal places: '.3f'\n",
    "\n",
    "\n",
    "#Another Heatmap\n",
    "cor_mat = df[:].corr()\n",
    "mask = np.array(cor_mat)\n",
    "mask[np.tril_indices_from(mask)] = False\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(30,12)\n",
    "sns.heatmap(data=cor_mat, mask=mask, square=True, annot=True, cbar=True);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Should Interpoloate NAN\n",
    "df1 = df.interpolate() # finds all NaN values and linear interp\n",
    "df1.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpolate\n",
    "\n",
    "#Get index of all rows which contains NAN\n",
    "index = df[df.isna().any(axis=1)].index\n",
    "df1.iloc[index]\n",
    "\n",
    "# plot before and after\n",
    "\n",
    "# 1. plot original df horsepower\n",
    "# 2. add the interpolated values\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(df['horsepower'].index, df['horsepower'])\n",
    "ax.set(xlabel='index', ylabel='horsepower')\n",
    "\n",
    "ax.scatter(index, df1.iloc[index]['horsepower'], label='interp')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "# Goal: plot all 8 features at the same time with target (y)\n",
    "# PCA is a technique to reduce to 2 or 3 dimensions\n",
    "\n",
    "# Principal Component Analysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# make sure df is cleaned\n",
    "df.dropna(inplace=True)\n",
    "X = df.loc[:, df.columns != 'mpg']\n",
    "y = df['mpg']\n",
    "\n",
    "# best practice is to scale before performing PCA\n",
    "# (because PCA uses covariance which is sensitive to scaling)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "pca = PCA(n_components=1) # X in 1-d so we can plot X vs y in 2-d\n",
    "pca.fit(X_scaled) # setup to compute PCA\n",
    "\n",
    "Z = pca.transform(X_scaled) # actually perform PCA\n",
    "\n",
    "# dimension is 1 only because we want to plot\n",
    "print('before', X.shape, 'after', Z.shape)\n",
    "Z\n",
    "\n",
    "# plot Z vs. y to explore relationship\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(Z, y)\n",
    "ax.set(title='PCA of data vs. mpg',\n",
    "      xlabel='1-d PCA projection of data',\n",
    "      ylabel='mpg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot 3D\n",
    "%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "pca2 = PCA(n_components=2)\n",
    "pca2.fit(X_scaled)\n",
    "Z2 = pca2.transform(X_scaled)\n",
    "\n",
    "# plot Z2[0], Z2[1] vs y in 3-d\n",
    "ax.scatter(Z2[:, 0], Z2[:, 1], y)\n",
    "ax.set_xlabel('dimension 0')\n",
    "ax.set_ylabel('dimension 1')\n",
    "ax.set_zlabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#swarm plot\n",
    "sns.swarmplot(x='Sex', y='Fare', hue='Survived', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing Fare values using median of Pclass groups\n",
    "class_fares = dict(df.groupby('Pclass')['Fare'].median())\n",
    "\n",
    "# create a column of the average fares\n",
    "df['fare_med'] = df['Pclass'].apply(lambda x: class_fares[x])\n",
    "\n",
    "# replace all missing fares with the value in this column\n",
    "df['Fare'].fillna(df['fare_med'], inplace=True, )\n",
    "del df['fare_med']\n",
    "\n",
    "#imput using  'backfill' method.\n",
    "sns.catplot(x='Embarked', y='Survived', data=df,\n",
    "            kind='bar', palette='muted', ci=None)\n",
    "plt.show()\n",
    "df['Embarked'].fillna(method='backfill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to reduce coding\n",
    "def plot_learning_curve(model):\n",
    "    # 3-fold cross validation to get learning curve R2 scores\n",
    "    # using default train_sizes\n",
    "    train_sizes, train_scores, val_scores = learning_curve(model,\n",
    "                                                           Z_train, \n",
    "                                                           y_train, cv=3)\n",
    "\n",
    "    # plot learning curve:\n",
    "    #   plot train_scores vs. train_sizes\n",
    "    #   plot val_scores vs. train_sizes\n",
    "    # train_sizes is the number of training samples used for training\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(train_sizes, train_scores.mean(axis=1),\n",
    "            label='train') # average for 5-folds\n",
    "    ax.plot(train_sizes, val_scores.mean(axis=1),\n",
    "            label='val')\n",
    "    ax.legend()\n",
    "    ax.set(title='Learning curve', xlabel='Train size', ylabel='R2')\n",
    "    return train_sizes, train_scores, val_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGDRegressor.learning_rate\n",
    "# default was 'invscaling'\n",
    "# try: 'constant' with eta0=1e-1, 'constant' with eta0=1e-4,\n",
    "# 'optimal' with default eta0\n",
    "\n",
    "# - make change\n",
    "model = SGDRegressor(max_iter=1000, tol=1e-3,\n",
    "                     random_state=8,\n",
    "                     learning_rate='optimal')\n",
    "\n",
    "# - cross validate (trains models)\n",
    "scores = cross_validate(model, Z_train, y_train, cv=5,\n",
    "                        return_train_score=True, return_estimator=True)\n",
    "# scores\n",
    "\n",
    "# - learning curve (plot for overfit / underfit)\n",
    "train_sizes, train_scores, val_scores = plot_learning_curve(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = []\n",
    "\n",
    "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()), ('LR', LinearRegression())])))\n",
    "pipelines.append(('ScaledLASSO', Pipeline([('poly', PolynomialFeatures()), ('Scaler', StandardScaler()), ('LASSO', Lasso(random_state=42))])))\n",
    "pipelines.append(('ScaledRID', Pipeline([('poly', PolynomialFeatures()), ('Scaler', StandardScaler()), ('RID', Ridge(random_state=42))])))\n",
    "pipelines.append(('ScaledKNN', Pipeline([('poly', PolynomialFeatures()), ('Scaler', StandardScaler()), ('KNN', KNeighborsRegressor(n_neighbors=2))])))\n",
    "pipelines.append(('ScaledCART', Pipeline([('poly', PolynomialFeatures()), ('Scaler', StandardScaler()), ('CART', DecisionTreeRegressor(random_state=42))])))\n",
    "pipelines.append(('ScaledGBM', Pipeline([('poly', PolynomialFeatures()), ('Scaler', StandardScaler()), ('GBM', GradientBoostingRegressor(random_state=42))])))\n",
    "pipelines.append(('ScaledRFR', Pipeline([('poly', PolynomialFeatures()), ('Scaler', StandardScaler()), ('RFR', RandomForestRegressor(random_state=42))])))\n",
    "pipelines.append(('ScaledSVR', Pipeline([('poly', PolynomialFeatures()), ('Scaler', StandardScaler()), ('SVR', SVR(kernel='linear'))])))\n",
    "pipelines.append(('ScaledXGBR', Pipeline([('poly', PolynomialFeatures()), ('Scaler', StandardScaler()), ('XGBR', XGBRegressor(random_state=42))])))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfold = KFold(random_state=42)\n",
    "    cv_results = -cross_val_score(model, X_train, y_train, cv=kfold, scoring='neg_mean_squared_log_error')\n",
    "    results.append(np.sqrt(cv_results))\n",
    "    names.append(name)\n",
    "    msg = \"{}: {} ({})\".format(name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "labels = ...\n",
    "predictions = ...\n",
    "\n",
    "cm = confusion_matrix(labels, predictions)\n",
    "recall = np.diag(cm) / np.sum(cm, axis = 1)\n",
    "precision = np.diag(cm) / np.sum(cm, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

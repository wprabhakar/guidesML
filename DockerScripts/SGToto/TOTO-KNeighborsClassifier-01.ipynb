{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cores:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walter/Software/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n",
      "/Users/walter/Software/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:923: DeprecationWarning: builtin type EagerTensor has no __module__ attribute\n",
      "  EagerTensor = c_api.TFE_Py_InitEagerTensor(_EagerTensorBase)\n",
      "/Users/walter/Software/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  return _inspect.getargspec(target)\n",
      "/Users/walter/Software/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/Users/walter/Software/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:4785: ResourceWarning: unclosed file <_io.TextIOWrapper name='/Users/walter/.keras/keras.json' mode='r' encoding='UTF-8'>\n",
      "  _config = json.load(open(_config_path))\n",
      "/Users/walter/Software/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/Users/walter/Software/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "#!conda install -n mldds -c anaconda joblib\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "import multiprocessing\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "print(\"Cores: \", num_cores)\n",
    "\n",
    "import time\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto( device_count = {'GPU': 0 , 'CPU': num_cores} )\n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:138: DeprecationWarning: invalid escape sequence \\s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import utils, preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from matplotlib.pyplot import figure\n",
    "from functools import reduce\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MyTotoResearch:\n",
    "    \n",
    "    @classmethod\n",
    "    def __init__(self, algo_no=0, inputPPFile='../input/PPv3.csv', inputTotoResult='../input/SGH.csv'):\n",
    "        self.algo_number = algo_no\n",
    "        print('Loaded MyTotoResearch algo_no: ', self.algo_number)\n",
    "\n",
    "    @classmethod\n",
    "    def load_totodata(self, inputPPFile='../input/PPv3.csv', inputTotoResult='../input/SGH.csv'):\n",
    "        pp = pd.read_csv(inputPPFile)\n",
    "        lr = pd.read_csv(inputTotoResult)\n",
    "        print(len(lr))\n",
    "        cols = ['D', 'N1','N2','N3','N4','N5','N6','N7']\n",
    "        lr = lr[cols]\n",
    "\n",
    "        #https://pandas.pydata.org/pandas-docs/stable/merging.html\n",
    "        df = pd.concat([pp, lr], axis=1, sort=False)\n",
    "        df = df.dropna()\n",
    "        df.reset_index().drop(['D'], axis=1)\n",
    "\n",
    "#         print(df.shape)\n",
    "#         counts = [df['N'+str(i)].value_counts() for i in range(1,8)]\n",
    "#         for i in range(len(counts)):\n",
    "#             df = df[~df['N'+str(i+1)].isin(counts[i][counts[i] <= 3].index)]\n",
    "#         print('After removing numbers that have shown 3 or less times')\n",
    "#         print(df.shape)\n",
    "\n",
    "        cols = ['N1','N2','N3','N4','N5','N6','N7']\n",
    "        lr = df[cols]\n",
    "\n",
    "        self.df = df\n",
    "        self.lresult = np.sort(lr.values[:, ::-1])\n",
    "        return self.lresult, self.df \n",
    "    \n",
    "    @classmethod\n",
    "    def modified_dataset ( self, dataset ):\n",
    "        self.dataset = dataset\n",
    "        return self.dataset\n",
    "    \n",
    "    @classmethod\n",
    "    def get_result_n(self, col_n):\n",
    "        aa = np.delete(self.lresult, np.s_[col_n:], axis=1)  \n",
    "        aa = np.delete(aa, np.s_[0:col_n-1], axis=1)  \n",
    "        return pd.DataFrame(aa, columns=list('N'))\n",
    "\n",
    "    @classmethod\n",
    "    def get_result_n_encoded(self, col_n):\n",
    "        aa = np.delete(self.lresult, np.s_[col_n:], axis=1)  \n",
    "        aa = np.delete(aa, np.s_[0:col_n-1], axis=1)  \n",
    "        # 1. INSTANTIATE\n",
    "        enc = preprocessing.OneHotEncoder()\n",
    "\n",
    "        # 2. FIT\n",
    "        enc.fit(aa)\n",
    "\n",
    "        # 3. Transform\n",
    "        onehotlabels = enc.transform(aa).toarray()\n",
    "        onehotlabels.shape\n",
    "        #print(onehotlabels)\n",
    "\n",
    "        #Convert 2d array to Dataframe\n",
    "        y = pd.DataFrame(aa, columns=list('N'))\n",
    "        y.head()\n",
    "        y = aa.astype(int).ravel()\n",
    "    #    print ( y )\n",
    "        return y\n",
    "        \n",
    "    @classmethod\n",
    "    def get_test_data(self, file_name = '../input/PPv3-Predict.csv' ):\n",
    "        self.data2Predict = pd.read_csv(file_name)\n",
    "        self.data2Predict.reset_index()\n",
    "        return self.data2Predict\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def plot_history(self, history):\n",
    "        loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
    "        val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
    "        acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n",
    "        val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n",
    "\n",
    "        if len(loss_list) == 0:\n",
    "            print('Loss is missing in history')\n",
    "            return \n",
    "\n",
    "        ## As loss always exists\n",
    "        epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
    "\n",
    "        ## Loss\n",
    "        figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "        plt.figure(1)\n",
    "        for l in loss_list:\n",
    "            plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "        for l in val_loss_list:\n",
    "            plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "\n",
    "        plt.title('Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        ## Accuracy\n",
    "        plt.figure(1)\n",
    "        for l in acc_list:\n",
    "            plt.plot(epochs, history.history[l], 'r', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "        for l in val_acc_list:    \n",
    "            plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "\n",
    "        plt.title('Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    @classmethod\n",
    "    def save_model(self, model, predict_number):\n",
    "        # serialize model to JSON\n",
    "        model_json = model.to_json()\n",
    "        with open(str(self.algo_number) + '_' + str(predict_number) + \"_model.json\", \"w\") as json_file:\n",
    "            json_file.write(simplejson.dumps(simplejson.loads(model_json), indent=4))\n",
    "\n",
    "        # serialize weights to HDF5\n",
    "        model.save_weights(str(self.algo_number) + '_' + str(predict_number) + \"_model.h5\")\n",
    "        print(\"Saved model to disk \", self.algo_number, \" Predict #N \", predict_number)\n",
    "\n",
    "    @classmethod\n",
    "    def print_result(self, predicted_values ):\n",
    "        test_df = pd.read_csv('../input/TestResult.csv', sep='\\s+', header=None, names=['D','N1','N2','N3','N4','N5','N6','N7'])\n",
    "        test_df['D'].replace(regex=True,inplace=True,to_replace=r'-',value=r'')\n",
    "        test_df['D'] = pd.to_numeric(test_df['D'])\n",
    "\n",
    "        cols = ['D', 'N1','N2','N3','N4','N5','N6','N7']\n",
    "        test_df = self.data2Predict.merge(test_df, left_on='T', right_on='D', how='inner')\n",
    "        test_df = test_df[cols]\n",
    "\n",
    "        tdfResult = predicted_values.drop(predicted_values.columns[0], axis=1) ;\n",
    "\n",
    "        actual_result = test_df[cols[1:]].values\n",
    "        predicted_result = tdfResult.values\n",
    "\n",
    "        matched = getIntersection(actual_result, predicted_result)\n",
    "\n",
    "        c = 0\n",
    "        for i in range(len(matched)):\n",
    "            print(int(self.data2Predict.loc[i]['T']), ' ', actual_result[i], ' ', predicted_result[i], ' ', matched[c])\n",
    "            c += 1\n",
    "        for i in range(c, len(predicted_result)):\n",
    "            print(int(self.data2Predict.loc[i]['T']), ' Predicted: ', predicted_result[i], ' ')\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(self, predict_number):\n",
    "        json_file = open(str(self.algo_number) + \"_\" + str(predict_number)+'_model.json', 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "        # load weights into new model\n",
    "        loaded_model.load_weights(str(self.algo_number) + \"_\" + str(predict_number)+\"_model.h5\")\n",
    "        print(\"Loaded model from disk \" + str(self.algo_number) + \"_\" + str(predict_number) + \"_model\" )\n",
    "        return loaded_model\n",
    "\n",
    "    @classmethod\n",
    "    def getTargets(self):\n",
    "        return np.array([self.get_result_n(i)['N'] for i in range(1,8)]).T\n",
    "\n",
    "    @classmethod\n",
    "    def getTarget(self, N):\n",
    "        return ([self.get_result_n(i)['N'] for i in range(1,8)])[N-1]\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def getIntersection(p1, p2):\n",
    "        return [reduce(np.intersect1d, (p.astype(int), a.astype(int))) for (p,a) in zip(p1, p2)]\n",
    "\n",
    "    @classmethod\n",
    "    def print_predictions(self, dfPredictions, result='../input/TestResult.csv'):\n",
    "        #load the test Toto Results files\n",
    "        test_df = pd.read_csv(result, sep='\\s+', header=None, names=['D','N1','N2','N3','N4','N5','N6','N7'])\n",
    "        test_df['D'].replace(regex=True,inplace=True,to_replace=r'-',value=r'')\n",
    "        test_df['D'] = pd.to_numeric(test_df['D'])\n",
    "\n",
    "        #Merge the Planet Position File with the Toto Results file\n",
    "        test_df = self.data2Predict.merge(test_df, left_on='T', right_on='D', how='inner')\n",
    "\n",
    "        #Extract only the Results for all dates\n",
    "        cols = ['D', 'N1','N2','N3','N4','N5','N6','N7']\n",
    "        test_df = test_df[cols]\n",
    "\n",
    "        tdfResult = dfPredictions.drop(dfPredictions.columns[0], axis=1) ;\n",
    "\n",
    "        actual_result = test_df[cols[1:]].values\n",
    "        predicted_result = tdfResult.values\n",
    "\n",
    "        matched = MyTotoResearch.getIntersection(actual_result, predicted_result)\n",
    "\n",
    "        c = 0\n",
    "        for i in range(len(matched)):\n",
    "            print(int(self.data2Predict.loc[i]['T']), ' ', actual_result[i], ' ', predicted_result[i], ' ', matched[c])\n",
    "            c += 1\n",
    "        for i in range(c, len(predicted_result)):\n",
    "            print(int(self.data2Predict.loc[i]['T']), ' Predicted: ', predicted_result[i], ' ')\n",
    "\n",
    "            \n",
    "def getAdjustedDataF(df,f):\n",
    "    #Use only Planet Positions Testing\n",
    "    cols = ['L','M','S', 'R','E','A','V' ,'J','U','K']\n",
    "    X = df[cols]\n",
    "    deg = f\n",
    "    \n",
    "#     X['S_3'] = X['S'] // (deg*3)\n",
    "#     X['L_3'] = X['L'] // (deg*3)\n",
    "#     X['M_3'] = X['M'] // (deg*3)\n",
    "#     X['R_3'] = X['R'] // (deg*3)\n",
    "#     X['E_3'] = X['E'] // (deg*3)\n",
    "#     X['A_3'] = X['A'] // (deg*3)\n",
    "#     X['V_3'] = X['V'] // (deg*3)\n",
    "#     X['J_3'] = X['J'] // (deg*3)\n",
    "#     X['U_3'] = X['U'] // (deg*3)\n",
    "\n",
    "\n",
    "#     X['S_2'] = X['S'] // (deg*2)\n",
    "#     X['L_2'] = X['L'] // (deg*2)\n",
    "#     X['M_2'] = X['M'] // (deg*2)\n",
    "#     X['R_2'] = X['R'] // (deg*2)\n",
    "#     X['E_2'] = X['E'] // (deg*2)\n",
    "#     X['A_2'] = X['A'] // (deg*2)\n",
    "#     X['V_2'] = X['V'] // (deg*2)\n",
    "#     X['J_2'] = X['J'] // (deg*2)\n",
    "#     X['U_2'] = X['U'] // (deg*2)\n",
    "\n",
    "    X['S_1'] = X['S'] // (deg)\n",
    "    X['L_1'] = X['L'] // (deg)\n",
    "    X['M_1'] = X['M'] // (deg)\n",
    "    X['R_1'] = X['R'] // (deg)\n",
    "    X['E_1'] = X['E'] // (deg)\n",
    "    X['A_1'] = X['A'] // (deg)\n",
    "    X['V_1'] = X['V'] // (deg)\n",
    "    X['J_1'] = X['J'] // (deg)\n",
    "    X['U_1'] = X['U'] // (deg)\n",
    "   \n",
    "    X = X.drop(cols, axis=1)\n",
    "    return X\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded MyTotoResearch algo_no:  1\n",
      "1521\n",
      "(1521, 7)\n",
      "0.0\n",
      " Time taken:  4.7999999999603915e-05  \n"
     ]
    }
   ],
   "source": [
    "from keras.models import Input, Model\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, History\n",
    "import json as simplejson\n",
    "from keras import regularizers\n",
    "from sklearn import preprocessing\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDRegressor, SGDClassifier, PassiveAggressiveClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def getAllData(df):\n",
    "    drop_cols = ['T', 'D', 'N1','N2','N3','N4','N5','N6','N7','L','M','S','R','E','A','V' ,'J','U','K']\n",
    "    X = df.drop(drop_cols, axis=1)\n",
    "    return X\n",
    "\n",
    "\n",
    "#early_stopping = MyEarlyStopping ( threshold=.99999, monitor='acc', verbose=1 )\n",
    "#early_stopping_loss = MyEarlyStoppingLoss ( threshold=.005, monitor='loss', verbose=1 )\n",
    "\n",
    "#Deep Neuro Network\n",
    "mtr = MyTotoResearch(algo_no=1)\n",
    "lresult, df = mtr.load_totodata()\n",
    "X = mtr.modified_dataset(getAllData(df)) #\n",
    "#target = np.array([mtr.get_result_n(i)['N'] for i in range(1,8)]).T\n",
    "target = mtr.getTargets() ;\n",
    "#print(target)\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X)\n",
    "Z = scaler.transform(X)\n",
    "                 \n",
    "#Z = X\n",
    "\n",
    "#sgd = SGDClassifier(max_iter=200, tol=1e-3, random_state=42, penalty=\"l2\")\n",
    "sgd =  PassiveAggressiveClassifier(max_iter=12000, random_state=42, tol=0.000001, C=0.001, loss='hinge')\n",
    "#loss=\"log\", alpha=0.001, penalty=\"l2\", max_iter=2000, tol=1e-3, random_state=42)\n",
    "\n",
    "multi_target_sgd = MultiOutputClassifier(sgd, n_jobs=7)\n",
    "print(target.shape)\n",
    "multi_target_sgd.fit(Z, mtr.getTargets()) \n",
    "print(multi_target_sgd.score(Z, mtr.getTargets()))\n",
    "\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "#print(\"Accuracy:\",metrics.accuracy_score(mtr.getTargets(), y_pred))\n",
    "\n",
    "# multi_target_sgd = sgd\n",
    "# multi_target_sgd.fit(Z, mtr.getTarget(0)) \n",
    "# print(multi_target_sgd.score(Z, mtr.getTarget(0)))\n",
    "\n",
    "start = time.clock()\n",
    "#history = multi_target_sgd.fit(X, target) #@, epochs=12000, verbose=0) #, callbacks=[early_stopping]) #, callbacks=[early_stopping]) #, checkpoint])    \n",
    "print(\" Time taken: \", (time.clock() - start),  \" \")\n",
    "\n",
    "# n_iters = [100,2000,5000,10000]\n",
    "# scores = []\n",
    "# for n_iter in n_iters:\n",
    "#     model = SGDClassifier(loss=\"log\", alpha=0.001, penalty=\"l2\", max_iter=n_iter, tol=1e-6, random_state=42)\n",
    "#     model.fit(Z, mtr.getTarget(0))\n",
    "#     scores.append(model.score(Z, mtr.getTarget(0)))\n",
    "  \n",
    "# plt.title(\"Effect of n_iter\")\n",
    "# plt.xlabel(\"n_iter\")\n",
    "# plt.ylabel(\"score\")\n",
    "# plt.plot(n_iters, scores) \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20180514   [17 24 29 45 46 49  5]   [10. 36. 34. 10. 45. 27.]   [45]\n",
      "20180517   [ 7 21 25 29 35 37 13]   [10.  6. 34. 38. 29. 27.]   [29]\n",
      "20180521   [ 8 10 16 30 37 44 17]   [16.  6. 34. 10. 29. 27.]   [10 16]\n",
      "20180524   [11 25 26 34 36 42 16]   [10.  6. 34. 38. 45. 27.]   [34]\n",
      "20180528   [ 5  9 27 28 30 44  2]   [10.  6. 34. 10. 29. 27.]   [27]\n",
      "20180531   [11 13 24 26 47 49 33]   [14.  6. 34. 17. 29. 44.]   []\n",
      "20180604   [20 22 31 37 43 45 27]   [14. 35. 14. 38. 29. 44.]   []\n",
      "20180607   [12 20 29 31 37 39 42]   [14. 35. 34. 36. 32. 27.]   []\n",
      "20180611   [16 25 30 37 44 49 34]   [14. 36. 28. 36. 29. 44.]   [44]\n",
      "20180614   [ 4 29 31 35 42 48  1]   [16. 35. 14. 38. 29. 44.]   [29 35]\n",
      "20180618   [11 15 22 23 26 43 25]   [10.  6. 14. 17. 22. 27.]   [22]\n",
      "20180621   [ 4  6 15 24 30 35 46]   [ 6. 11. 21.  9. 31. 46.]   [ 6 46]\n",
      "20180625   [ 2  5 25 38 44 48  9]   [15. 18. 21. 27. 40. 40.]   []\n",
      "20180628   [ 2  7 22 27 40 47 48]   [ 7. 11. 14. 32. 41. 46.]   [7]\n",
      "20180702   [12 13 26 33 35 38 23]   [24. 11. 19. 20. 33. 43.]   [33]\n",
      "20180705   [ 8 11 28 30 32 34 39]   [15. 11. 19. 15. 33. 43.]   [11]\n",
      "20180709   [ 6 23 31 38 39 43 33]   [23. 21. 27. 32. 39. 40.]   [23 39]\n",
      "20180712   [ 4 15 25 32 40 41 10]   [15. 26. 35. 24. 39. 37.]   [15]\n",
      "20180716   [ 4  8 19 24 32 47 22]   [24. 19. 33. 24. 33. 37.]   [19 24]\n",
      "20180719   [13 14 23 35 37 46 45]   [ 7. 11. 33. 24. 39. 38.]   []\n",
      "20180723   [ 2 23 26 28 39 40 12]   [ 7. 11. 33. 32. 39. 36.]   [39]\n",
      "20180726   [ 1  9 13 17 28 40 37]   [ 7. 26. 19.  9. 39. 36.]   [9]\n",
      "20180730   [ 8 10 19 20 41 43  7]   [ 7. 11.  8. 29. 33. 36.]   [7 8]\n",
      "20180802   [ 1 10 15 27 41 46 35]   [ 7. 25. 27. 29. 40. 37.]   [27]\n",
      "20180806   [ 7 18 20 27 36 40 15]   [13. 30. 16. 30. 35. 40.]   [40]\n",
      "20180809   [13 16 20 23 39 42 28]   [10. 16. 19. 30. 35. 40.]   [16]\n",
      "20180813   [ 1  3  6 16 22 36 17]   [ 7. 16. 14. 38. 41. 44.]   [16]\n",
      "20180816   [22 23 25 32 33 36 20]   [24. 18. 28. 37. 31. 45.]   []\n",
      "20180820   [ 9 10 25 38 40 42  2]   [19.  9. 22. 26. 31. 44.]   [9]\n",
      "20180823   [ 2  3 23 30 39 41 19]   [14. 11. 13. 20. 43. 45.]   []\n",
      "20180827   [ 5  6 16 24 26 29 38]   [14. 16. 14. 33. 36. 44.]   [16]\n",
      "20180830   [ 3  9 27 29 31 40 46]   [14. 18. 28. 31. 33. 45.]   [31]\n",
      "20180903   [ 4  5 13 18 39 40  3]   [10.  6. 22. 26. 31. 44.]   []\n",
      "20180906   [ 2 15 17 20 23 30 45]   [ 8. 13. 25. 25. 36. 45.]   [45]\n",
      "20180910   [ 2  6  9 15 40 43 18]   [11. 19. 14. 34. 32. 42.]   []\n",
      "20180913   [ 6 16 17 40 44 48 34]   [11. 18. 17. 26. 40. 44.]   [17 40 44]\n",
      "20180917   [16 21 22 24 25 27  1]   [11. 27. 17. 10. 31. 44.]   [27]\n",
      "20180920   [ 5 12 18 30 32 38 22]   [11. 18. 17. 30. 35. 40.]   [18 30]\n",
      "20180924   [ 6  8 17 24 29 47 34]   [12. 22. 19. 29. 41. 43.]   [29]\n",
      "20180927   [ 2 25 29 33 42 45 20]   [17. 18. 22. 26. 34. 44.]   []\n",
      "20181001   [11 15 23 24 32 40 43]   [11.  9. 17. 26. 40. 43.]   [11 40 43]\n",
      "20181004   [ 5 12 23 32 37 42 43]   [ 5. 21. 17. 30. 42. 42.]   [ 5 42]\n",
      "20181008   [17 18 23 39 43 49  2]   [31. 32.  5. 43. 42. 42.]   [43]\n",
      "20181011   [ 1 16 18 24 29 46 35]   [ 9. 21. 17. 12. 22. 37.]   []\n",
      "20181015   [ 1  4 24 32 35 48 20]   [ 9. 27. 12. 16. 40. 42.]   []\n",
      "20181018   [ 5 14 17 31 46 48 47]   [10. 34. 30. 11. 40. 45.]   []\n",
      "20181022   [ 5 22 24 40 43 48  2]   [ 9. 34. 14. 34. 41. 43.]   [43]\n",
      "20181025   [ 7  8 13 15 35 48 30]   [19. 19. 25. 34. 40. 37.]   []\n",
      "20181029   [ 2  6 10 20 28 31 30]   [ 7. 22. 17. 32. 41. 41.]   []\n",
      "20181101   [ 6 27 28 41 44 48 15]   [ 8. 16. 13. 32. 31. 45.]   []\n",
      "20181105   [ 3  8 14 28 43 49 26]   [ 8. 24. 17. 32. 33. 41.]   [8]\n",
      "20181108   [ 8 13 16 26 28 38 46]   [ 7. 22.  8. 32. 33. 42.]   [8]\n",
      "20181112   [ 4 12 21 34 41 47 33]   [ 7. 34. 25. 13. 40. 33.]   [33 34]\n",
      "20181115  Predicted:  [19. 25. 25. 27. 40. 33.]  \n",
      "20181119  Predicted:  [23. 37. 13. 28. 39. 38.]  \n",
      "20181122  Predicted:  [ 7. 21. 35. 29. 32. 36.]  \n",
      "20181126  Predicted:  [ 6. 12. 13. 33. 22. 45.]  \n",
      "20181129  Predicted:  [ 2. 12. 32. 18. 22. 45.]  \n",
      "20181203  Predicted:  [16. 16. 29. 36. 37. 45.]  \n",
      "20181206  Predicted:  [ 2. 17. 17. 21. 39. 45.]  \n",
      "20181210  Predicted:  [11. 19. 17. 23. 30. 38.]  \n",
      "20181213  Predicted:  [17. 12. 21. 33. 44. 45.]  \n",
      "20181217  Predicted:  [ 5. 19. 35. 28. 39. 45.]  \n",
      "20181220  Predicted:  [15. 14. 13. 33. 30. 44.]  \n",
      "20181224  Predicted:  [ 5. 21. 13. 21. 39. 45.]  \n",
      "20181227  Predicted:  [14. 27. 34. 38. 47. 44.]  \n",
      "20181231  Predicted:  [14. 17. 38. 17. 37. 44.]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def getAllData(df):\n",
    "    drop_cols = ['T', 'L','M','S','R','E','A','V' ,'J','U','K']\n",
    "    X = df.drop(drop_cols, axis=1)\n",
    "    return X\n",
    "\n",
    "test_data = mtr.get_test_data()\n",
    "X = mtr.modified_dataset(getAllData(test_data)) #\n",
    "\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X)\n",
    "Z = scaler.transform(X)\n",
    "\n",
    "#Z = X\n",
    "#X = mtr.modified_dataset(getAdjustedDataF(test_data,f))\n",
    "predictions = multi_target_sgd.predict(Z)\n",
    "dfResult= pd.DataFrame(predictions, columns=['N1', 'N2', 'N3', 'N4', 'N5','N6', 'N7'])\n",
    "mtr.print_predictions(dfResult)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 9 14 36 46 48\n",
    "5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

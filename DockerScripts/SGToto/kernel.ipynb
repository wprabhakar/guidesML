{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "091adb57df9a939c3e63b4833c731bd85f4d1d30"
   },
   "outputs": [],
   "source": [
    "    c.test=c.test+1\n",
    "    k=k+1\n",
    "class A:\n",
    "    def __init__(self):\n",
    "        self.test = 0\n",
    "\n",
    "def main():\n",
    "    Count=A()\n",
    "    k=0\n",
    " \n",
    "    for i in range(0,25):\n",
    "        add(Count,k)\n",
    "    print(\"Count.test=\", Count.test)\n",
    "    print(\"k =\", k)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "94075a64fbc110883ddd12bfb27a9ed7d3419c1c"
   },
   "outputs": [],
   "source": [
    "l=[]\n",
    "def convert(b):\n",
    "    if(b==0):\n",
    "        return l\n",
    "    dig=b%2\n",
    "    l.append(dig)\n",
    "    convert(b//2)\n",
    "convert(6)\n",
    "for i in l:\n",
    "    print(i,end=\"\")\n",
    "l.reverse()\n",
    "for i in l:\n",
    "    print(i,end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ec6a23ddea92034a96b64358bd3f3d4149a19b27"
   },
   "source": [
    "from random import randint\n",
    "\n",
    "def random_with_N_digits(n):\n",
    "    range_start = 10**(n-1)\n",
    "    range_end = (10**n)-1\n",
    "    return randint(range_start, range_end)\n",
    "\n",
    "#print(random_with_N_digits(38))\n",
    "\n",
    "v1 = \"12345678901234567890123456789012345678\" \n",
    "for x in range(38, 0, -1):\n",
    "    s = ''\n",
    "    c = 0\n",
    "    for i in range (x, 39 ):\n",
    "        if x == i: s = s + '.'            \n",
    "        c = c + 1\n",
    "        if c == 10: c = 0\n",
    "        s = s + str(c)\n",
    "    print(v1[0:39-len(s)]+ s)\n",
    "\n",
    "for x in range(0, 39):\n",
    "    s = '.'\n",
    "    c = 0\n",
    "    for i in range (0, x ):\n",
    "        if x > i: s = s + '0'\n",
    "    v = s + v1\n",
    "    print(v[0:39-len(v)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-multilearn in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.1.0)\n",
      "Requirement already satisfied: scikit-learn==0.20.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.20.0)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from scikit-learn==0.20.0) (1.15.3)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from scikit-learn==0.20.0) (1.1.0)\n",
      "Requirement already satisfied: sklearn in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from sklearn) (0.20.0)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.15.3)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install scikit-multilearn\n",
    "#!pip3 uninstall scikit-learn\n",
    "!pip3 install scikit-learn==0.20.0\n",
    "!pip3 install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d23ad2fdffa908b4f6ec1ad9d578886d8c218342"
   },
   "outputs": [],
   "source": [
    "lst=[3,4,6,1,2]\n",
    "lst[1:2]=[7,8]\n",
    "print(lst)\n",
    "\n",
    "20180917,16,21,22,24,25,27,1\n",
    "20180913,6,16,17,40,44,48,34\n",
    "20180910,2,6,9,15,40,43,18\n",
    "20180906,2,15,17,20,23,30,45\n",
    "20180903,4,5,13,18,39,40,3\n",
    "20180830,3,9,27,29,31,40,46\n",
    "20180827,5,6,16,24,26,29,38\n",
    "20180823,2,3,23,30,39,41,19\n",
    "20180820,9,10,25,38,40,42,2\n",
    "20180816,22,23,25,32,33,36,20\n",
    "20180813,1,3,6,16,22,36,17\n",
    "20180809,13,16,20,23,39,42,28\n",
    "20180806,7,18,20,27,36,40,15\n",
    "20180802,1,10,15,27,41,46,35\n",
    "20180730,8,10,19,20,41,43,7\n",
    "20180726,1,9,13,17,28,40,37\n",
    "20180723,2,23,26,28,39,40,12\n",
    "20180719,13,14,23,35,37,46,45\n",
    "20180716,4,8,19,24,32,47,22\n",
    "20180712,4,15,25,32,40,41,10\n",
    "20180709,6,23,31,38,39,43,33\n",
    "20180705,8,11,28,30,32,34,39\n",
    "20180702,12,13,26,33,35,38,23\n",
    "20180625,2,3,25,38,44,48,9\n",
    "20180621,4,6,15,24,30,35,46\n",
    "20180618,11,15,22,23,26,43,25\n",
    "20180614,4,29,31,35,42,48,1\n",
    "20180611,16,25,30,37,44,49,34\n",
    "20180607,12,20,29,31,37,39,42\n",
    "20180604,20,22,31,37,43,45,27\n",
    "20180531,11,13,24,26,47,49,33\n",
    "20180528,5,9,27,28,30,44,2\n",
    "20180524,11,25,26,34,36,42,16\n",
    "20180521,8,10,16,30,37,44,17\n",
    "20180517,7,21,25,29,35,37,13\n",
    "20180514,17,24,29,45,46,49,5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3b35bc990a898747db17198ba63b2ba3df55e76d"
   },
   "outputs": [],
   "source": [
    "column_names= ['CONTROL', 'AGE1', 'METRO3', 'REGION', 'LMED', 'FMR','IPOV','BEDRMS','BUILT','STATUS','TYPE','VALUE','NUNITS','ROOMS','PER','ZINC2','ZADEQ','ZSMHC','STRUCTURETYPE','OWNRENT','UTILITY','OTHERCOST','COST06','COST08','COST12','COSTMED','ASSISTED']\n",
    "ahads = pd.read_csv('../input/american-housing-data/2013.csv', names=column_names)\n",
    "ahads.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "438791305cbb883d551e3f7c1f32ac581abc5caa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SGH.csv', 'PPv3.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import utils, preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier, Ridge\n",
    "from sklearn.multioutput import MultiOutputClassifier, MultiOutputRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor, AdaBoostClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, SVR\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "#from sklearn.linear_model import RidgeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# Compare Algorithms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import statsmodels.api as sm\n",
    "\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #Hide messy TensorFlow warnings\n",
    "#warnings.filterwarnings(\"ignore\") #Hide messy Numpy warnings\n",
    "#warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "\n",
    "seed = 21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "3f82280a5a62dbaaf14212b85ec7bc3d7dae2bff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def getAccuracy(prediction, actual):\n",
    "    matched = []\n",
    "    for (p,a) in zip(prediction, actual):\n",
    "        matched.append(len(set(p.astype(int)) & set(a)))\n",
    "    return matched\n",
    "\n",
    "def getPercent(prediction, actual, n):\n",
    "    matched = []\n",
    "    for (p,a) in zip(prediction, actual):\n",
    "        matched.append(set(p.astype(int)) & set(a))\n",
    "    return sum(len(num) > n for num in matched)/len(matched)*100.00\n",
    "\n",
    "def getCounts(prediction, actual):\n",
    "    matched = []\n",
    "    N = [0,0,0,0,0,0,0,0]\n",
    "    if len(prediction) == 0: return N\n",
    "    for (a,p) in zip(actual, prediction):\n",
    "        matched.append(set(p.astype(int)) & set(a))\n",
    "    if len(matched) == 0:\n",
    "        return N\n",
    "    N[0] = sum(len(num) > 0 for num in matched)/len(matched)*100.00\n",
    "    N[1] = sum(len(num) > 1 for num in matched)/len(matched)*100.00\n",
    "    N[2] = sum(len(num) > 2 for num in matched)/len(matched)*100.00\n",
    "    N[3] = sum(len(num) > 3 for num in matched)/len(matched)*100.00\n",
    "    N[4] = sum(len(num) > 4 for num in matched)/len(matched)*100.00\n",
    "    N[5] = sum(len(num) > 5 for num in matched)/len(matched)*100.00\n",
    "    N[6] = sum(len(num) > 6 for num in matched)/len(matched)*100.00\n",
    "    N[7] = sum(len(num) > 7 for num in matched)/len(matched)*100.00\n",
    "#    matched.extend(N)\n",
    "    return N\n",
    "\n",
    "\n",
    "def getAccuracy1dCount(prediction, actual):\n",
    "    iMatched = 0\n",
    "    print(len(prediction))\n",
    "    for i in range(0,len(prediction)):\n",
    "        if prediction[i] == actual[i]:\n",
    "            iMatched = iMatched +1\n",
    "    return iMatched\n",
    "\n",
    "def getAccuracy1dPercentCorrect(prediction, actual):\n",
    "    iMatched = 0\n",
    "    print(len(prediction))\n",
    "    for i in range(0,len(prediction)):\n",
    "        if prediction[i] == actual[i]:\n",
    "            iMatched = iMatched +1\n",
    "    return iMatched/len(prediction) * 100.0\n",
    "\n",
    "\n",
    "def getAccuracyCount(prediction, actual):\n",
    "    matched = []\n",
    "    if len(prediction) == 0: return 0\n",
    "    for (p,a) in zip(prediction, actual):\n",
    "#        print ( \"p: \", p, \" a: \", a, (set(p.astype(int)) & set(a)) )\n",
    "        matched.append((set(p.astype(int)) & set(a)))\n",
    "    return sum(len(num) > 0 for num in matched)/len(matched)*100.00\n",
    "\n",
    "def getIntersection(p1, p2):\n",
    "    return [reduce(np.intersect1d, (p.astype(int), a.astype(int))) for (p,a) in zip(p1, p2)]\n",
    "\n",
    "def getUnion(p1, p2):\n",
    "    if len(p1) == 0: return p2\n",
    "    return [reduce(np.union1d, (p.astype(int), a.astype(int))) for (p,a) in zip(p1, p2)]\n",
    "\n",
    "def getUnion1dArray(p1,p2):\n",
    "    return reduce(np.union1d, (p1,p2))\n",
    "\n",
    "def getIntersection1dArray(prediction, actual):\n",
    "    iMatched = 0\n",
    "#     return reduce(np.intersect1d,([prediction,actual]))\n",
    "#     for idx, i in enumerate(prediction):\n",
    "#       if prediction[idx] == actual[idx]:\n",
    "#         iMatched = iMatched + 1\n",
    "#     return iMatched\n",
    "\n",
    "# def getAccuracy1dCount(prediction, actual):\n",
    "#     iMatched = 0\n",
    "#     for idx in enumerate(prediction):\n",
    "#       if prediction[idx] == actual[idx]:\n",
    "#         iMatched = iMatched + 1\n",
    "#     return iMatched\n",
    "\n",
    "\n",
    "\n",
    "def unionPrediction(ff):\n",
    "    predicted = []\n",
    "    for i, f in enumerate(ff):\n",
    "#        i_index = name_.index(sInputDir + f + '.csv')        \n",
    "        i_index = name_.index(f)        \n",
    "        if i == 0: predicted = list_[i_index][cols].values\n",
    "        if i > 0:\n",
    "            predicted = getUnion(predicted,list_[i_index][cols].values)\n",
    "    return predicted\n",
    "\n",
    "def getMatches(prediction, actual):\n",
    "    matched = []\n",
    "    for (p,a) in zip(prediction, actual):\n",
    "        print ( len(p), \"** p: \", p, \" a: \", a, (set(p.astype(int)) & set(a)) )\n",
    "        matched.append((set(p.astype(int)) & set(a)))\n",
    "    return matched\n",
    "\n",
    "def getMatchedCount(prediction, actual):\n",
    "    return getAccuracyCount(prediction, actual)\n",
    "\n",
    "def printPredictions ( prediction, actual ):\n",
    "    for (p,a) in zip(prediction, actual):\n",
    "        print ( '[',len(set(p.astype(int) )&set(a.astype(int))),'/',len(p),'',set(p.astype(int) ), ' ', set(a.astype(int)), ' ', set(p.astype(int) )&set(a.astype(int)))\n",
    "\n",
    "def bins_labels(bins, **kwargs):\n",
    "    bin_w = (max(bins) - min(bins)) / (len(bins) - 1)\n",
    "    plt.xticks(np.arange(min(bins)+bin_w/2, max(bins), bin_w), bins, **kwargs)\n",
    "    plt.xlim(bins[0], bins[-1])\n",
    "\n",
    "def getColums (idx):\n",
    "    return list(os.path.splitext(basename(name_[idx]))[0][2:])\n",
    "\n",
    "\n",
    "def showResult(str_alg, prediction, actual ):\n",
    "    print( str_alg, \" Accuracy predict 1 in 7: \", getAccuracy(prediction, actual))\n",
    "\n",
    "def printResult(predictions, actual):\n",
    "    df_result=pd.DataFrame({ 'Predicted':list(predictions), 'Actual':list(actual)})\n",
    "    print(df_result)  \n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "8f415532e79fc69a390131dc4f90475acd87b04b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             T           L  Lg  Lt  Ls  Lb           S  Sg  St  Ss ...  \\\n",
      "1547  20181217  300.014568   7   5   3   3  359.526457   6   3   7 ...   \n",
      "1548  20181220  302.236280   7   5  12  11   40.189495   4   2   2 ...   \n",
      "1549  20181224  305.200567   7   5   1   6   98.931481   2   7   4 ...   \n",
      "1550  20181227  307.425821   7  11  11   7  142.704093   1   4   7 ...   \n",
      "1551  20181231  310.396111   7  11   6  11  197.112557   4  11   4 ...   \n",
      "\n",
      "               U  Ug  Ut  Us  Ub           K  Kg  Kt  Ks  Kb  \n",
      "1547  255.692043   6   4   1  11  274.279218   7   1   7   2  \n",
      "1548  256.040966   6   4   1   3  274.120357   7   1   7   1  \n",
      "1549  256.509451   6   4   2  11  273.908541   7   1   7   4  \n",
      "1550  256.862581   6   4   2   7  273.749679   7   1   7  12  \n",
      "1551  257.334785   6   4   5   5  273.537864   7   1   7   3  \n",
      "\n",
      "[5 rows x 51 columns]\n",
      "1485\n"
     ]
    }
   ],
   "source": [
    "pp = pd.read_csv('../input/PPv3.csv')\n",
    "print(pp.tail())\n",
    "\n",
    "lr = pd.read_csv('../input/SGH.csv')\n",
    "#print(lr.describe())\n",
    "\n",
    "#print(lr)\n",
    "\n",
    "#print(len(lr))\n",
    "#lr = lr.sort_values(by=['D'])\n",
    "#lr = lr.drop_duplicates () ;\n",
    "print(len(lr))\n",
    "cols = ['D', 'N1','N2','N3','N4','N5','N6','N7']\n",
    "lr = lr[cols]\n",
    "#print(lr.head(30))\n",
    "\n",
    "#https://pandas.pydata.org/pandas-docs/stable/merging.html\n",
    "df = pd.concat([pp, lr], axis=1, sort=False)\n",
    "df = df.dropna()\n",
    "#print(len(df))\n",
    "#df.head()\n",
    "df.reset_index().drop(['D'], axis=1)\n",
    "\n",
    "cols = ['N1','N2','N3','N4','N5','N6','N7']\n",
    "lr = df[cols]\n",
    "\n",
    "\n",
    "\n",
    "# cols = ['L','M','S','R','E','A','V' ,'J','U']\n",
    "# cols = ['L', 'M', 'R', 'J', 'U']\n",
    "# X = df[cols]\n",
    "\n",
    "X = df\n",
    "drop_cols = ['T','D','N1','N2','N3','N4','N5','N6','N7','L','M','S','R','E','A','V' ,'J','U','K']\n",
    "X = X.drop(drop_cols, axis=1)\n",
    "\n",
    "related_X = X\n",
    "dataset = related_X\n",
    "\n",
    "lresult = np.sort(lr.values[:, ::-1])\n",
    "#print(lresult)\n",
    "\n",
    "col_n = 2  #Column Number interested\n",
    "aa = np.delete(lresult, np.s_[col_n:], axis=1)  \n",
    "aa = np.delete(aa, np.s_[0:col_n-1], axis=1)  \n",
    "#print(aa)\n",
    "#Convert 2d array to Dataframe\n",
    "y = pd.DataFrame(aa, columns=list('N'))\n",
    "#    y.head()\n",
    "y = aa.astype(int).ravel()\n",
    "#print ( y )\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X) # make the predictions by the model\n",
    "\n",
    "#print(model.summary())\n",
    "corr = X.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d388603d1adceea3c19a4161e589d9d792220aa7"
   },
   "source": [
    "**Remove Highly Correlated Features** ( corr > 0.9 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "de31fd3a3dba2571e784e1b35aa678e4998d2b05",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          Lg        Lt        Ls        Lb        Sg        St        Ss  \\\n",
      "Lg  1.000000 -0.081977  0.092373  0.080282  0.105479  0.043813  0.073459   \n",
      "Lt -0.081977  1.000000  0.043682 -0.009368  0.072553  0.024206  0.074964   \n",
      "Ls  0.092373  0.043682  1.000000  0.036746  0.072839  0.069562  0.065559   \n",
      "Lb  0.080282 -0.009368  0.036746  1.000000  0.064596  0.030046  0.073900   \n",
      "Sg  0.105479  0.072553  0.072839  0.064596  1.000000 -0.016538  0.127059   \n",
      "St  0.043813  0.024206  0.069562  0.030046 -0.016538  1.000000  0.076140   \n",
      "Ss  0.073459  0.074964  0.065559  0.073900  0.127059  0.076140  1.000000   \n",
      "Sb  0.089778  0.001914  0.079165  0.067914  0.149761  0.024560  0.073088   \n",
      "Mg -0.015066  0.104374  0.104124  0.031791  0.124923  0.070854  0.125722   \n",
      "Mt  0.012333 -0.114037  0.048935  0.002225  0.062516  0.040366 -0.003577   \n",
      "Ms  0.073039  0.042591  0.117887  0.054593  0.087403  0.074881  0.041011   \n",
      "Mb  0.146214 -0.006031  0.071008  0.075302  0.030421  0.034024  0.085298   \n",
      "Rg  0.070756  0.066980  0.099686  0.072330  0.131065  0.079773  0.093633   \n",
      "Rt  0.037286 -0.006844  0.040230  0.030393  0.055181  0.052723  0.039523   \n",
      "Rs  0.055604  0.001320  0.075996  0.086376  0.106122  0.002281  0.023697   \n",
      "Rb  0.137462  0.072346  0.065927  0.100510  0.077872  0.094541  0.041919   \n",
      "Ag  0.003350  0.065967  0.091223  0.035758  0.062330  0.041222  0.097930   \n",
      "At  0.099931 -0.044698  0.072838  0.051590  0.116145  0.036031  0.054194   \n",
      "As  0.057944  0.039817  0.059479  0.017391  0.079594  0.058431  0.091214   \n",
      "Ab  0.046661  0.047432  0.072235  0.008035  0.106163  0.100984  0.061903   \n",
      "Eg  0.019684  0.088279  0.091002  0.057728  0.143224  0.059746  0.118651   \n",
      "Et  0.139811  0.005947  0.056207  0.054622  0.030160  0.012904  0.044534   \n",
      "Es  0.001940  0.051888  0.079721  0.058687  0.073000  0.027397  0.098198   \n",
      "Eb  0.033974  0.062937  0.095147  0.022914  0.058260  0.058791  0.098626   \n",
      "Vg  0.263808 -0.000240  0.101577  0.085347  0.171322  0.053694  0.130407   \n",
      "Vt  0.045713  0.100751  0.131030  0.040081  0.070651  0.078955  0.060632   \n",
      "Vs  0.035952  0.047684  0.074726  0.016825  0.120660  0.038137  0.057273   \n",
      "Vb  0.096617  0.073102  0.079216  0.030753  0.106273  0.065085  0.098327   \n",
      "Jg  0.166005  0.033777  0.107008  0.049304  0.123457  0.070325  0.082665   \n",
      "Jt -0.043282 -0.027075  0.041385  0.044098  0.033189  0.055415  0.026592   \n",
      "Js  0.071110 -0.011520  0.010302  0.048566  0.036404  0.072533 -0.021124   \n",
      "Jb  0.109929  0.055943  0.083599  0.001758  0.037562  0.043671  0.056838   \n",
      "Ug  0.045295  0.054489  0.115892  0.113310  0.101476  0.061406  0.055072   \n",
      "Ut  0.070596  0.061635  0.035286  0.024042  0.089338  0.053847  0.069749   \n",
      "Us  0.110080  0.099606  0.049716  0.080011  0.073257  0.046357  0.061041   \n",
      "Ub  0.064477 -0.000105  0.099629  0.027558  0.090637  0.021196  0.085806   \n",
      "Kg  0.074595  0.080735  0.151431  0.059797  0.222676  0.076484  0.092481   \n",
      "Kt  0.056142  0.033394  0.051060  0.009278  0.056221  0.001317  0.057878   \n",
      "Ks  0.101452  0.046631  0.071778  0.074492  0.077591  0.054161  0.072597   \n",
      "Kb  0.083626  0.033842  0.108770  0.049514  0.055411  0.058356  0.028797   \n",
      "\n",
      "          Sb        Mg        Mt    ...           Js        Jb        Ug  \\\n",
      "Lg  0.089778 -0.015066  0.012333    ...     0.071110  0.109929  0.045295   \n",
      "Lt  0.001914  0.104374 -0.114037    ...    -0.011520  0.055943  0.054489   \n",
      "Ls  0.079165  0.104124  0.048935    ...     0.010302  0.083599  0.115892   \n",
      "Lb  0.067914  0.031791  0.002225    ...     0.048566  0.001758  0.113310   \n",
      "Sg  0.149761  0.124923  0.062516    ...     0.036404  0.037562  0.101476   \n",
      "St  0.024560  0.070854  0.040366    ...     0.072533  0.043671  0.061406   \n",
      "Ss  0.073088  0.125722 -0.003577    ...    -0.021124  0.056838  0.055072   \n",
      "Sb  1.000000  0.113598  0.099698    ...     0.007709  0.118978  0.043979   \n",
      "Mg  0.113598  1.000000  0.015986    ...     0.097360  0.060293  0.104496   \n",
      "Mt  0.099698  0.015986  1.000000    ...     0.063062  0.027713  0.027820   \n",
      "Ms  0.078998  0.119859  0.068007    ...     0.043834  0.098254  0.074396   \n",
      "Mb  0.041838  0.092295  0.090757    ...     0.030133  0.066871  0.088933   \n",
      "Rg  0.086229  0.114866  0.071077    ...     0.084119  0.105202  0.043106   \n",
      "Rt  0.115175  0.102339  0.044881    ...     0.117654 -0.029844 -0.153420   \n",
      "Rs  0.063012  0.083135  0.063787    ...    -0.000784  0.052718  0.116287   \n",
      "Rb  0.091716  0.092735  0.041161    ...     0.027730  0.082880  0.069302   \n",
      "Ag  0.060398  0.203650  0.039046    ...    -0.073344  0.085747  0.268562   \n",
      "At  0.106615  0.087878  0.059532    ...     0.055616  0.082853  0.097416   \n",
      "As  0.058619  0.086140  0.050409    ...     0.063705  0.050791  0.034145   \n",
      "Ab  0.073049  0.097627  0.022279    ...     0.007444 -0.013301  0.084459   \n",
      "Eg  0.104863  0.597042  0.057848    ...     0.053272  0.100741  0.142174   \n",
      "Et  0.056309  0.082419  0.015346    ...     0.055329  0.030587  0.004088   \n",
      "Es  0.033000  0.110062  0.096882    ...     0.011925  0.039032  0.085972   \n",
      "Eb  0.053318  0.092509  0.044235    ...     0.073202  0.079545  0.082115   \n",
      "Vg  0.095897  0.240857  0.115934    ...     0.056319  0.088092  0.196152   \n",
      "Vt  0.081978  0.077390 -0.027114    ...     0.077665  0.058755  0.081048   \n",
      "Vs  0.067954  0.108059  0.071890    ...     0.030189  0.078936  0.033978   \n",
      "Vb  0.016849  0.122341  0.026514    ...     0.041912  0.056560  0.135651   \n",
      "Jg  0.128613  0.128692  0.057784    ...     0.087253  0.096803 -0.011313   \n",
      "Jt -0.014872  0.087191  0.046272    ...     0.042172  0.050124  0.174234   \n",
      "Js  0.007709  0.097360  0.063062    ...     1.000000  0.023387  0.018381   \n",
      "Jb  0.118978  0.060293  0.027713    ...     0.023387  1.000000  0.077740   \n",
      "Ug  0.043979  0.104496  0.027820    ...     0.018381  0.077740  1.000000   \n",
      "Ut  0.088689  0.143868  0.107671    ...     0.031985  0.065899 -0.040357   \n",
      "Us  0.071180  0.109708  0.018292    ...     0.096263  0.044580  0.060530   \n",
      "Ub  0.059827  0.048690  0.033817    ...     0.035437  0.044289  0.101300   \n",
      "Kg  0.129581  0.136450  0.081500    ...     0.045606  0.093051  0.299380   \n",
      "Kt  0.076370  0.029767  0.047681    ...     0.009823  0.045460 -0.109902   \n",
      "Ks  0.020979  0.044063  0.070416    ...     0.017210  0.049724  0.104883   \n",
      "Kb  0.073747  0.103103  0.035484    ...    -0.004413  0.037728  0.078065   \n",
      "\n",
      "          Ut        Us        Ub        Kg        Kt        Ks        Kb  \n",
      "Lg  0.070596  0.110080  0.064477  0.074595  0.056142  0.101452  0.083626  \n",
      "Lt  0.061635  0.099606 -0.000105  0.080735  0.033394  0.046631  0.033842  \n",
      "Ls  0.035286  0.049716  0.099629  0.151431  0.051060  0.071778  0.108770  \n",
      "Lb  0.024042  0.080011  0.027558  0.059797  0.009278  0.074492  0.049514  \n",
      "Sg  0.089338  0.073257  0.090637  0.222676  0.056221  0.077591  0.055411  \n",
      "St  0.053847  0.046357  0.021196  0.076484  0.001317  0.054161  0.058356  \n",
      "Ss  0.069749  0.061041  0.085806  0.092481  0.057878  0.072597  0.028797  \n",
      "Sb  0.088689  0.071180  0.059827  0.129581  0.076370  0.020979  0.073747  \n",
      "Mg  0.143868  0.109708  0.048690  0.136450  0.029767  0.044063  0.103103  \n",
      "Mt  0.107671  0.018292  0.033817  0.081500  0.047681  0.070416  0.035484  \n",
      "Ms  0.081698  0.081845  0.063884  0.110621  0.023901  0.037109  0.066738  \n",
      "Mb  0.054014  0.111208  0.019849  0.089568  0.019415  0.052101  0.127219  \n",
      "Rg  0.142823  0.077196  0.042248  0.049303  0.058705  0.074904  0.075326  \n",
      "Rt  0.145833 -0.002700  0.030971  0.070610  0.287774  0.060830  0.005766  \n",
      "Rs  0.001658  0.066326  0.068229  0.125022  0.037250  0.430954  0.000347  \n",
      "Rb  0.089646  0.063691  0.046004  0.112767  0.054704  0.010822  0.133865  \n",
      "Ag  0.051929  0.067564  0.084221  0.111552  0.079066  0.120865  0.072335  \n",
      "At  0.091162  0.040489  0.053510  0.186191  0.061887 -0.006005  0.094011  \n",
      "As  0.057472  0.048270  0.048314  0.066171  0.015645  0.063654  0.047578  \n",
      "Ab  0.071867  0.051997  0.051367  0.114117  0.026848  0.045170  0.087844  \n",
      "Eg  0.150470  0.053130  0.091846  0.158406  0.053585  0.101249  0.069463  \n",
      "Et  0.082592 -0.027763  0.048090  0.062653  0.138431  0.104042  0.060382  \n",
      "Es  0.029124  0.071084  0.073110  0.103437  0.046795  0.124489  0.068293  \n",
      "Eb  0.081372  0.089718  0.052790  0.068207  0.070444 -0.012004  0.033671  \n",
      "Vg  0.066248  0.116348  0.034479  0.201032 -0.064952  0.050261  0.087295  \n",
      "Vt  0.081018  0.075890  0.040558  0.112603  0.051781  0.023716  0.030341  \n",
      "Vs  0.073735  0.084478  0.031875  0.105083  0.046000  0.028783  0.009089  \n",
      "Vb  0.064764  0.075080  0.034233  0.097531  0.024864  0.078360  0.020393  \n",
      "Jg  0.153990  0.088249  0.074232  0.052081  0.162465  0.078073  0.089764  \n",
      "Jt  0.026702  0.017478  0.043413  0.116175  0.012009  0.001904  0.036554  \n",
      "Js  0.031985  0.096263  0.035437  0.045606  0.009823  0.017210 -0.004413  \n",
      "Jb  0.065899  0.044580  0.044289  0.093051  0.045460  0.049724  0.037728  \n",
      "Ug -0.040357  0.060530  0.101300  0.299380 -0.109902  0.104883  0.078065  \n",
      "Ut  1.000000  0.089695  0.041405  0.080981  0.151056 -0.000216  0.063767  \n",
      "Us  0.089695  1.000000  0.022969  0.085309  0.070214  0.079680  0.025998  \n",
      "Ub  0.041405  0.022969  1.000000  0.117470  0.065555  0.084686  0.081938  \n",
      "Kg  0.080981  0.085309  0.117470  1.000000  0.025058  0.135516  0.101313  \n",
      "Kt  0.151056  0.070214  0.065555  0.025058  1.000000  0.021430  0.030161  \n",
      "Ks -0.000216  0.079680  0.084686  0.135516  0.021430  1.000000  0.067396  \n",
      "Kb  0.063767  0.025998  0.081938  0.101313  0.030161  0.067396  1.000000  \n",
      "\n",
      "[40 rows x 40 columns]\n",
      "Selected columns after Removing highly correlated cols:  Index(['Lg', 'Lt', 'Ls', 'Lb', 'Sg', 'St', 'Ss', 'Sb', 'Mg', 'Mt', 'Ms', 'Mb',\n",
      "       'Rg', 'Rt', 'Rs', 'Rb', 'Ag', 'At', 'As', 'Ab', 'Eg', 'Et', 'Es', 'Eb',\n",
      "       'Vg', 'Vt', 'Vs', 'Vb', 'Jg', 'Jt', 'Js', 'Jb', 'Ug', 'Ut', 'Us', 'Ub',\n",
      "       'Kg', 'Kt', 'Ks', 'Kb'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Print out the statistics\n",
    "columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "for i in range(corr.shape[0]):\n",
    "    for j in range(i+1, corr.shape[0]):\n",
    "        if corr.iloc[i,j] >= 0.9:\n",
    "            if columns[j]:\n",
    "                columns[j] = False\n",
    "selected_columns = X.columns[columns]\n",
    "related_X = X[selected_columns]\n",
    "print()\n",
    "print(corr)\n",
    "print('Selected columns after Removing highly correlated cols: ', selected_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9e14130c997817e4df8ba7eabf3bcef84b529417"
   },
   "source": [
    "**Remove features with P values > 0.05**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "fa13244716c75fb2a020693ee204dc05ff8d999f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected columns after p-value filter:  Index(['Lt', 'Ls', 'Lb', 'St', 'Mt', 'Mb', 'Rt', 'Rb', 'At', 'As', 'Eg', 'Et',\n",
      "       'Eb', 'Vt', 'Vs', 'Vb', 'Jt', 'Js', 'Jb', 'Ks', 'Kb'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#selected_columns = selected_columns[1:].values\n",
    "related_X = X[selected_columns]\n",
    "import statsmodels.formula.api as sm\n",
    "def backwardElimination(x, Y, sl, columns):\n",
    "    numVars = len(x[0])\n",
    "    for i in range(0, numVars):\n",
    "        regressor_OLS = sm.OLS(Y, x).fit()\n",
    "        maxVar = max(regressor_OLS.pvalues).astype(float)\n",
    "        if maxVar > sl:\n",
    "            for j in range(0, numVars - i):\n",
    "                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n",
    "                    x = np.delete(x, j, 1)\n",
    "                    columns = np.delete(columns, j)\n",
    "                    \n",
    "    regressor_OLS.summary()\n",
    "    return x, columns\n",
    "SL = 0.05\n",
    "data_modeled, sel_cols = backwardElimination(related_X.values, y, SL, selected_columns)\n",
    "related_X = X[sel_cols]\n",
    "print('Selected columns after p-value filter: ', sel_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3f81416cbec02106c86bff310dadafd71faea71b"
   },
   "source": [
    "**Split Train and Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "98a50421b71e471873db2cdfa4774ec7dddd89e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1336, 21) (1336,)\n",
      "(149, 21) (149,)\n"
     ]
    }
   ],
   "source": [
    "# create training and testing vars\n",
    "X_train, X_test, y_train, y_test = train_test_split(related_X, y, test_size=0.1, shuffle=False)\n",
    "print (X_train.shape, y_train.shape)\n",
    "print (X_test.shape, y_test.shape)\n",
    "\n",
    "dataset = related_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "02b0fa8213f64402a32ec302f84f7f9f73dcb5a0"
   },
   "source": [
    "**AdaBoostClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "f2878b0319b5f79e7f8c8952fab4708f7e879297"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-605f7a8c8675>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# model = XGBClassifier()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# model = XGBClassifier()\n",
    "# learning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "# param_grid = dict(learning_rate=learning_rate)\n",
    "# kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "# grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\n",
    "# grid_result = grid_search.fit(X_train, y_train)\n",
    "# # summarize results\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# means = grid_result.cv_results_['mean_test_score']\n",
    "# stds = grid_result.cv_results_['std_test_score']\n",
    "# params = grid_result.cv_results_['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "# \tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "# # plot\n",
    "# pyplot.errorbar(learning_rate, means, yerr=stds)\n",
    "# pyplot.title(\"XGBoost learning_rate vs Log Loss\")\n",
    "# pyplot.xlabel('learning_rate')\n",
    "# pyplot.ylabel('Log Loss')\n",
    "# pyplot.savefig('learning_rate.png')\n",
    "\n",
    "parameters = {'max_depth':range(3,20)}\n",
    "\n",
    "seed = 21\n",
    "#m = AdaBoostClassifier(RandomForestClassifier(random_state=seed),n_estimators=300,learning_rate=.0001,random_state=seed)\n",
    "#m = SVC() #random_state=seed,n_estimators=500)\n",
    "m = MLPClassifier(hidden_layer_sizes=(150,150,150), max_iter=5000, alpha=0.01,\n",
    "                      activation='tanh', solver='sgd', early_stopping=True, verbose=True, shuffle=False, random_state=seed,tol=0.000001)\n",
    "\n",
    "parameters = {'learning_rate':[0.0001, 0.001, 0.01, 0.1, 0.2, 0.3], \n",
    "              'max_depth':range(3,20),\n",
    "              'n_estimators': range(60,120,10),\n",
    "             }\n",
    "# XGBClassifier(alpha=0.01, base_score=0.5, booster='gbtree',\n",
    "#        colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
    "#        max_delta_step=0, max_depth=3, max_iter=30, min_child_weight=1,\n",
    "#        missing=None, n_estimators=100, n_jobs=1, nthread=None,\n",
    "#        objective='multi:softprob', random_state=21, reg_alpha=0,\n",
    "#        reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
    "#        subsample=1, tol=1e-06)\n",
    "clf = GridSearchCV(XGBClassifier(random_state=seed), parameters)\n",
    "\n",
    "m = clf\n",
    "# X = df\n",
    "# #drop_cols = ['T','D','N1','N2','N3','N4','N5','N6','N7']\n",
    "# drop_cols = ['T','D','N1','N2','N3','N4','N5','N6','N7','L','M','S','R','E','A','V' ,'J','U','K']\n",
    "# X = X.drop(selected_columns, axis=1)\n",
    "\n",
    "# dataset = X\n",
    "# create training and testing vars\n",
    "# X_train, X_test, y_train, y_test = train_test_split(dataset, y, test_size=0.1, shuffle=False)\n",
    "# print (X_train.shape, y_train.shape)\n",
    "# print (X_test.shape, y_test.shape)\n",
    "model = m.fit(X_train, y_train)\n",
    "print (m.best_score_, m.best_params_) \n",
    "model = m.best_estimator_\n",
    "print(model)\n",
    "predictions = model.predict(X_train)\n",
    "print(\"Train Predicted\", predictions.astype(int))\n",
    "print(\"Train Actuals  \", y_train)\n",
    "print(\"Train score    \", m.score(X_train,y_train))\n",
    "\n",
    "#model = m.fit(X_train, y_train)\n",
    "predictions = m.predict(X_test)\n",
    "print(\"Test Predicted\", predictions.astype(int))\n",
    "print(\"Test Actuals  \", y_test)\n",
    "print(\"Test score    \", m.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d651c654c079c83210fa7aae5ed4467b0585044e"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b2c6cfe30519ec7e4a3dce8c18da963747db87e6"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8393bc86522977ca094d8ce2abfddf5e55e7ec84"
   },
   "source": [
    "**Linear Regression**\n",
    "            #https://towardsdatascience.com/simple-and-multiple-linear-regression-in-python-c928425168f9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "78a68ae36d7bfd0d4ef8d2d830627ed0ed058bb1"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.cross_validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2d94ec0baa1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Necessary imports:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.cross_validation'"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "# Necessary imports: \n",
    "from sklearn.cross_validation import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "lm = linear_model.LinearRegression()\n",
    "model = lm.fit(X_train, y_train)\n",
    "predictions = lm.predict(X_train)\n",
    "print(\"Train Predicted\", predictions.astype(int))\n",
    "print(\"Train Actuals  \", y_train)\n",
    "print(\"Train score    \", lm.score(X_train,y_train))\n",
    "\n",
    "print(\"Coefficient: \", lm.coef_)\n",
    "print(\"Intercept  : \", lm.intercept_)\n",
    "\n",
    "# Perform 6-fold cross validation\n",
    "scores = cross_val_score(model, X_train, y_train, cv=3)\n",
    "print('Crossvalidated scores', scores)\n",
    "#Cross-validated scores: [ 0.4554861   0.46138572  0.40094084  0.55220736  0.43942775  0.56923406]\n",
    "    \n",
    "# Make cross validated predictions\n",
    "predictions = cross_val_predict(model, X_train, y_train, cv=6)\n",
    "print(\"CV Train Predicted\", predictions.astype(int))\n",
    "\n",
    "accuracy = metrics.r2_score(y_train, predictions)\n",
    "print ('Cross-Predicted Accuracy:', accuracy)\n",
    "\n",
    "predictions = lm.predict(X_test)\n",
    "df_result=pd.DataFrame({'Actual':y_test, 'Predicted':predictions.astype(int)})  \n",
    "print(df_result)  \n",
    "print(\"Test  Predicted\", predictions.astype(int))\n",
    "print(\"Test  Actuals  \", y_test)\n",
    "print(\"Test  score    \", lm.score(X_test,y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a5b8199351ce1391a2591d7d681f7e4464641dc8"
   },
   "source": [
    "**SVC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "cc1b584c35e848260fcb0f31a5184327bfb1d8c5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.cross_validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-271fcdc0061b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Necessary imports:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.cross_validation'"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "# Necessary imports: \n",
    "from sklearn.cross_validation import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "svr_lin = SVR(kernel='linear', C=1e3)\n",
    "svr_poly = SVR(kernel='poly', C=1e3, degree=2)\n",
    "m = svr_lin\n",
    "model = m.fit(X_train, y_train)\n",
    "predictions = m.predict(X_train)\n",
    "print(\"Train Predicted\", predictions.astype(int))\n",
    "print(\"Train Actuals  \", y_train)\n",
    "#print(\"Train score    \", m.score(X_train,y_train))\n",
    "\n",
    "#print(\"Coefficient: \", lm.coef_)\n",
    "#print(\"Intercept  : \", lm.intercept_)\n",
    "\n",
    "# # Perform 6-fold cross validation\n",
    "# scores = cross_val_score(model, X_train, y_train, cv=3)\n",
    "# print('Crossvalidated scores', scores)\n",
    "# #Cross-validated scores: [ 0.4554861   0.46138572  0.40094084  0.55220736  0.43942775  0.56923406]\n",
    "    \n",
    "# # Make cross validated predictions\n",
    "# predictions = cross_val_predict(model, X_train, y_train, cv=3)\n",
    "# print(\"CV Train Predicted\", predictions.astype(int))\n",
    "\n",
    "# accuracy = metrics.r2_score(y_train, predictions)\n",
    "# print ('Cross-Predicted Accuracy:', accuracy)\n",
    "\n",
    "predictions = m.predict(X_test)\n",
    "print(\"Test  Predicted\", predictions.astype(int))\n",
    "print(\"Test  Actuals  \", y_test)\n",
    "printResult(predictions, y_test)\n",
    "\n",
    "#print(\"Test  score    \", m.score(X_test,y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1e73ca48eb812e5dda20c1440214952367234ecd"
   },
   "source": [
    "**Plot SVR ( rbf, linear, poly )**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3407003117164b4294a8ef7727392b32448c7637"
   },
   "outputs": [],
   "source": [
    "svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "svr_lin = SVR(kernel='linear', C=1e3)\n",
    "svr_poly = SVR(kernel='poly', C=1e3, degree=2)\n",
    "y_rbf = svr_rbf.fit(X_train, y_train).predict(X_train)\n",
    "y_lin = svr_lin.fit(X_train, y_train).predict(X_train)\n",
    "y_poly = svr_poly.fit(X_train, y_train).predict(X_train)\n",
    "lw = 2\n",
    "plt.scatter(X, y, color='darkorange', label='data')\n",
    "plt.plot(X, y_rbf, color='navy', lw=lw, label='RBF model')\n",
    "plt.plot(X, y_lin, color='c', lw=lw, label='Linear model')\n",
    "plt.plot(X, y_poly, color='cornflowerblue', lw=lw, label='Polynomial model')\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('target')\n",
    "plt.title('Support Vector Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4645a443c35f9c8aabc96c7bc4c386c1a6182a36"
   },
   "source": [
    "**KFold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d88bc5dcad8919ec54c5ad51213c7bd31515d537"
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=2) # Define the split - into 2 folds \n",
    "kf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator\n",
    "print(kf) \n",
    "KFold(n_splits=2, random_state=None, shuffle=False)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "899acef4106d457350d7240af7770271ce5ff995"
   },
   "source": [
    "**PCA**\n",
    "#https://towardsdatascience.com/an-approach-to-choosing-the-number-of-components-in-a-principal-component-analysis-pca-3b9f3d6e73fe\n",
    "1. Take mean => data_mean = np.mean(X)\n",
    "2. Center by subtracting mean => data_center = X - data_mean\n",
    "3. Find covariance => cov_matrix = np.cov(data_center)\n",
    "4. Calculate  eigenvalues and eigenvectors  => eigenval, eigenvec = np.linalg.eig(cov_matrix)\n",
    "5. Calculate significance => significance = [np.abs(i)/np.sum(eigenval) for i in eigenval]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b0209947044ad3465dced109aa785fa7af5ebea9"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fc28e0ccb751060b52133024926d477ecb5ec6c3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "pca = PCA().fit(X)\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') #for each component\n",
    "plt.title('Planet Position Dataset Explained Variance')\n",
    "plt.show()\n",
    "\n",
    "#Based on the plot Determine the number of components to use ( best of n will be selected)\n",
    "n = 3\n",
    "pca = PCA(n_components=n)\n",
    "dataset = pca.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3535aaf1084e914ff098987f860dee4495a44c95"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=1)\n",
    "scaled_X = preprocessing.scale(X)\n",
    "dataset = poly.fit_transform(scaled_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d22103279f063d96eb4965a240abf4ca2928ff1e"
   },
   "outputs": [],
   "source": [
    "print(sel_cols)\n",
    "#data = pd.DataFrame(data = dataset, columns = sel_cols)\n",
    "data = pd.DataFrame(X[sel_cols])\n",
    "#X[sel_cols]\n",
    "fig = plt.figure(figsize = (20, 25))\n",
    "j = 0\n",
    "for i in data.columns:\n",
    "    plt.subplot(6, 4, j+1)\n",
    "    j += 1\n",
    "    sns.distplot(data[i][y==1], label = '1')\n",
    "    sns.distplot(data[i][y==2], label = '2')\n",
    "    sns.distplot(data[i][y==3], label = '3')\n",
    "    sns.distplot(data[i][y==4], label = '4')\n",
    "    sns.distplot(data[i][y==5], label = '5')\n",
    "    sns.distplot(data[i][y==6], label = '6')\n",
    "    sns.distplot(data[i][y==7], label = '7')\n",
    "    sns.distplot(data[i][y==8], label = '8')\n",
    "    plt.legend(loc='best')\n",
    "fig.suptitle('Data Analysis')\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "sns.set_palette(\"husl\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1441cbb3c15a053405f6b546c17bd9ec3509a04f"
   },
   "source": [
    "**LinearRegression snippet which can run after setting**\n",
    "\n",
    "dataset => Dependent Variables\n",
    "\n",
    "y => Independent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd432c958db2866e18c08fafe35485f0f81c6ef5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Input dataset, y\n",
    "\n",
    "# create training and testing vars\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, y, test_size=0.1, shuffle=False)\n",
    "print (X_train.shape, y_train.shape)\n",
    "print (X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "# Necessary imports: \n",
    "from sklearn.cross_validation import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "lm = linear_model.LinearRegression()\n",
    "model = lm.fit(X_train, y_train)\n",
    "predictions = lm.predict(X_train)\n",
    "print(\"Train Predicted\", predictions.astype(int))\n",
    "print(\"Train Actuals  \", y_train)\n",
    "print(\"Train score    \", lm.score(X_train,y_train))\n",
    "\n",
    "print(\"Coefficient: \", lm.coef_)\n",
    "print(\"Intercept  : \", lm.intercept_)\n",
    "\n",
    "# Perform 6-fold cross validation\n",
    "scores = cross_val_score(model, X_train, y_train, cv=3)\n",
    "print('Crossvalidated scores', scores)\n",
    "#Cross-validated scores: [ 0.4554861   0.46138572  0.40094084  0.55220736  0.43942775  0.56923406]\n",
    "    \n",
    "# Make cross validated predictions\n",
    "predictions = cross_val_predict(model, X_train, y_train, cv=6)\n",
    "print(\"CV Train Predicted\", predictions.astype(int))\n",
    "\n",
    "accuracy = metrics.r2_score(y_train, predictions)\n",
    "print ('Cross-Predicted Accuracy:', accuracy)\n",
    "\n",
    "predictions = lm.predict(X_test)\n",
    "print(\"Test  Predicted\", predictions.astype(int))\n",
    "print(\"Test  Actuals  \", y_test)\n",
    "print(\"Test  score    \", lm.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "de39b0a94ce8b8e92e999fcb942ac4f5b1b63f43"
   },
   "source": [
    "**RandomForest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2231953dea6bc25944498bbae3468e44fca93e39"
   },
   "outputs": [],
   "source": [
    "#NOT WORKING - Scatter Name Error\n",
    "def mychart(*args):\n",
    "\n",
    "    # pass some 2d n x 1 arrays, x, y, z\n",
    "    \n",
    "    # 1st array is independent vars\n",
    "    # reshape to 1 dimensional array\n",
    "    x = args[0].reshape(-1)\n",
    "    \n",
    "    # following are dependent vars plotted on y axis\n",
    "    data = []\n",
    "    for i in range(1, len(args)):\n",
    "        data.append(Scatter(x=x,\n",
    "                            y=args[i].reshape(-1),\n",
    "                            mode = 'markers'))\n",
    "\n",
    "    layout = Layout(\n",
    "        yaxis=dict(\n",
    "            autorange=True))\n",
    "    \n",
    "    fig = Figure(data=data, layout=layout)\n",
    "    \n",
    "    return iplot(fig) # png to save notebook w/static image   \n",
    "    \n",
    "#mychart(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b33bcee74dd94d44ac4707660e3506b3d8d52087",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#rf_random.best_params_\n",
    "mychart(X_train, y_train, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9bdf1728c1068666758eed3dc315d20e42f3de7f"
   },
   "outputs": [],
   "source": [
    "\n",
    "# try RandomForestRegressor# try Ra \n",
    "\n",
    "scaled_X = preprocessing.scale(dataset)\n",
    "# create training and testing vars\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.1, shuffle=False)\n",
    "print (X_train.shape, y_train.shape)\n",
    "print (X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(random_state = 21)\n",
    "from pprint import pprint\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf.get_params())\n",
    "\n",
    "# First 2 are most important\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(a) for a in np.linspace(start = 300, stop = 1200, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['sqrt'] #['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "#max_depth = [int(a) for a in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "print(\"Search grid:\")\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)\n",
    "# Use the random grid to search for best hyperparameters# Use th \n",
    "# First create the base model to tune\n",
    "# rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, \n",
    "                               param_distributions = random_grid, \n",
    "                               n_iter = 100, \n",
    "                               cv = 3, verbose=2, \n",
    "                               random_state=42, \n",
    "                               n_jobs = 1)\n",
    "# Fit the random search model\n",
    "model = rf_random.fit(X_train, y_train)\n",
    "rf_random.best_params_\n",
    "# predictions = rf_random.predict(X_train)\n",
    "\n",
    "\n",
    "\n",
    "# rf = RandomForestRegressor(n_estimators= 300,\n",
    "#     min_samples_split= 2,\n",
    "#     min_samples_leaf= 4,\n",
    "#     max_features= 'sqrt',\n",
    "#     max_depth= None,\n",
    "#     bootstrap= True,\n",
    "#     )\n",
    "\n",
    "# model = m.fit(X_train, y_train)\n",
    "# predictions = m.predict(X_train)\n",
    "# print(\"Test  Predicted\", predictions.astype(int))\n",
    "# print(\"Test  Actuals  \", y_test)\n",
    "# print(\"Test  score    \", m.score(X_train,y_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ee7e0ef6223addba9c783205857e39b117530dbc"
   },
   "source": [
    "**Neuro Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "248e11ff8928d919be8042bd56abf99605c2c497"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "05300f2cfe98b5ff3d2d6242630d9d02b5b88144"
   },
   "outputs": [],
   "source": [
    "xgbreg = XGBRegressor(nthreads=1)  \n",
    "\n",
    "#one_to_left = st.beta(10, 1)  \n",
    "#from_zero_positive = st.expon(0, 50)\n",
    "\n",
    "param_grid = {\n",
    "        'silent': [False],\n",
    "        'max_depth': [6, 10, 15, 20],\n",
    "        'learning_rate': [0.001, 0.01, 0.1, 0.2, 0,3],\n",
    "        'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'min_child_weight': [0.5, 1.0, 3.0, 5.0, 7.0, 10.0],\n",
    "        'gamma': [0, 0.25, 0.5, 1.0],\n",
    "        'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0],\n",
    "        'n_estimators': [100]}\n",
    "\n",
    "params = {  \n",
    "    \"n_estimators\": [10, 20, 40, 80, 160, 320, 640],\n",
    "    \"max_depth\": [4, 8, 16, 32, 64, 128],\n",
    "    \"learning_rate\": [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50],\n",
    "    \"colsample_bytree\": [1.0, 0.99, 0.95, 0.90, 0.85, 0.80, 0.60],\n",
    "    \"subsample\": [1.0, 0.99, 0.95, 0.90, 0.85, 0.80, 0.60],\n",
    "    \"gamma\": [0, 1.0, 1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0],\n",
    "    'reg_alpha': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "    \"min_child_weight\": [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "}\n",
    "\n",
    "#Input dataset, y\n",
    "# scaled_X = preprocessing.scale(dataset)\n",
    "# # create training and testing vars\n",
    "# X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.1, shuffle=False)\n",
    "# print (X_train.shape, y_train.shape)\n",
    "# print (X_test.shape, y_test.shape)\n",
    "\n",
    "#m = RandomizedSearchCV(xgbreg, params, n_jobs=1)  \n",
    "#model = m.fit(X_train, y_train)\n",
    "#predictions = m.predict(X_train)\n",
    "#m.best_params_\n",
    "\n",
    "m = XGBRegressor(nthreads=1,\n",
    "                      colsample_bytree=0.9,\n",
    "                      gamma=8.0,\n",
    "                      learning_rate=0.45,\n",
    "                      max_depth=32,\n",
    "                      min_child_weight=70,\n",
    "                      n_estimators=10,\n",
    "                      reg_alpha=60,\n",
    "                      subsample=0.9)\n",
    "\n",
    "model = m.fit(X_train, y_train)\n",
    "predictions = m.predict(X_test)\n",
    "print(\"Test  Predicted\", predictions.astype(int))\n",
    "print(\"Test  Actuals  \", y_test)\n",
    "print(\"Test  score    \", m.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4628de59efe8d2e3fb68dbb81bd92ec320ddca86",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#Input dataset, y\n",
    "scaled_X = preprocessing.scale(dataset)\n",
    "# create training and testing vars\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.1, shuffle=False)\n",
    "print (X_train.shape, y_train.shape)\n",
    "print (X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "# Necessary imports: \n",
    "from sklearn.cross_validation import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "iDepth = int(len(X_train[0]))\n",
    "print(iDepth)\n",
    "#m = MLPClassifier(hidden_layer_sizes=(20000,500), max_iter=100, alpha=.001,\n",
    "#                      activation='relu', solver='sgd', verbose=10,  early_stopping=True, shuffle=False, random_state=21,tol=0.000001)\n",
    "\n",
    "\n",
    "# m = MLPRegressor(\n",
    "#     hidden_layer_sizes=(10,10),  activation='tanh', solver='lbfgs', alpha=0.001, batch_size='auto',\n",
    "#     learning_rate='constant', learning_rate_init=0.1, power_t=0.5, max_iter=1000, shuffle=False,\n",
    "#     random_state=9, tol=0.000001, verbose=10, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n",
    "#     early_stopping=True, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "model = m.fit(X_train, y_train)\n",
    "predictions = m.predict(X_train)\n",
    "print(\"Train Predicted\", predictions.astype(int))\n",
    "print(\"Train Actuals  \", y_train)\n",
    "print(\"Train score    \", m.score(X_train,y_train))\n",
    "\n",
    "\n",
    "# Perform 6-fold cross validation\n",
    "#scores = cross_val_score(model, X_train, y_train, cv=3)\n",
    "#print('Crossvalidated scores', scores)\n",
    "#Cross-validated scores: [ 0.4554861   0.46138572  0.40094084  0.55220736  0.43942775  0.56923406]\n",
    "    \n",
    "# Make cross validated predictions\n",
    "#predictions = cross_val_predict(model, X_train, y_train, cv=6)\n",
    "#print(\"CV Train Predicted\", predictions.astype(int))\n",
    "\n",
    "#accuracy = metrics.r2_score(y_train, predictions)\n",
    "#print ('Cross-Predicted Accuracy:', accuracy)\n",
    "\n",
    "predictions = m.predict(X_test)\n",
    "print(\"Test  Predicted\", predictions.astype(int))\n",
    "print(\"Test  Actuals  \", y_test)\n",
    "print(\"Test  score    \", m.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "181320086664649c3e1994a651f019998212e40a"
   },
   "source": [
    "**MultiOutput ML**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "5cfb96cca75e7b279956f3ae8e5a05c47d5dfdde"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:62: DataConversionWarning: Data with input dtype int64 were all converted to float64 by the scale function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1336, 21) (1336, 7)\n",
      "(149, 21) (149, 7)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'getAccuracyCount' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-a22d0e5228c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mytestPredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#, [list(item) for item in y_train])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mtraining_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgetAccuracyCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytrainPredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mtesting_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgetAccuracyCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytestPredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mtotal_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtraining_score\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtesting_score\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#- ( training_score - testing_score )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'getAccuracyCount' is not defined"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "#oHL = [x for x in itertools.product((7,14),repeat=2)]\n",
    "oHL = [x for x in itertools.product((7,14,21,28,35,49),repeat=3)]\n",
    "\n",
    "# [('149.18016316360567', 4), ('150.367720933971', 1), ('151.40055459550697', 12), \n",
    "#  ('151.64519953381827', 37), ('152.14403407949203', 0), ('152.23646666398747', 13), \n",
    "#  ('152.83526905919706', 5), ('153.2120323112165', 21), ('153.2321263513242', 111), \n",
    "#  ('153.38433870514007', 138), ('153.43909496443354', 15), ('153.94797653016116', 3), \n",
    "#  ('154.68392074910582', 18), ('155.15814009564764', 23), ('155.21540810995458', 2), \n",
    "#  ('155.2430374151027', 61), ('155.58714785194712', 30), ('155.9066430896596', 102), \n",
    "#  ('155.9217136197404', 97), ('156.27084756661174', 22), ('156.46776915966723', 24), \n",
    "#  ('156.48786319977495', 60), ('156.59034280432425', 85), ('156.94450026122252', 10), \n",
    "#  ('157.33382228830928', 75), ('157.86028613913112', 103), ('157.90750713338423', 25), \n",
    "#  ('158.00747498292006', 82), ('158.05720773218664', 7), ('158.20942008600247', 16), \n",
    "#  ('158.28175863039024', 32), ('158.2842703854037', 14), ('158.36163243981835', 11), \n",
    "#  ('158.44401800425993', 34), ('158.4540650243138', 148), ('158.65852188240967', 42), \n",
    "#  ('158.74090744685128', 33), ('158.7584897319455', 186), ('158.8006872161717', 20), \n",
    "#  ('158.8057107261986', 9), ('158.81575774625247', 73), ('159.0101675842945', 6), \n",
    "#  ('159.03528513442916', 144), ('159.03779688944257', 28), ('159.2824418277539', 169), \n",
    "#  ('159.35478037214162', 99), ('159.3969778563678', 31), ('159.4843869308363', 39), \n",
    "#  ('159.5818430253587', 172), ('159.6290640196118', 72), ('159.64413454969255', 132), \n",
    "#  ('159.7089378290399', 54), ('159.7290318691476', 204), ('159.7913233934815', 74), \n",
    "#  ('159.86115018285577', 29), ('159.87119720290963', 180), ('159.93097697223004', 48), \n",
    "#  ('160.01587429168507', 114), ('160.0284330667524', 188), ('160.1655748904875', 110), \n",
    "#  ('160.1781336655548', 131), ('160.29769320419564', 26), ('160.40268456375838', 191), \n",
    "#  ('160.47251135313266', 139), ('160.61718844190813', 80), ('160.69957400634974', 136), \n",
    "#  ('160.76688904071054', 113), ('160.99646344894103', 187), ('161.13862878270305', 174), \n",
    "#  ('161.1833380219427', 19), ('161.2084555720773', 47), ('161.2185025921312', 98), \n",
    "#  ('161.27325885142466', 66), ('161.29335289153238', 119), ('161.3656914359201', 92), \n",
    "#  ('161.43802998030785', 64), ('161.44556524534823', 141), ('161.51288027970904', 146), \n",
    "#  ('161.67011614355184', 203), ('161.73994293292608', 168), ('161.74496644295303', 209), \n",
    "#  ('161.8147932323273', 183), ('161.8770847566612', 84), ('161.96198207611621', 86), \n",
    "#  ('162.03432062050393', 81), ('162.0368323755174', 88), ('162.11670618494554', 166), \n",
    "#  ('162.17899770927943', 68), ('162.26138327372104', 115), ('162.26389502873448', 94), \n",
    "#  ('162.30106900293373', 36), ('162.31111602298756', 38), ('162.48342241691114', 57), \n",
    "#  ('162.5607844713258', 121), ('162.62809950568663', 67), ('162.63814652574047', 160), \n",
    "#  ('162.7079733151147', 53), ('162.7104850701282', 118), ('162.71299682514166', 135), \n",
    "#  ('162.7577060643813', 17), ('162.77277659446207', 35), ('162.92498894827793', 109), \n",
    "#  ('162.93001245830487', 151), ('162.98728047261181', 27), ('163.1595868665354', 140), \n",
    "#  ('163.2319254109231', 127), ('163.30928746533777', 195), ('163.36404372463124', 78), \n",
    "#  ('163.4514527990998', 51), ('163.45396455411324', 157), ('163.45898806414016', 176), \n",
    "#  ('163.60868866294257', 171), ('163.6835389623438', 179), ('163.75838926174498', 208), \n",
    "#  ('163.8930193304666', 43), ('164.12008198368363', 112), ('164.12761724872402', 192), \n",
    "#  ('164.25973556243218', 55), ('164.27731784752643', 106), ('164.33709761684685', 49), \n",
    "#  ('164.34965639191415', 83), ('164.35216814692762', 124), ('164.41697142627498', 62), \n",
    "#  ('164.42701844632882', 199), ('164.71888437889322', 91), ('164.86607322268213', 147), \n",
    "#  ('164.8685849776956', 149), ('164.87612024273602', 201), ('164.94343527709682', 117), \n",
    "#  ('165.02079733151146', 70), ('165.0258208415384', 202), ('165.09815938592612', 210), \n",
    "#  ('165.1006711409396', 215), ('165.16547442028696', 63), ('165.16798617530043', 71), \n",
    "#  ('165.23027769963429', 40), ('165.24534822971506', 93), ('165.31266326407587', 56), ('165.32019852911628', 87), ('165.38249005345017', 41), ('165.4548285978379', 44), ('165.47241088293214', 154), ('165.54474942731986', 205), ('165.60955270666722', 120), ('165.6969617811357', 178), ('165.7718120805369', 137), ('165.83912711489774', 182), ('165.84415062492462', 159), ('165.91397741429893', 58), ('166.06618976811478', 104), ('166.06870152312825', 175), ('166.14104006751597', 133), ('166.14355182252945', 193), ('166.21840212193064', 130), ('166.28320540127797', 145), ('166.29074066631836', 163), ('166.36810272073302', 197), ('166.4379295101073', 65), ('166.5002210344412', 96), ('166.51529156452196', 165), ('166.58511835389623', 184), ('166.65996865329743', 156), ('166.80715749708637', 150), ('166.95936985090225', 162), ('167.03673190531686', 129), ('167.03924366033036', 167), ('167.17889723907888', 76), ('167.18643250411927', 128), ('167.55063698107142', 50), ('167.70536108990075', 198), ('167.7078728449142', 155), ('167.775187879275', 181), ('167.85506168870313', 122), ('167.8575734437166', 158), ('167.99722702246515', 45), ('168.2167544106418', 77), ('168.2318249407226', 164), ('168.30667524012378', 107), ('168.381525539525', 177), ('168.59602941767474', 126), ('168.82811558091868', 206), ('169.03759594904153', 79), ('169.0526664791223', 211), ('169.12249326849656', 185), ('169.1275167785235', 213), ('169.2747056223124', 123), ('169.7162721536792', 116), ('169.7986577181208', 173), \n",
    "#  ('170.09554716071213', 95), ('170.1528151750191', 90), ('170.23520073946068', 46), \n",
    "#  ('170.46979865771812', 214), ('170.61447574649358', 152),  ('170.61698750150705', 125), \n",
    "#  ('170.76668810030947', 200), ('171.13842784230195', 89), ('171.43782903990677', 190), \n",
    "#  ('171.58501788369568', 105), ('171.81208053691273', 207), ('172.25867057830646', 194), \n",
    "#  ('172.8524494634891', 100), ('172.9298115179038', 170), ('173.20911867540087', 108), \n",
    "#  ('173.6009524575011', 189), ('174.94323433669575', 101)]\n",
    "# 216\n",
    "\n",
    "\n",
    "\n",
    "y = lresult\n",
    "scaled_X = preprocessing.scale(dataset)\n",
    "# create training and testing vars\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, y.astype(int), test_size=0.1, shuffle=False)\n",
    "print (X_train.shape, y_train.shape)\n",
    "print (X_test.shape, y_test.shape)\n",
    "\n",
    "match = [95, 90, 46, 214, 152, 125, 200, 89, 190, 105, 207, 194, 100, 170, 108, 189, 101, 4, 1, 12, 37, 0, 13]\n",
    "\n",
    "i = 0\n",
    "prediction_dict= {}\n",
    "prediction_results = []\n",
    "for hl in oHL:\n",
    "    i = i + 1\n",
    "    if i in match:\n",
    "        m = MultiOutputClassifier(MLPClassifier(hidden_layer_sizes=hl,max_iter=50, alpha=0.1, activation='relu', solver='lbfgs', early_stopping=True, verbose=False, shuffle=False, random_state=seed,tol=0.000001))\n",
    "    #    print(m)\n",
    "        model = m.fit(X_train, [list(item) for item in y_train])\n",
    "        ytrainPredicted = m.predict(X_train)\n",
    "        #ytestPredicted = m.predict(X_test)\n",
    "        ytestPredicted = m.predict(X_test).astype(int) #, [list(item) for item in y_train])\n",
    "\n",
    "        training_score = (getAccuracyCount(ytrainPredicted, y_train))\n",
    "        testing_score = (getAccuracyCount(ytestPredicted, y_test))\n",
    "        total_score = (training_score + testing_score) #- ( training_score - testing_score )\n",
    "        sKey = \"{}\".format(total_score)\n",
    "    #     print(sKey)\n",
    "        print(sKey)\n",
    "        prediction_dict[sKey] = i\n",
    "        print ( hl, \" Atleast 1 matched: \", total_score, ' Training: ', training_score, '(', len(ytrainPredicted), ') Test: ', testing_score, '(', len(ytestPredicted), ')')\n",
    "        print(len(ytestPredicted),ytestPredicted)\n",
    "        prediction_results.append(ytestPredicted)\n",
    "\n",
    "print(sorted(prediction_dict.items()))\n",
    "\n",
    "def getCombinedPrediction(lst, ff):\n",
    "    comb_predicted = []\n",
    "    for p in range(1,len(ff)):\n",
    "        if len(comb_predicted) == 0:\n",
    "            comb_predicted = lst[p]\n",
    "            continue\n",
    "        comb_predicted = np.hstack((comb_predicted, lst[p])) #np.reshape(prediction_results[p], (-1,1))))\n",
    "    comb_predicted = [np.unique(x) for x in comb_predicted]\n",
    "    return comb_predicted\n",
    "\n",
    "iBest_xx = 0\n",
    "iBestCombo = []\n",
    "iBestN = []\n",
    "predicted = []\n",
    "actual = y_test\n",
    "arr = []\n",
    "print(len(prediction_results))\n",
    "for z in range(1, len(prediction_results)):\n",
    "    for x in itertools.combinations([ x for x in range(1,len(prediction_results))],z):\n",
    "        predicted = getCombinedPrediction(prediction_results, x)\n",
    "#     plt.hist(getAccuracy(predicted,actual))\n",
    "#     plt.show()\n",
    "        N = getCounts(predicted,actual)\n",
    "        if sum(iBestN) < sum(N):\n",
    "            print('=> ', z, x)\n",
    "            iBestResult = predicted\n",
    "            iBestIndex = i\n",
    "            iBest_xx = z\n",
    "            iBestN = N\n",
    "            iBestCombo = x\n",
    "            print(\"Best: \", iBestCombo, iBestN)\n",
    "\n",
    "print(\"Final BestCombo \", iBestCombo, iBestN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "668bd70a8f9171529d125361a495a3832ccdbd20"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prediction_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9cfeb51b06de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0muse_comb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombinations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prediction_results' is not defined"
     ]
    }
   ],
   "source": [
    "iBest_xx = 0\n",
    "iBestCombo = []\n",
    "iBestN = []\n",
    "predicted = []\n",
    "actual = y_test\n",
    "arr = []\n",
    "use_comb = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "print(len(prediction_results))\n",
    "for z in range(1, len(prediction_results)):\n",
    "    for x in itertools.combinations([ x for x in range(0,len(prediction_results))],z):\n",
    "        predicted = getCombinedPrediction(prediction_results, x)\n",
    "        N = getCounts(predicted,actual)\n",
    "        if sum(iBestN) < sum(N):\n",
    "            print('=> ', z, x)\n",
    "            iBestResult = predicted\n",
    "            iBestIndex = i\n",
    "            iBest_xx = z\n",
    "            iBestN = N\n",
    "            iBestCombo = x\n",
    "            print(\"Best: \", iBestCombo, iBestN)\n",
    "            if iBestN[0] == 100.:\n",
    "                plt.hist(getAccuracy(predicted,actual))\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "print(\"Final BestCombo \", iBestCombo, iBestN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "38dbeb1e1ed227031199045adb0e6709916692a4"
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "2a523bf364ab90c9db6f9d4be91809a73f03c7bd",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3. 10. 18. ... 29. 36. 42.]\n",
      " [ 6.  8. 22. ... 40. 41. 45.]\n",
      " [ 5.  8. 15. ... 21. 35. 36.]\n",
      " ...\n",
      " [ 8. 14. 23. ... 30. 32. 48.]\n",
      " [ 2.  7. 12. ... 23. 34. 44.]\n",
      " [ 4. 13. 14. ... 38. 45. 47.]]\n",
      "(1336, 21) (1336, 7)\n",
      "(149, 21) (149, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:9: DataConversionWarning: Data with input dtype int64 were all converted to float64 by the scale function.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  Atleast 1 matched:  168.4563758389262  Training:  100.0 ( 1336 ) Test:  68.45637583892618 ( 149 )\n",
      "149 [[ 3  9 14 ... 34 41 43]\n",
      " [ 2  9 19 ... 40 31 46]\n",
      " [ 3  9 19 ... 33 35 45]\n",
      " ...\n",
      " [ 8 13  7 ... 37 35 41]\n",
      " [13 12 24 ... 38 48 36]\n",
      " [ 5 13 23 ... 40 34 42]]\n"
     ]
    }
   ],
   "source": [
    "#MultiOutput\n",
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = lresult\n",
    "print(y)\n",
    "#Input dataset, y\n",
    "scaled_X = preprocessing.scale(dataset)\n",
    "# create training and testing vars\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, y.astype(int), test_size=0.1, shuffle=False)\n",
    "print (X_train.shape, y_train.shape)\n",
    "print (X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "# Necessary imports: \n",
    "#from sklearn.cross_validation import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "oModelList = [\n",
    "        MultiOutputClassifier(MLPClassifier(hidden_layer_sizes=(28,28,28,28,28), max_iter=500, alpha=0.1,\n",
    "                          activation='relu', solver='lbfgs', early_stopping=True, verbose=False, shuffle=False, random_state=21,tol=0.000001)),\n",
    "\n",
    "#        MultiOutputClassifier(MLPClassifier(hidden_layer_sizes=(28,28,28,28,28), max_iter=100, alpha=0.1,\n",
    "#                          activation='relu', solver='lbfgs', early_stopping=True, verbose=False, shuffle=False, random_state=21,tol=0.000001)),\n",
    "\n",
    "#         MultiOutputClassifier(MLPClassifier(hidden_layer_sizes=(51,51,51,51,51,51), max_iter=23, alpha=0.1,\n",
    "#                           activation='relu', solver='lbfgs', early_stopping=True, verbose=False, shuffle=False, random_state=21,tol=0.000001)),\n",
    "\n",
    "#         MultiOutputClassifier(MLPClassifier(hidden_layer_sizes=(21,21,21,21,21,21), max_iter=23, alpha=0.1,\n",
    "#                           activation='relu', solver='lbfgs', early_stopping=True, verbose=False, shuffle=False, random_state=21,tol=0.000001)),\n",
    "\n",
    "\n",
    "#        MultiOutputClassifier(MLPClassifier(hidden_layer_sizes=(7,14,7,14,7,14,7,14), max_iter=23, alpha=0.01,\n",
    "#                          activation='relu', solver='sgd', early_stopping=True, verbose=False, shuffle=False, random_state=21,tol=0.000001)),\n",
    "\n",
    "#        MultiOutputClassifier(MLPClassifier(hidden_layer_sizes=(17,17,17,17,17,17,17,17,17), max_iter=23, alpha=0.01,\n",
    "#                         activation='relu', solver='sgd', early_stopping=True, verbose=False, shuffle=False, random_state=21,tol=0.000001)),\n",
    "\n",
    "#        MultiOutputClassifier(MLPClassifier(hidden_layer_sizes=(7,14,7,14,7,14,7,14), max_iter=23, alpha=0.01,\n",
    "#                          activation='relu', solver='lbfgs', early_stopping=True, verbose=False, shuffle=False, random_state=21,tol=0.000001)),\n",
    "\n",
    "\n",
    "#        MultiOutputClassifier(MLPClassifier(hidden_layer_sizes=(7,7,7,7,7,7,7,7,7), max_iter=23, alpha=0.01,\n",
    "#                          activation='relu', solver='lbfgs', early_stopping=True, verbose=False, shuffle=False, random_state=21,tol=0.000001)),\n",
    "#        MultiOutputClassifier(MLPClassifier(hidden_layer_sizes=(50,50,50,50), max_iter=10, alpha=0.01,\n",
    "#                         activation='relu', solver='sgd', early_stopping=True, verbose=False, shuffle=False, random_state=21,tol=0.000001)),\n",
    "#        MultiOutputClassifier(MLPClassifier(hidden_layer_sizes=(50,50,50,50), max_iter=10, alpha=0.1,\n",
    "#                         activation='relu', solver='lbfgs', early_stopping=True, verbose=False, shuffle=False, random_state=21,tol=0.000001)),\n",
    "#        MultiOutputClassifier(MLPClassifier(hidden_layer_sizes=(23), max_iter=19, alpha=0.1,\n",
    "#                          activation='relu', solver='lbfgs', early_stopping=True, verbose=False, shuffle=False, random_state=21,tol=0.000001)),\n",
    "#        MultiOutputClassifier(MLPClassifier(hidden_layer_sizes=(30,30,30,30,30), max_iter=23, alpha=0.1,\n",
    "#                          activation='relu', solver='lbfgs', early_stopping=True, verbose=False, shuffle=False, random_state=21,tol=0.000001)),\n",
    "#        MultiOutputClassifier(MLPClassifier(hidden_layer_sizes=(30,30,30,30,30,30), max_iter=23, alpha=0.1,\n",
    "#                          activation='relu', solver='lbfgs', early_stopping=True, verbose=False, shuffle=False, random_state=21,tol=0.000001)),\n",
    "#        MultiOutputRegressor(MLPRegressor(hidden_layer_sizes=(20,20,20,20,20,20), max_iter=23, alpha=0.1,\n",
    "#                          activation='relu', solver='lbfgs', early_stopping=True, verbose=False, shuffle=False, random_state=21,tol=0.000001)),\n",
    "#        MultiOutputRegressor(MLPRegressor(hidden_layer_sizes=(10,10,10,10,10,10), max_iter=23, alpha=0.01,\n",
    "#                          activation='relu', solver='lbfgs', early_stopping=True, verbose=False, shuffle=False, random_state=21,tol=0.000001)),\n",
    "#        MultiOutputRegressor(MLPRegressor(hidden_layer_sizes=(100,100,100,100), max_iter=1000, alpha=0.01,\n",
    "#                          activation='relu', solver='sgd', early_stopping=True, verbose=False, shuffle=False, random_state=21,tol=0.000001)),\n",
    "#        MultiOutputRegressor(MLPRegressor(hidden_layer_sizes=(100,100,100,100,100,100), max_iter=23, alpha=0.01,\n",
    "#                          activation='relu', solver='lbfgs', early_stopping=True, verbose=False, shuffle=False, random_state=21,tol=0.000001)),\n",
    "#        MultiOutputRegressor(MLPRegressor(hidden_layer_sizes=(40,40,40,40,40,40), max_iter=23, alpha=0.01,\n",
    "#                          activation='relu', solver='lbfgs', early_stopping=True, verbose=False, shuffle=False, random_state=21,tol=0.000001)),\n",
    "#        MultiOutputClassifier(MLPClassifier(hidden_layer_sizes=(40,40,40,40,40,40), max_iter=100, alpha=0.001,\n",
    "#                          activation='tanh', solver='sgd', early_stopping=True, verbose=False, shuffle=False, random_state=21,tol=0.000001)),\n",
    "#        MultiOutputClassifier(MLPClassifier(hidden_layer_sizes=(10,10,10,10,10,10), max_iter=100, alpha=0.001,\n",
    "#                          activation='tanh', solver='sgd', early_stopping=True, verbose=False, shuffle=False, random_state=21,tol=0.000001)),\n",
    "    ]\n",
    "\n",
    "prediction_results = []\n",
    "i = 0\n",
    "for m in oModelList:\n",
    "    # m = MLPRegressor(\n",
    "    #     hidden_layer_sizes=(10,10),  activation='tanh', solver='lbfgs', alpha=0.001, batch_size='auto',\n",
    "    #     learning_rate='constant', learning_rate_init=0.1, power_t=0.5, max_iter=1000, shuffle=False,\n",
    "    #     random_state=9, tol=0.000001, verbose=10, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n",
    "    #     early_stopping=True, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "    model = m.fit(X_train, [list(item) for item in y_train])\n",
    "    ytrainPredicted = m.predict(X_train)\n",
    "    #ytestPredicted = m.predict(X_test)\n",
    "    ytestPredicted = m.predict(X_test).astype(int) #, [list(item) for item in y_train])\n",
    "\n",
    "    training_score = (getAccuracyCount(ytrainPredicted, y_train))\n",
    "    testing_score = (getAccuracyCount(ytestPredicted, y_test))\n",
    "    total_score = (training_score + testing_score) #- ( training_score - testing_score )\n",
    "\n",
    "    i = i + 1\n",
    "    print ( i, \" Atleast 1 matched: \", total_score, ' Training: ', training_score, '(', len(ytrainPredicted), ') Test: ', testing_score, '(', len(ytestPredicted), ')')\n",
    "    print(len(ytestPredicted),ytestPredicted)\n",
    "    prediction_results.append(ytestPredicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "af2e16015f08c835bacafb8b82dafee43e5016fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Predictions#  0\n",
      "Final BestCombo  [] []\n"
     ]
    }
   ],
   "source": [
    "def getCombinedPrediction(lst, ff):\n",
    "    comb_predicted = []\n",
    "    for p in range(1,len(ff)):\n",
    "        if len(comb_predicted) == 0: \n",
    "            comb_predicted = lst[p]\n",
    "            continue\n",
    "        comb_predicted = np.hstack((comb_predicted, lst[p])) #np.reshape(prediction_results[p], (-1,1))))\n",
    "    comb_predicted = [np.unique(x) for x in comb_predicted]\n",
    "    return comb_predicted\n",
    "\n",
    "iBestCombo = []\n",
    "iBestN = []\n",
    "predicted = []\n",
    "actual = y_test\n",
    "arr = []\n",
    "print(\"Total Predictions# \", len(prediction_results))\n",
    "for z in range(1, len(prediction_results)):\n",
    "    for x in itertools.combinations([ x for x in range(1,len(prediction_results))],z):\n",
    "        print(x)\n",
    "    \n",
    "#Determine the best combination\n",
    "for i in range(0, len(arr)):\n",
    "    predicted = getCombinedPrediction(prediction_results, arr[i])\n",
    "    print(','.join(str(x) for x in arr[i]))\n",
    "    plt.hist(getAccuracy(predicted,actual))\n",
    "    plt.show()\n",
    "    N = getCounts(predicted,actual)\n",
    "    if sum(iBestN) < sum(N):\n",
    "        iBestResult = predicted\n",
    "        iBestIndex = i\n",
    "        iBestN = N\n",
    "        iBestCombo = arr[i]\n",
    "        print(\"Best: \", iBestCombo, arr[i])\n",
    "\n",
    "print(\"Final BestCombo \", iBestCombo, iBestN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "62482f173c4c7938cf52817e99d58ea53b715d47"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getMatches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-50e00e3ee6f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetMatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprintResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'getMatches' is not defined"
     ]
    }
   ],
   "source": [
    "matches = getMatches(predicted,actual)\n",
    "\n",
    "printResult(predicted, actual)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "705434acedb112a828ed89908433ea2e75ab5961",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "actual = y_test\n",
    "predicted = [];\n",
    "##Use the following for single output\n",
    "# for p in range(0,len(prediction_results)):\n",
    "# #      print(np.reshape(prediction_results[p], (-1,1)))\n",
    "#     if len(predicted) == 0: \n",
    "#         predicted = np.reshape(prediction_results[p], (-1,1))\n",
    "#         continue\n",
    "#     predicted = np.hstack((predicted, np.reshape(prediction_results[p], (-1,1))))\n",
    "\n",
    "##Use this for MultiOutput\n",
    "predicted = [];\n",
    "for i in range(0, len(prediction_results)):\n",
    "    predicted = [];\n",
    "#    for p in range(i,i+1):\n",
    "    for p in range(0,i):\n",
    "    #      print(np.reshape(prediction_results[p], (-1,1)))\n",
    "        if len(predicted) == 0: \n",
    "            predicted = prediction_results[p] #np.reshape(prediction_results[p], (-1,1))\n",
    "            continue\n",
    "#        predicted = np.intersect1d(predicted,prediction_results[p])\n",
    "        predicted = np.hstack((predicted, prediction_results[p])) #np.reshape(prediction_results[p], (-1,1))))\n",
    "    predicted = [np.unique(x) for x in predicted]\n",
    "    matched = getAccuracy(predicted,actual)\n",
    "    plt.hist(matched)\n",
    "    plt.show()\n",
    "\n",
    "printResult(predicted, actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fbab7c8bd160c4f1e970d65fbf78d989a4842201"
   },
   "outputs": [],
   "source": [
    "actual = y_test\n",
    "predicted = [];\n",
    "##Use the following for single output\n",
    "# for p in range(0,len(prediction_results)):\n",
    "# #      print(np.reshape(prediction_results[p], (-1,1)))\n",
    "#     if len(predicted) == 0: \n",
    "#         predicted = np.reshape(prediction_results[p], (-1,1))\n",
    "#         continue\n",
    "#     predicted = np.hstack((predicted, np.reshape(prediction_results[p], (-1,1))))\n",
    "\n",
    "##Use this for MultiOutput\n",
    "for p in range(0,len(prediction_results)):\n",
    "#      print(np.reshape(prediction_results[p], (-1,1)))\n",
    "    if len(predicted) == 0: \n",
    "        predicted = prediction_results[p] #np.reshape(prediction_results[p], (-1,1))\n",
    "        continue\n",
    "    predicted = np.hstack((predicted, prediction_results[p])) #np.reshape(prediction_results[p], (-1,1))))\n",
    "\n",
    "predicted = [np.unique(x) for x in predicted]\n",
    "printResult(predicted, actual)\n",
    "#printResult(predicted, actual)\n",
    "#print(getMatches(ytestPredicted,actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fb362538f82f8f19080084ae30992764e1e9da7d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actual = y_test\n",
    "ytestPredicted = predicted\n",
    "\n",
    "matched = getAccuracy(ytestPredicted,actual)\n",
    "\n",
    "plt.hist(matched)\n",
    "plt.show()\n",
    "\n",
    "#print(matched_count)\n",
    "matches = getMatches(ytestPredicted,actual)\n",
    "unique, counts = np.unique(matched, return_counts=True)\n",
    "iTotal = len(actual)\n",
    "#print(matches)\n",
    "for idx, x in enumerate(counts):\n",
    "    print ( \" {} Matched {} predicted number Accuracy {}% \".format(x, idx, x/iTotal*100.) )\n",
    "\n",
    "def printResult(predictions, actual):\n",
    "    print(list(predictions))\n",
    "#     df_result=pd.DataFrame({ 'N':list([len(x) for x in predictions]), 'Predicted':list(predictions), 'Actual':list(actual)})\n",
    "#     print(df_result)  \n",
    "\n",
    "#showResult(\"Test\", predicted, actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "68fea1d2e1e210858620ee6c7cf43b6c20ef914b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Input dataset, y\n",
    "scaled_X = preprocessing.scale(dataset)\n",
    "# create training and testing vars\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.1, shuffle=False)\n",
    "print (X_train.shape, y_train.shape)\n",
    "print (X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "# Necessary imports: \n",
    "from sklearn.cross_validation import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "fit_rf = RandomForestClassifier(random_state=21)\n",
    "\n",
    "param_dist = {'max_depth': [2, 3, 4],\n",
    "              'bootstrap': [True, False],\n",
    "              'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "              'criterion': ['gini', 'entropy']}\n",
    "\n",
    "m = GridSearchCV(fit_rf, cv = 10,\n",
    "                     param_grid=param_dist, \n",
    "                     n_jobs = 3)\n",
    "\n",
    "# m = MLPRegressor(\n",
    "#     hidden_layer_sizes=(10,10),  activation='tanh', solver='lbfgs', alpha=0.001, batch_size='auto',\n",
    "#     learning_rate='constant', learning_rate_init=0.1, power_t=0.5, max_iter=1000, shuffle=False,\n",
    "#     random_state=9, tol=0.000001, verbose=10, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n",
    "#     early_stopping=True, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "#model = m.fit(X_train, y_train)\n",
    "#print('Best RandomForest Parameters using grid search: \\n', \n",
    "#      m.best_params_)\n",
    "# {'bootstrap': False, 'criterion': 'entropy', 'max_depth': 2, 'max_features': 'auto'}\n",
    "# Set best parameters given by grid search \n",
    "fit_rf.set_params(criterion = 'entropy',\n",
    "                  max_features = 'auto', \n",
    "                  max_depth = 2)\n",
    "\n",
    "fit_rf.set_params(warm_start=True, \n",
    "                  oob_score=True)\n",
    "\n",
    "min_estimators = 15\n",
    "max_estimators = 1000\n",
    "\n",
    "# error_rate = {}\n",
    "\n",
    "for i in range(min_estimators, max_estimators + 1):\n",
    "    fit_rf.set_params(n_estimators=i)\n",
    "    fit_rf.fit(X_train, y_train)\n",
    "\n",
    "    oob_error = 1 - fit_rf.oob_score_\n",
    "    error_rate[i] = oob_error\n",
    "    print(i, oob_error)\n",
    "    \n",
    "#fit_rf.fit(X_train, y_train)\n",
    "predictions = fit_rf.predict(X_train)\n",
    "print(\"Train Predicted\", predictions.astype(int))\n",
    "print(\"Train Actuals  \", y_train)\n",
    "print(\"Train score    \", m.score(X_train,y_train))\n",
    "\n",
    "\n",
    "# Perform 6-fold cross validation\n",
    "#scores = cross_val_score(model, X_train, y_train, cv=3)\n",
    "#print('Crossvalidated scores', scores)\n",
    "#Cross-validated scores: [ 0.4554861   0.46138572  0.40094084  0.55220736  0.43942775  0.56923406]\n",
    "    \n",
    "# Make cross validated predictions\n",
    "#predictions = cross_val_predict(model, X_train, y_train, cv=6)\n",
    "#print(\"CV Train Predicted\", predictions.astype(int))\n",
    "\n",
    "#accuracy = metrics.r2_score(y_train, predictions)\n",
    "#print ('Cross-Predicted Accuracy:', accuracy)\n",
    "\n",
    "predictions = fit_rf.predict(X_test)\n",
    "print(\"Test  Predicted\", predictions.astype(int))\n",
    "print(\"Test  Actuals  \", y_test)\n",
    "print(\"Test  score    \", m.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5509bcb2bff870970d3b184b733414097d7ee84c"
   },
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "# fp = np.concatenate((y_test,Ypred2),axis=1)\n",
    "# np.savetxt(\"RF\" + \"\".join(cols) + \".csv\", fp, delimiter=\",\", header=\"N1,N2,N3,N4,N5,N6,N7,P1,P2,P3,P4,P5,P6,P7\",fmt='%d')\n",
    "\n",
    "#import the data, specify data types\n",
    "import pandas as pd\n",
    "pp = pd.read_csv('../input/2018tr/PP.csv')\n",
    "print(pp.tail())\n",
    "\n",
    "lr = pd.read_csv('../input/2018tr/SGH.csv')\n",
    "#print(lr.describe())\n",
    "\n",
    "#print(lr)\n",
    "\n",
    "#print(len(lr))\n",
    "#lr = lr.sort_values(by=['D'])\n",
    "#lr = lr.drop_duplicates () ;\n",
    "print(len(lr))\n",
    "cols = ['D', 'N1','N2','N3','N4','N5','N6','N7']\n",
    "lr = lr[cols]\n",
    "print(lr.head(30))\n",
    "\n",
    "#https://pandas.pydata.org/pandas-docs/stable/merging.html\n",
    "df = pd.concat([pp, lr], axis=1, sort=False)\n",
    "df = df.dropna()\n",
    "print(len(df))\n",
    "df.head()\n",
    "df.reset_index().drop(['D'], axis=1)\n",
    "\n",
    "cols = ['N1','N2','N3','N4','N5','N6','N7']\n",
    "lr = df[cols]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d63394f99f72b3e4245ffda711d233f71d91219"
   },
   "outputs": [],
   "source": [
    "lresult = np.sort(lr.values[:, ::-1])\n",
    "print(lresult)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c6356a5741e6f015b8e1ca6c80f7a43fafded81a"
   },
   "outputs": [],
   "source": [
    "cols = ['L','M','S','R','E','A','V']\n",
    "X = df[cols]\n",
    "\n",
    "\n",
    "col_n = 1  #Column Number interested\n",
    "aa = np.delete(lresult, np.s_[col_n:], axis=1)  \n",
    "aa = np.delete(aa, np.s_[0:col_n-1], axis=1)  \n",
    "print(aa)\n",
    "#Convert 2d array to Dataframe\n",
    "y = pd.DataFrame(aa, columns=list('N'))\n",
    "y.head()\n",
    "y = aa \n",
    "print ( len(aa) )\n",
    "\n",
    "def extractOutliers(df, lower_bound, upper_bound):\n",
    "    t_y = pd.DataFrame(df, columns=list('N'))\n",
    "    if ( lower_bound == None):\n",
    "        res = t_y.N.quantile([upper_bound], interpolation='higher')\n",
    "    else:\n",
    "        res = t_y.N.quantile([lower_bound, upper_bound])\n",
    "    return res, t_y\n",
    "\n",
    "def removeOutliers(df, lower_bound, upper_bound):\n",
    "    res, t_y = extractOutliers(df, lower_bound, upper_bound) \n",
    "    if lower_bound == None:\n",
    "        true_index = (t_y.N.values < res.loc[upper_bound])\n",
    "    else:\n",
    "        true_index = (res.loc[lower_bound] < t_y.N.values) & (t_y.N.values < res.loc[upper_bound])\n",
    "    return t_y[true_index], true_index\n",
    "\n",
    "y = lresult\n",
    "y, true_index = removeOutliers(aa, None, .98)\n",
    "y = y.values\n",
    "scaled_X = preprocessing.scale(X)\n",
    "scaled_X = scaled_X[true_index]\n",
    "print(len(y), len(scaled_X))\n",
    "# print(y.head())\n",
    "# print(lresult)\n",
    "print(scaled_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a0e6837a9d6b8e7c97abbd202a3295681d832a6a"
   },
   "outputs": [],
   "source": [
    "#shuffle the data\n",
    "#shuffled_indices = np.arange(scaled_X.shape[0])\n",
    "#np.random.shuffle(shuffled_indices)\n",
    "\n",
    "#shuffled_X = scaled_X[shuffled_indices]\n",
    "#shuffled_y = y[shuffled_indices]\n",
    "\n",
    "shuffled_X = scaled_X\n",
    "\n",
    "shuffled_y = y.astype(int)\n",
    "#shuffled_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3fe5f7d668ac47a9583b3d1b1cfbfb4166fa7466"
   },
   "outputs": [],
   "source": [
    "samples_count = shuffled_X.shape[0]\n",
    "print(samples_count)\n",
    "\n",
    "train_samples_count = int(0.8*samples_count)\n",
    "validation_samples_count = int(0.1*samples_count)\n",
    "test_samples_count = samples_count - ( train_samples_count + validation_samples_count)\n",
    "#print(test_samples_count)\n",
    "\n",
    "train_X = shuffled_X[:train_samples_count]\n",
    "train_y = shuffled_y[:train_samples_count]\n",
    "\n",
    "validation_X = shuffled_X[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_y = shuffled_X[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "test_X = shuffled_X[train_samples_count+validation_samples_count]\n",
    "test_y = shuffled_y[train_samples_count+validation_samples_count]\n",
    "\n",
    "#By right if we have balanced distributrion of the target, the last column should be approximately .50\n",
    "print(np.sum(train_X),train_samples_count,np.sum(train_X)/train_samples_count)\n",
    "print(np.sum(validation_X),validation_samples_count,np.sum(validation_X)/validation_samples_count)\n",
    "print(np.sum(test_X),test_samples_count,np.sum(test_X)/test_samples_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dee9c50a405a72966558cd819544be1d59b3fec9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "modelNames = [#\"MLkNN\", \n",
    "        \"Ridge\", \n",
    "        \"Nearest Neighbors\" #, \"T\",\"U\" #,\"F\",\"G\",\"K\"\n",
    "#         , \"Linear SVM\", \"RBF SVM\",\n",
    "#         \"Decision Tree\", \"AdaBoost\", \"Random Forest\", \"Neural Net\",\n",
    "#         \"Naive Bayes\", \"Gaussian Process\", \"QDA\",\n",
    "#         \"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\",\n",
    "#         \"Decision Tree\", \"AdaBoost\", \"Random Forest\", \"Neural Net\",\n",
    "#         \"Naive Bayes\", \"Gaussian Process\", \"QDA\"\n",
    "    ]\n",
    "seed = 0\n",
    "classifiers = [\n",
    "#    MultiOutputClassifier(LinearDiscriminantAnalysis()),\n",
    "#    MLPRegressor(alpha=1),\n",
    "    (AdaBoostClassifier(random_state=seed)),\n",
    "#    MultiOutputRegressor(RandomForestRegressor(max_depth=5, n_estimators=10, max_features=1,random_state=seed)),\n",
    "\n",
    "\n",
    "#    MultiOutputClassifier(GaussianNB()),\n",
    "\n",
    "#     MultiOutputClassifier(SVC(kernel=\"linear\", C=0.025,random_state=seed)),\n",
    "#     MultiOutputClassifier(SVC(kernel=\"linear\", C=0.05,random_state=seed)),\n",
    "\n",
    "#    (KNeighborsClassifier(15)),\n",
    "\n",
    "#    (GaussianNB()),\n",
    "#    RandomForestClassifier(max_depth=9, n_estimators=10, max_features=1, random_state = seed),\n",
    "\n",
    "\n",
    "#     SVC(gamma=.1, C=90,random_state=seed),\n",
    "#     (Ridge(alpha=.5,random_state=seed)),\n",
    "\n",
    "#    RandomForestClassifier(max_depth=9, n_estimators=10, max_features=1, random_state = seed),\n",
    "#     (RandomForestClassifier(max_depth=8, n_estimators=10, max_features=1)),\n",
    "\n",
    "\n",
    "\n",
    "#    (KNeighborsRegressor(11)),\n",
    "#    (KNeighborsClassifier(17)),\n",
    "#     MultiOutputClassifier(KNeighborsClassifier(19)),\n",
    "#     MultiOutputClassifier(KNeighborsClassifier(15)),\n",
    "#     MultiOutputClassifier(KNeighborsClassifier(16)),\n",
    "\n",
    "\n",
    "    (KNeighborsRegressor(2)),\n",
    "    (KNeighborsClassifier(2)),\n",
    "    MultiOutputRegressor(KNeighborsRegressor(25)),\n",
    "    MultiOutputClassifier(KNeighborsClassifier(45)),\n",
    "    MultiOutputRegressor(KNeighborsRegressor(90)),\n",
    "    MultiOutputClassifier(KNeighborsClassifier(14)),\n",
    "    MultiOutputClassifier(KNeighborsClassifier(7)),\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#    MultiOutputClassifier(RidgeClassifier(random_state=seed)),\n",
    "\n",
    "\n",
    "#     MultiOutputRegressor(LogisticRegression()),\n",
    "#        MultiOutputClassifier(MLPRegressor(alpha=1)),\n",
    "#    MLkNN(k=20),\n",
    "    MultiOutputClassifier(SVC(gamma=2, C=1,random_state=seed)),\n",
    "#    MultiOutputClassifier(DecisionTreeClassifier(max_depth=43)),\n",
    "#    MultiOutputClassifier(RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)),\n",
    "\n",
    "#    MultiOutputClassifier(GaussianNB()),\n",
    "#    MultiOutputClassifier(GaussianProcessClassifier(1.0 * RBF(1.0))),\n",
    "#    MultiOutputClassifier(QuadraticDiscriminantAnalysis()),\n",
    "    \n",
    "#    MultiOutputRegressor(Ridge()),\n",
    "#    MultiOutputRegressor(DecisionTreeRegressor(max_depth=43)),\n",
    "    \n",
    "#    MultiOutputClassifier(MLPClassifier(alpha=1)),\n",
    "#    MultiOutputClassifier(GaussianNB()),\n",
    "#    MultiOutputClassifier(GaussianProcessClassifier(1.0 * RBF(1.0))),\n",
    "#    MultiOutputClassifier(QuadraticDiscriminantAnalysis()),\n",
    "\n",
    "\n",
    "    ]\n",
    "\n",
    "#X = shuffled_X\n",
    "#scaler = preprocessing.MinMaxScaler()\n",
    "#X = scaler.fit_transform(shuffled_X)\n",
    "\n",
    "#lab_enc = preprocessing.LabelEncoder()\n",
    "#encoded_y = lab_enc.fit_transform(shuffled_y)\n",
    "#print(utils.multiclass.type_of_target(encoded_y))\n",
    "#print(encoded_y)\n",
    "#y = shuffled_y ;\n",
    "\n",
    "y = y.astype(int)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(scaled_X,y, test_size= 0.10, shuffle=False)\n",
    "\n",
    "# results = []\n",
    "# names = []\n",
    "# scoring = 'accuracy'\n",
    "# seed = 2\n",
    "# for name, model in zip(modelNames, classifiers):\n",
    "#     print(name)\n",
    "#     kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "#     cv_results = model_selection.cross_val_score(model, x_train, y_train, cv=kfold, scoring=scoring)\n",
    "#     results.append(cv_results)\n",
    "#     names.append(name)\n",
    "#     msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "#     print(msg)\n",
    "#print(names)\n",
    "\n",
    "prediction_results = []\n",
    "iIndex = 0\n",
    "\n",
    "\n",
    "\n",
    "for name, clf in zip(modelNames, classifiers):\n",
    "    pipe_svr = Pipeline([\n",
    "#        ('scl', MinMaxScaler()),\n",
    "        ('reg', clf)\n",
    "        ])\n",
    "    grid_param_svr = {\n",
    "#        \"reg__estimators__alpha\" = [0.1, 0.05, 0.01]\n",
    "    }\n",
    "\n",
    "    alg = clf \n",
    "#     (GridSearchCV(estimator=pipe_svr, \n",
    "#                        param_grid=grid_param_svr, \n",
    "#                        cv=10,\n",
    "#                        scoring = 'neg_mean_squared_error',\n",
    "#                        n_jobs = -1))\n",
    "    alg.fit(x_train, y_train.ravel()) #[list(item) for item in y_train])\n",
    "#    print(\" Training \", accuracy_score(y_train, alg.predict(x_train))*100)\n",
    "#    ypred = alg.predict(x_test) #, [list(item) for item in y_train])\n",
    "#    print(\" Test     \", accuracy_score(y_test, ypred)*100)\n",
    "    \n",
    "    ytrainPredicted = alg.predict(x_train).astype(int)\n",
    "#    print(ytrainPredicted.astype(int))\n",
    "#    print(y_train.ravel())\n",
    "\n",
    "    ytestPredicted = alg.predict(x_test).astype(int) #, [list(item) for item in y_train])\n",
    "\n",
    "    training_score = (getAccuracy1dPercentCorrect(ytrainPredicted.astype(int), y_train))\n",
    "    testing_score = (getAccuracy1dPercentCorrect(ytestPredicted.astype(int), y_test))\n",
    "\n",
    "    total_score = (training_score + testing_score) #- ( training_score - testing_score )\n",
    "    iIndex = iIndex+1\n",
    "    print (iIndex, name + \" Score: \", total_score, ' Training: ', training_score, '(', len(ytrainPredicted), ') Test: ', testing_score, '(', len(ytestPredicted), ')')\n",
    "    print(len(ytestPredicted),ytestPredicted)\n",
    "    prediction_results.append(ytestPredicted.astype(int))\n",
    "    \n",
    "#odel = MLPClassifier(hidden_layer_sizes=(40,40,40), max_iter=100, alpha=0.1,\n",
    "#                    solver='adam',activation='tanh', verbose=10,  random_state=0,tol=0.000000001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b4d647d50a767103c876629d18e5ffd88327d9ea"
   },
   "outputs": [],
   "source": [
    "#plt.boxplot(y_test)\n",
    "\n",
    "#prediction_results = []\n",
    "prediction_results.append(ytestPredicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5f0a04104ff01bdc112701a4002103d3f5bfb376"
   },
   "outputs": [],
   "source": [
    "actual = y_test\n",
    "predicted = [];\n",
    "for p in range(0,len(prediction_results)):\n",
    "#      print(np.reshape(prediction_results[p], (-1,1)))\n",
    "    if len(predicted) == 0: \n",
    "        predicted = np.reshape(prediction_results[p], (-1,1))\n",
    "        continue\n",
    "    predicted = np.hstack((predicted, np.reshape(prediction_results[p], (-1,1))))\n",
    "    \n",
    "#print(predicted)\n",
    "\n",
    "#print(getAccuracy(predicted,actual))\n",
    "matched = getAccuracy(predicted,actual)\n",
    "matches = getMatches(predicted,actual)\n",
    "print(matches)\n",
    "unique, counts = np.unique(matched, return_counts=True)\n",
    "iTotal = len(actual)\n",
    "iMatchN = 1\n",
    "iAtleastN = iTotal - dict(zip(unique, counts))[iMatchN-1]\n",
    "\n",
    "print ( \"Union Accuracy: \", iAtleastN/iTotal*100. )\n",
    "\n",
    "matched_count = getAccuracy(predicted, y_test)\n",
    "\n",
    "plt.hist(matched_count)\n",
    "plt.show()\n",
    "\n",
    "print(matched_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8f8645d7789e88fccbec2b7924a9abb9a7c475f9"
   },
   "outputs": [],
   "source": [
    "matched_count = getAccuracy(predicted, y_test)\n",
    "\n",
    "plt.hist(matched_count)\n",
    "plt.show()\n",
    "\n",
    "#print(matched_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "53fca4d2ac5944e7d10fbdf04e48c46484a9ddca"
   },
   "outputs": [],
   "source": [
    "printPredictions(predicted,actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d0f365229dee9698427834feaf79989bf2d26774"
   },
   "outputs": [],
   "source": [
    "def knnCombinations(st,ed):\n",
    "    iBestI = 0 \n",
    "    iBestJ = 0\n",
    "    iHighest = 0\n",
    "    iLowest = 10000\n",
    "    for i in range(st,ed):\n",
    "        alg1 = MultiOutputClassifier(KNeighborsRegressor(i))\n",
    "        alg1.fit(x_train, [list(item) for item in y_train])\n",
    "        p1 = alg1.predict(x_test)\n",
    "        for j in range(st,ed):\n",
    "#            if i >= j: continue\n",
    "            alg1 = MultiOutputClassifier(KNeighborsClassifier(j))            \n",
    "            alg1.fit(x_train, [list(item) for item in y_train])\n",
    "            predicted = getUnion(p1, alg1.predict(x_test))\n",
    "            N = getCounts(predicted,actual)\n",
    "            if ( sum(N) > iHighest ):\n",
    "                print('H ',sum(N), \"union of knn \", i, j)\n",
    "                iHighest = sum(N)\n",
    "            if ( sum(N) < iLowest):\n",
    "                print('L ',sum(N), \"union of knn \", i, j)\n",
    "                iLowest = sum(N)\n",
    "\n",
    "    \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(shuffled_X,y, test_size= 0.10)\n",
    "knnCombinations(10,20)\n",
    "print ( 'Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "058d0790c2d55ca6a0a950500bb781247d335ccc"
   },
   "outputs": [],
   "source": [
    "def rfCombinations(st,ed):\n",
    "    iBestI = 0 \n",
    "    iBestJ = 0\n",
    "    iHighest = 0\n",
    "    iLowest = 10000\n",
    "#    for i in range(st,ed):\n",
    "    step = 0.05\n",
    "    for i in range(st, ed):\n",
    "#         alg1 = MultiOutputClassifier(RandomForestClassifier(max_depth=i, n_estimators=100, max_features=1,random_state=seed))\n",
    "#        alg1 = MultiOutputClassifier(Ridge(alpha=i,random_state=seed))\n",
    "        alg1 = MultiOutputClassifier(SVC(gamma=2, C=i,random_state=seed))\n",
    "\n",
    "        alg1.fit(x_train, [list(item) for item in y_train])\n",
    "        p1 = alg1.predict(x_test)\n",
    "#         for j in range(0.05,0.3,0.05):\n",
    "        for j in range(st, ed):\n",
    "            if i >= j: continue\n",
    "#             alg1 = MultiOutputClassifier(RandomForestClassifier(max_depth=j, n_estimators=100, max_features=1,random_state=seed))\n",
    "#            alg = MultiOutputClassifier(Ridge(alpha=j,random_state=seed))\n",
    "            alg = MultiOutputClassifier(SVC(gamma=2, C=j,random_state=seed))\n",
    "#             alg = MultiOutputClassifier(KNeighborsClassifier(j))            \n",
    "            alg.fit(x_train, [list(item) for item in y_train])\n",
    "            predicted = getUnion(p1, alg.predict(x_test))\n",
    "            N = getCounts(predicted,actual)\n",
    "            if ( sum(N) > iHighest ):\n",
    "                print('H ',sum(N), \"union of knn \", i, j)\n",
    "                iHighest = sum(N)\n",
    "            if ( sum(N) < iLowest):\n",
    "                print('L ',sum(N), \"union of knn \", i, j)\n",
    "                iLowest = sum(N)\n",
    "\n",
    "    \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(shuffled_X,y, test_size= 0.10)\n",
    "rfCombinations(1,5)\n",
    "print ( 'Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

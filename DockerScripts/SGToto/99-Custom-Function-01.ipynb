{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 9)\n"
     ]
    }
   ],
   "source": [
    "# Generate predictors\n",
    "X_raw = np.reshape(np.random.random(100*9), (100, 9))\n",
    "\n",
    "# Standardize the predictors\n",
    "scaler = StandardScaler().fit(X_raw)\n",
    "X = scaler.transform(X_raw)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2.04110388  -8.5724505   -7.23085155   6.87484683  -7.42147162\n",
      "   5.5858523   21.31690013  -0.84706693 -22.61726735  -4.77937261\n",
      " -22.7455036   -6.8954092    0.07074924   2.43202914 -10.76550653\n",
      " -41.35170936  -5.65726034  -2.3728673    8.14786604  14.96951487\n",
      "   4.47156115 -29.28242162   5.49649993 -26.4347966    7.57117087\n",
      "  22.54042899  -6.33131555  32.09758329 -10.86437351  21.40124674\n",
      " -12.59003552  -3.37852505  15.62912372 -32.56274255   8.62355047\n",
      "  16.23333353  10.43385017 -16.4257275    3.40393089 -11.57441275\n",
      "   0.3948683   -4.35615823 -23.8913256  -10.57974635 -26.56892105\n",
      "  24.51586948  14.85344067  29.83525859 -10.60900179   7.30337067\n",
      "  -4.65095041  -0.40459003  20.18433457  17.57085456  15.21616621\n",
      " -18.54668836 -10.18576955  13.72151126   1.05325125  38.88514474\n",
      "  19.76540699  13.61383201  -0.73306291   6.07779034   4.45245843\n",
      " -31.60316399  10.52112459 -14.54845361  -7.23491191 -19.31832958\n",
      "  23.67091443  -4.33569764  -1.90458896   0.90209002   4.15632061\n",
      "   1.63650061  11.68759141  -4.53093045  19.81388579  -9.71428193\n",
      "  -9.37486438 -11.91748715   1.27927457   3.27396922  -1.80897134\n",
      "  -0.95505624   0.93348415   1.56182966  20.87005513   9.32614396\n",
      "  -6.10683758  -5.40191297  -0.45876274  -0.53435745  12.8111353\n",
      "  -0.92564939  -3.56051843  -7.46955615  -5.53230523  -6.68287299]\n"
     ]
    }
   ],
   "source": [
    "# Lets generate target\n",
    "\n",
    "# Define coefficients\n",
    "some_weights = np.array([2,6,7,3,5,7,1,2,4])\n",
    "Y_truth = np.matmul(X,some_weights)\n",
    "\n",
    "print(Y_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_pred, y_truth, w=None):\n",
    "    y_true = np.array(y_truth)\n",
    "    y_pred = np.array(y_pred)\n",
    "    if np.any(y_true==0):\n",
    "        print(\"Remove zeros from set...\")\n",
    "        idx = np.where(y_true==0)\n",
    "        y_true = np.delete(y_true, idx)\n",
    "        y_pred = np.delete(y_pred, idx)\n",
    "        if type(w) != type(None):\n",
    "            w = np.array(w)\n",
    "            w = np.delete(w, idx)     \n",
    "    if type(w) == type(None):\n",
    "        return(np.mean(np.abs((y_true - y_pred) / y_true)) * 100)\n",
    "    return 100/sum(w)*np.dot(w, (np.abs((y_true - y_pred) / y_true)))\n",
    "    \n",
    "custom_loss_function = mean_absolute_percentage_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.99999998 5.99999997 6.99999996 2.99999998 4.99999997 6.99999995\n",
      " 1.         1.99999998 3.99999997]\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def objective_function(w, X, Y):\n",
    "    return custom_loss_function(np.matmul(X,w), Y)\n",
    "\n",
    "Y = Y_truth\n",
    "# add some noise\n",
    "#Y = Y_truth*np.exp(np.random.normal(loc=0.0, scale=0.2, size=100))\n",
    "\n",
    "#print(Y)\n",
    "\n",
    "# provide a starting weights at which to initialize\n",
    "# the parameter search space\n",
    "initial_weights = np.array([1]*X.shape[1])\n",
    "#initial_weights = np.array([np.mean(X)]*X.shape[1])\n",
    "\n",
    "result = minimize(objective_function, initial_weights, args=(X,Y),\n",
    "                  method='BFGS', options={'maxiter': 1000})\n",
    "\n",
    "# The optimal values for the input parameters are stored\n",
    "# in result.x\n",
    "estimated_weights = result.x\n",
    "\n",
    "print(estimated_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correcxt_weights</th>\n",
       "      <th>estimated_weights</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   correcxt_weights  estimated_weights  error\n",
       "0                 2                2.0    0.0\n",
       "1                 6                6.0    0.0\n",
       "2                 7                7.0    0.0\n",
       "3                 3                3.0    0.0\n",
       "4                 5                5.0    0.0\n",
       "5                 7                7.0    0.0\n",
       "6                 1                1.0    0.0\n",
       "7                 2                2.0    0.0\n",
       "8                 4                4.0    0.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    \"correcxt_weights\": some_weights, \n",
    "    \"estimated_weights\": estimated_weights,\n",
    "    \"error\": np.array(np.around(some_weights-estimated_weights,2))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(custom_loss_function(np.matmul(X,estimated_weights), Y),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to factor in the regularization\n",
    "#We need to create a Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManyTargetsModel:\n",
    "    \"\"\"\n",
    "    Linear model: Y = XB, fit by minimizing the provided loss_function\n",
    "    with L2 regularization\n",
    "    \"\"\"\n",
    "    def __init__(self, loss_function, regularization=0.00012):\n",
    "        self.regularization = regularization\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        prediction = np.matmul(X, self.beta)\n",
    "        return(prediction)\n",
    "\n",
    "#     def score(self, X, y_true):\n",
    "#         return(sum(self.predict(X)))\n",
    "\n",
    "\n",
    "    def model_error(self):\n",
    "        error = self.loss_function(\n",
    "            self.predict(self.X), self.Y, w=self.sample_weights\n",
    "        )\n",
    "        return(error)\n",
    "    \n",
    "    def l2_regularized_loss(self, beta):\n",
    "        self.beta = beta\n",
    "        m = len(self.X)\n",
    "#        return (self.model_error())\n",
    "        return(self.model_error()/m + \\\n",
    "                sum(((self.regularization)/2*m)*(np.array(self.beta)**2)))\n",
    "    \n",
    "    def fit(self, X, Y, maxiter=250, sample_weights=None, initial_weights=None):        \n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "        self.beta = None  #latest weights\n",
    "        self.sample_weights = sample_weights\n",
    "        self.estimated_weights = initial_weights\n",
    "        \n",
    "        # Initialize estimated_weights\n",
    "        if type(self.estimated_weights)==type(None):\n",
    "            # set estimated_weights = 1 for every feature\n",
    "            self.estimated_weights = np.array([1]*self.X.shape[1])\n",
    "            \n",
    "        res = minimize(self.l2_regularized_loss, self.estimated_weights\n",
    "                       #, args=(X,Y),\n",
    "                       ,method='BFGS', options={'maxiter': maxiter})\n",
    "        self.beta = res.x\n",
    "        self.estimated_weights = self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.9999998 , 5.99999991, 6.99999968, 2.99999989, 4.99999973,\n",
       "       6.99999967, 1.0000003 , 1.99999989, 4.00000013])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ManyTargetsModel(mean_absolute_percentage_error, regularization=0.000012)\n",
    "model.fit( X, Y)\n",
    "model.estimated_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>initial_weights</th>\n",
       "      <th>estimated_weights</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   initial_weights  estimated_weights  error\n",
       "0                2                2.0    0.0\n",
       "1                6                6.0    0.0\n",
       "2                7                7.0    0.0\n",
       "3                3                3.0    0.0\n",
       "4                5                5.0    0.0\n",
       "5                7                7.0    0.0\n",
       "6                1                1.0   -0.0\n",
       "7                2                2.0    0.0\n",
       "8                4                4.0   -0.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    \"initial_weights\": some_weights, \n",
    "    \"estimated_weights\": model.estimated_weights,\n",
    "    \"error\": np.array(np.around(some_weights-model.estimated_weights,6))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted Y vs. observed Y\n",
    "#plt.scatter(model.predict(X), Y)\n",
    "round(custom_loss_function(np.matmul(X,model.estimated_weights), Y),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Used to cross-validate models and identify optimal lambda\n",
    "class CustomCrossValidator:\n",
    "    \n",
    "    \"\"\"\n",
    "    Cross validates arbitrary model using MAPE criterion on\n",
    "    list of lambdas.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def cross_validate(self, ModelClass, X, Y, lambdas, \n",
    "                        loss_function, \n",
    "                        sample_weights=None,\n",
    "                        num_folds=10):\n",
    "        \"\"\"\n",
    "        lambdas: set of regularization parameters to try\n",
    "        num_folds: number of folds to cross-validate against\n",
    "        \"\"\"\n",
    "        \n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.ModelClass = ModelClass\n",
    "        self.loss_function = loss_function\n",
    "        self.sample_weights = sample_weights\n",
    "    \n",
    "\n",
    "        self.lambdas = lambdas\n",
    "        self.cv_scores = []\n",
    "        X = self.X\n",
    "        Y = self.Y \n",
    "        \n",
    "        # Beta values are not likely to differ dramatically\n",
    "        # between differnt folds. Keeping track of the estimated\n",
    "        # beta coefficients and passing them as starting values\n",
    "        # to the .fit() operator on our model class can significantly\n",
    "        # lower the time it takes for the minimize() function to run\n",
    "        beta_init = None\n",
    "        \n",
    "        for lam in self.lambdas:\n",
    "            print(\"Lambda: {}\".format(lam))\n",
    "            \n",
    "            # Split data into training/holdout sets\n",
    "            kf = KFold(n_splits=num_folds, shuffle=True)\n",
    "            kf.get_n_splits(X)\n",
    "            \n",
    "            # Keep track of the error for each holdout fold\n",
    "            k_fold_scores = []\n",
    "            \n",
    "            # Iterate over folds, using k-1 folds for training\n",
    "            # and the k-th fold for validation\n",
    "            f = 1\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                # Training data\n",
    "                CV_X = X[train_index,:]\n",
    "                CV_Y = Y[train_index]\n",
    "                CV_weights = None\n",
    "                if type(self.sample_weights) != type(None):\n",
    "                    CV_weights = self.sample_weights[train_index]\n",
    "                \n",
    "                # Holdout data\n",
    "                holdout_X = X[test_index,:]\n",
    "                holdout_Y = Y[test_index]\n",
    "                holdout_weights = None\n",
    "                if type(self.sample_weights) != type(None):\n",
    "                    holdout_weights = self.sample_weights[test_index]\n",
    "                \n",
    "                # Fit model to training sample\n",
    "                lambda_fold_model = self.ModelClass(self.loss_function, regularization=lam)\n",
    "                lambda_fold_model.fit(CV_X, CV_Y, sample_weights=CV_weights, initial_weights=beta_init)\n",
    "                \n",
    "                # Extract beta values to pass as beta_init \n",
    "                # to speed up estimation of the next fold\n",
    "                beta_init = lambda_fold_model.beta\n",
    "                \n",
    "                # Calculate holdout error\n",
    "                fold_preds = lambda_fold_model.predict(holdout_X)\n",
    "                fold_mape = self.loss_function(holdout_Y, fold_preds, w=holdout_weights)\n",
    "                k_fold_scores.append(fold_mape)\n",
    "                print(\"Fold: {}. Error: {}\".format( f, fold_mape))\n",
    "                f += 1\n",
    "            \n",
    "            # Error associated with each lambda is the average\n",
    "            # of the errors across the k folds\n",
    "            lambda_scores = np.mean(k_fold_scores)\n",
    "            print(\"** AVERAGE: {}\".format(lambda_scores))\n",
    "            self.cv_scores.append(lambda_scores)\n",
    "        \n",
    "        # Optimal lambda is that which minimizes the cross-validation error\n",
    "        self.lambda_star_index = np.argmin(self.cv_scores)\n",
    "        self.lambda_star = self.lambdas[self.lambda_star_index]\n",
    "        print(\"\\n\\n**BEST LAMBDA: {}**\".format(self.lambda_star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 1\n",
      "Fold: 1. Error: 1343315.7669230883\n",
      "Fold: 2. Error: 10117286.405875206\n",
      "Fold: 3. Error: 502567.5719595025\n",
      "Fold: 4. Error: 6897773.432129745\n",
      "Fold: 5. Error: 866656.001443986\n",
      "** AVERAGE: 3945519.8356663054\n",
      "Lambda: 0.1\n",
      "Fold: 1. Error: 595807.8279506762\n",
      "Fold: 2. Error: 117232.14174044305\n",
      "Fold: 3. Error: 199046.06853327173\n",
      "Fold: 4. Error: 136417.92202956617\n",
      "Fold: 5. Error: 36356.05396702196\n",
      "** AVERAGE: 216972.0028441958\n",
      "Lambda: 0.01\n",
      "Fold: 1. Error: 13928.831982322816\n",
      "Fold: 2. Error: 12191.400491754674\n",
      "Fold: 3. Error: 50647.65520196705\n",
      "Fold: 4. Error: 20154.86605634329\n",
      "Fold: 5. Error: 138511.81373967763\n",
      "** AVERAGE: 47086.91349441309\n",
      "Lambda: 0.001\n",
      "Fold: 1. Error: 5346.244646863285\n",
      "Fold: 2. Error: 1519.5868544844739\n",
      "Fold: 3. Error: 1439.0510586443656\n",
      "Fold: 4. Error: 889.627951195533\n",
      "Fold: 5. Error: 1575.8220999228656\n",
      "** AVERAGE: 2154.066522222104\n",
      "Lambda: 0.0001\n",
      "Fold: 1. Error: 804.3843824454036\n",
      "Fold: 2. Error: 36.3264268232217\n",
      "Fold: 3. Error: 57.44985774160144\n",
      "Fold: 4. Error: 34.89535892709185\n",
      "Fold: 5. Error: 41.16151149630369\n",
      "** AVERAGE: 194.84350748672446\n",
      "Lambda: 1e-05\n",
      "Fold: 1. Error: 7.100610245225817e-07\n",
      "Fold: 2. Error: 5.537417056340805e-07\n",
      "Fold: 3. Error: 6.101964708409395e-07\n",
      "Fold: 4. Error: 5.68368746357997e-07\n",
      "Fold: 5. Error: 6.006688186036582e-07\n",
      "** AVERAGE: 6.086073531918513e-07\n",
      "Lambda: 1e-06\n",
      "Fold: 1. Error: 5.00975007878055e-07\n",
      "Fold: 2. Error: 6.467086912015543e-07\n",
      "Fold: 3. Error: 7.069841770582105e-07\n",
      "Fold: 4. Error: 6.034519595605061e-07\n",
      "Fold: 5. Error: 5.838507889526515e-07\n",
      "** AVERAGE: 6.083941249301955e-07\n",
      "\n",
      "\n",
      "**BEST LAMBDA: 1e-06**\n"
     ]
    }
   ],
   "source": [
    "# specify lambdas values to search\n",
    "lambdas = [1, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001]\n",
    "\n",
    "cross_validator = CustomCrossValidator()\n",
    "cross_validator.cross_validate(ManyTargetsModel, X, Y,  lambdas, custom_loss_function,  num_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with regularization:  1e-06\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>y_truth</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.463340</td>\n",
       "      <td>0.605805</td>\n",
       "      <td>0.022953</td>\n",
       "      <td>-1.143801</td>\n",
       "      <td>0.235715</td>\n",
       "      <td>0.644254</td>\n",
       "      <td>1.276217</td>\n",
       "      <td>-1.254529</td>\n",
       "      <td>-1.377642</td>\n",
       "      <td>0.235711</td>\n",
       "      <td>0.235711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.640901</td>\n",
       "      <td>-0.544786</td>\n",
       "      <td>-1.709071</td>\n",
       "      <td>0.282615</td>\n",
       "      <td>-0.169942</td>\n",
       "      <td>-0.611693</td>\n",
       "      <td>-0.131537</td>\n",
       "      <td>-1.024020</td>\n",
       "      <td>-0.953006</td>\n",
       "      <td>-22.225730</td>\n",
       "      <td>-22.225729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.970912</td>\n",
       "      <td>0.187193</td>\n",
       "      <td>0.221589</td>\n",
       "      <td>-1.385247</td>\n",
       "      <td>-0.244116</td>\n",
       "      <td>0.242864</td>\n",
       "      <td>1.323801</td>\n",
       "      <td>1.097155</td>\n",
       "      <td>0.517549</td>\n",
       "      <td>6.528140</td>\n",
       "      <td>6.528139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.102926</td>\n",
       "      <td>-0.695344</td>\n",
       "      <td>-0.291127</td>\n",
       "      <td>-0.486043</td>\n",
       "      <td>0.042624</td>\n",
       "      <td>0.714035</td>\n",
       "      <td>1.176323</td>\n",
       "      <td>-0.320013</td>\n",
       "      <td>0.941989</td>\n",
       "      <td>2.053385</td>\n",
       "      <td>2.053385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.091495</td>\n",
       "      <td>-0.200845</td>\n",
       "      <td>1.579307</td>\n",
       "      <td>-1.185153</td>\n",
       "      <td>-1.450891</td>\n",
       "      <td>1.515346</td>\n",
       "      <td>0.734429</td>\n",
       "      <td>-0.988471</td>\n",
       "      <td>-1.072902</td>\n",
       "      <td>3.930470</td>\n",
       "      <td>3.930470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.617708</td>\n",
       "      <td>-1.643191</td>\n",
       "      <td>-1.058965</td>\n",
       "      <td>-0.515723</td>\n",
       "      <td>0.322674</td>\n",
       "      <td>-0.080178</td>\n",
       "      <td>0.069471</td>\n",
       "      <td>-0.409094</td>\n",
       "      <td>1.209383</td>\n",
       "      <td>-16.913551</td>\n",
       "      <td>-16.913551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.003564</td>\n",
       "      <td>1.601169</td>\n",
       "      <td>0.676719</td>\n",
       "      <td>1.484848</td>\n",
       "      <td>1.340410</td>\n",
       "      <td>-0.745118</td>\n",
       "      <td>-0.771643</td>\n",
       "      <td>1.473302</td>\n",
       "      <td>-0.689814</td>\n",
       "      <td>19.693391</td>\n",
       "      <td>19.693391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.260826</td>\n",
       "      <td>-0.173039</td>\n",
       "      <td>-1.437188</td>\n",
       "      <td>1.023740</td>\n",
       "      <td>-1.353533</td>\n",
       "      <td>-0.130372</td>\n",
       "      <td>0.651006</td>\n",
       "      <td>-1.704830</td>\n",
       "      <td>1.556819</td>\n",
       "      <td>-11.717325</td>\n",
       "      <td>-11.717324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.691403</td>\n",
       "      <td>1.387041</td>\n",
       "      <td>-0.710818</td>\n",
       "      <td>0.725680</td>\n",
       "      <td>1.220434</td>\n",
       "      <td>1.577667</td>\n",
       "      <td>1.119616</td>\n",
       "      <td>-1.504898</td>\n",
       "      <td>1.521724</td>\n",
       "      <td>25.483311</td>\n",
       "      <td>25.483310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.054124</td>\n",
       "      <td>-1.452695</td>\n",
       "      <td>1.669528</td>\n",
       "      <td>0.316676</td>\n",
       "      <td>-0.136198</td>\n",
       "      <td>-1.354090</td>\n",
       "      <td>1.262854</td>\n",
       "      <td>-1.366396</td>\n",
       "      <td>-1.273295</td>\n",
       "      <td>-12.910427</td>\n",
       "      <td>-12.910426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.666406</td>\n",
       "      <td>1.252924</td>\n",
       "      <td>-0.879921</td>\n",
       "      <td>-1.353745</td>\n",
       "      <td>-0.597139</td>\n",
       "      <td>1.195820</td>\n",
       "      <td>-0.679192</td>\n",
       "      <td>1.290141</td>\n",
       "      <td>-0.240465</td>\n",
       "      <td>2.288324</td>\n",
       "      <td>2.288324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.009733</td>\n",
       "      <td>-1.446147</td>\n",
       "      <td>-0.385777</td>\n",
       "      <td>-1.138016</td>\n",
       "      <td>-1.131140</td>\n",
       "      <td>-0.749925</td>\n",
       "      <td>-1.342200</td>\n",
       "      <td>-0.191087</td>\n",
       "      <td>-0.244946</td>\n",
       "      <td>-28.420167</td>\n",
       "      <td>-28.420166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.465297</td>\n",
       "      <td>0.668361</td>\n",
       "      <td>-1.104108</td>\n",
       "      <td>-0.448108</td>\n",
       "      <td>-1.012594</td>\n",
       "      <td>-1.534951</td>\n",
       "      <td>-0.925220</td>\n",
       "      <td>0.198157</td>\n",
       "      <td>1.473457</td>\n",
       "      <td>-18.436214</td>\n",
       "      <td>-18.436214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.377694</td>\n",
       "      <td>-0.235807</td>\n",
       "      <td>0.799166</td>\n",
       "      <td>0.762433</td>\n",
       "      <td>0.704368</td>\n",
       "      <td>-0.393625</td>\n",
       "      <td>0.754585</td>\n",
       "      <td>-0.692469</td>\n",
       "      <td>0.655534</td>\n",
       "      <td>8.469471</td>\n",
       "      <td>8.469470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.206496</td>\n",
       "      <td>0.780511</td>\n",
       "      <td>-0.554355</td>\n",
       "      <td>0.356267</td>\n",
       "      <td>-1.360204</td>\n",
       "      <td>-1.289249</td>\n",
       "      <td>-0.166742</td>\n",
       "      <td>0.717288</td>\n",
       "      <td>-1.034897</td>\n",
       "      <td>-17.239130</td>\n",
       "      <td>-17.239129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.675017</td>\n",
       "      <td>-0.744504</td>\n",
       "      <td>0.656593</td>\n",
       "      <td>-1.288110</td>\n",
       "      <td>-1.167937</td>\n",
       "      <td>0.148573</td>\n",
       "      <td>1.499930</td>\n",
       "      <td>1.634422</td>\n",
       "      <td>-0.220005</td>\n",
       "      <td>-7.996156</td>\n",
       "      <td>-7.996156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.249255</td>\n",
       "      <td>-0.917527</td>\n",
       "      <td>-0.613900</td>\n",
       "      <td>1.481831</td>\n",
       "      <td>0.500128</td>\n",
       "      <td>1.432433</td>\n",
       "      <td>1.490268</td>\n",
       "      <td>-0.278244</td>\n",
       "      <td>-1.652839</td>\n",
       "      <td>-1.005381</td>\n",
       "      <td>-1.005381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.268131</td>\n",
       "      <td>1.321735</td>\n",
       "      <td>-1.378878</td>\n",
       "      <td>-1.505212</td>\n",
       "      <td>-1.218942</td>\n",
       "      <td>1.428645</td>\n",
       "      <td>0.679257</td>\n",
       "      <td>1.572812</td>\n",
       "      <td>-1.272461</td>\n",
       "      <td>-3.060268</td>\n",
       "      <td>-3.060268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.574802</td>\n",
       "      <td>0.737586</td>\n",
       "      <td>-0.987255</td>\n",
       "      <td>1.229598</td>\n",
       "      <td>1.292035</td>\n",
       "      <td>-1.482112</td>\n",
       "      <td>1.058227</td>\n",
       "      <td>-0.602254</td>\n",
       "      <td>1.412911</td>\n",
       "      <td>3.943882</td>\n",
       "      <td>3.943882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.903783</td>\n",
       "      <td>0.470976</td>\n",
       "      <td>-0.297671</td>\n",
       "      <td>-1.595859</td>\n",
       "      <td>0.168141</td>\n",
       "      <td>-1.615096</td>\n",
       "      <td>0.764214</td>\n",
       "      <td>1.536560</td>\n",
       "      <td>-1.267286</td>\n",
       "      <td>-13.934625</td>\n",
       "      <td>-13.934624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.370815</td>\n",
       "      <td>-0.385737</td>\n",
       "      <td>0.784692</td>\n",
       "      <td>1.413219</td>\n",
       "      <td>0.728598</td>\n",
       "      <td>-0.256319</td>\n",
       "      <td>-0.111970</td>\n",
       "      <td>-0.633002</td>\n",
       "      <td>0.531441</td>\n",
       "      <td>9.272996</td>\n",
       "      <td>9.272996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.286458</td>\n",
       "      <td>-0.578795</td>\n",
       "      <td>1.791680</td>\n",
       "      <td>1.221010</td>\n",
       "      <td>1.572366</td>\n",
       "      <td>0.243872</td>\n",
       "      <td>1.351531</td>\n",
       "      <td>1.470753</td>\n",
       "      <td>1.040448</td>\n",
       "      <td>30.182867</td>\n",
       "      <td>30.182866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.807184</td>\n",
       "      <td>-0.808333</td>\n",
       "      <td>1.475963</td>\n",
       "      <td>-1.368672</td>\n",
       "      <td>1.385543</td>\n",
       "      <td>0.713994</td>\n",
       "      <td>0.273988</td>\n",
       "      <td>-0.123665</td>\n",
       "      <td>1.209105</td>\n",
       "      <td>16.550112</td>\n",
       "      <td>16.550111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.201713</td>\n",
       "      <td>-0.711380</td>\n",
       "      <td>-1.036933</td>\n",
       "      <td>-0.666560</td>\n",
       "      <td>-1.317363</td>\n",
       "      <td>0.562426</td>\n",
       "      <td>-1.104238</td>\n",
       "      <td>0.151767</td>\n",
       "      <td>0.148525</td>\n",
       "      <td>-16.786356</td>\n",
       "      <td>-16.786356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.894562</td>\n",
       "      <td>-0.311911</td>\n",
       "      <td>-0.574409</td>\n",
       "      <td>-0.927094</td>\n",
       "      <td>1.307377</td>\n",
       "      <td>0.923455</td>\n",
       "      <td>0.485042</td>\n",
       "      <td>-0.460393</td>\n",
       "      <td>0.390451</td>\n",
       "      <td>7.242644</td>\n",
       "      <td>7.242644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.474020</td>\n",
       "      <td>0.130047</td>\n",
       "      <td>1.270395</td>\n",
       "      <td>-1.628624</td>\n",
       "      <td>-1.157094</td>\n",
       "      <td>-0.323110</td>\n",
       "      <td>0.703023</td>\n",
       "      <td>0.832643</td>\n",
       "      <td>-1.690845</td>\n",
       "      <td>-6.707091</td>\n",
       "      <td>-6.707091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.365633</td>\n",
       "      <td>0.207009</td>\n",
       "      <td>-0.352072</td>\n",
       "      <td>0.808244</td>\n",
       "      <td>-1.109727</td>\n",
       "      <td>-1.243610</td>\n",
       "      <td>1.379550</td>\n",
       "      <td>1.383166</td>\n",
       "      <td>-0.764365</td>\n",
       "      <td>-12.694476</td>\n",
       "      <td>-12.694475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.280746</td>\n",
       "      <td>1.288927</td>\n",
       "      <td>-0.794331</td>\n",
       "      <td>-1.150486</td>\n",
       "      <td>-0.899902</td>\n",
       "      <td>0.230090</td>\n",
       "      <td>0.766790</td>\n",
       "      <td>-1.078344</td>\n",
       "      <td>-0.832184</td>\n",
       "      <td>-6.324229</td>\n",
       "      <td>-6.324228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.203091</td>\n",
       "      <td>-0.322998</td>\n",
       "      <td>-1.172566</td>\n",
       "      <td>1.199950</td>\n",
       "      <td>-0.242681</td>\n",
       "      <td>1.376896</td>\n",
       "      <td>-0.763822</td>\n",
       "      <td>-1.669100</td>\n",
       "      <td>0.846853</td>\n",
       "      <td>0.757974</td>\n",
       "      <td>0.757974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.077467</td>\n",
       "      <td>0.177870</td>\n",
       "      <td>0.975567</td>\n",
       "      <td>1.547870</td>\n",
       "      <td>0.486238</td>\n",
       "      <td>0.667674</td>\n",
       "      <td>-0.270191</td>\n",
       "      <td>-0.545513</td>\n",
       "      <td>-1.415189</td>\n",
       "      <td>12.777665</td>\n",
       "      <td>12.777666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>-1.358744</td>\n",
       "      <td>-1.261230</td>\n",
       "      <td>-0.479547</td>\n",
       "      <td>-0.559569</td>\n",
       "      <td>1.096572</td>\n",
       "      <td>0.567517</td>\n",
       "      <td>1.061808</td>\n",
       "      <td>0.471975</td>\n",
       "      <td>-1.394917</td>\n",
       "      <td>-9.438826</td>\n",
       "      <td>-9.438826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1.682297</td>\n",
       "      <td>-0.026850</td>\n",
       "      <td>-0.100488</td>\n",
       "      <td>0.927531</td>\n",
       "      <td>-1.023564</td>\n",
       "      <td>1.019254</td>\n",
       "      <td>0.881029</td>\n",
       "      <td>0.323192</td>\n",
       "      <td>-0.617103</td>\n",
       "      <td>6.358629</td>\n",
       "      <td>6.358629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1.077443</td>\n",
       "      <td>-1.325434</td>\n",
       "      <td>1.583091</td>\n",
       "      <td>0.060382</td>\n",
       "      <td>0.253489</td>\n",
       "      <td>0.225815</td>\n",
       "      <td>1.145960</td>\n",
       "      <td>0.206747</td>\n",
       "      <td>0.348076</td>\n",
       "      <td>11.264976</td>\n",
       "      <td>11.264976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.320053</td>\n",
       "      <td>-1.280072</td>\n",
       "      <td>-0.329306</td>\n",
       "      <td>-0.547173</td>\n",
       "      <td>0.582897</td>\n",
       "      <td>-1.051472</td>\n",
       "      <td>-1.372399</td>\n",
       "      <td>-1.215559</td>\n",
       "      <td>-1.343193</td>\n",
       "      <td>-24.609097</td>\n",
       "      <td>-24.609096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>-0.794759</td>\n",
       "      <td>1.292839</td>\n",
       "      <td>0.699215</td>\n",
       "      <td>0.756494</td>\n",
       "      <td>0.744749</td>\n",
       "      <td>1.417172</td>\n",
       "      <td>-1.784227</td>\n",
       "      <td>0.678001</td>\n",
       "      <td>-1.703016</td>\n",
       "      <td>19.735162</td>\n",
       "      <td>19.735162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>-0.170850</td>\n",
       "      <td>-1.416001</td>\n",
       "      <td>-1.474817</td>\n",
       "      <td>1.010615</td>\n",
       "      <td>1.346831</td>\n",
       "      <td>-0.891382</td>\n",
       "      <td>-0.282473</td>\n",
       "      <td>0.925488</td>\n",
       "      <td>-0.258033</td>\n",
       "      <td>-15.098730</td>\n",
       "      <td>-15.098730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1.605818</td>\n",
       "      <td>-1.612791</td>\n",
       "      <td>-0.341177</td>\n",
       "      <td>-0.095874</td>\n",
       "      <td>0.569930</td>\n",
       "      <td>-0.535698</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>0.180778</td>\n",
       "      <td>-1.430995</td>\n",
       "      <td>-15.402208</td>\n",
       "      <td>-15.402207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1.119874</td>\n",
       "      <td>-0.858375</td>\n",
       "      <td>-0.400813</td>\n",
       "      <td>0.434959</td>\n",
       "      <td>1.444287</td>\n",
       "      <td>-0.843131</td>\n",
       "      <td>-0.166217</td>\n",
       "      <td>1.584794</td>\n",
       "      <td>1.575020</td>\n",
       "      <td>6.211651</td>\n",
       "      <td>6.211650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>-1.219898</td>\n",
       "      <td>0.354955</td>\n",
       "      <td>-0.495114</td>\n",
       "      <td>-1.281919</td>\n",
       "      <td>0.117205</td>\n",
       "      <td>-0.633999</td>\n",
       "      <td>0.442413</td>\n",
       "      <td>-0.601785</td>\n",
       "      <td>0.205218</td>\n",
       "      <td>-11.413871</td>\n",
       "      <td>-11.413871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>-1.551333</td>\n",
       "      <td>-1.672694</td>\n",
       "      <td>-0.350680</td>\n",
       "      <td>-1.466739</td>\n",
       "      <td>-1.182608</td>\n",
       "      <td>0.056949</td>\n",
       "      <td>0.274751</td>\n",
       "      <td>1.058400</td>\n",
       "      <td>-1.416634</td>\n",
       "      <td>-28.783186</td>\n",
       "      <td>-28.783186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>-1.179041</td>\n",
       "      <td>-0.707760</td>\n",
       "      <td>1.255114</td>\n",
       "      <td>0.567860</td>\n",
       "      <td>0.889624</td>\n",
       "      <td>1.459596</td>\n",
       "      <td>-0.542862</td>\n",
       "      <td>-1.534913</td>\n",
       "      <td>1.311404</td>\n",
       "      <td>20.182949</td>\n",
       "      <td>20.182948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>-1.310935</td>\n",
       "      <td>0.099731</td>\n",
       "      <td>-0.392134</td>\n",
       "      <td>0.581845</td>\n",
       "      <td>-0.630315</td>\n",
       "      <td>-0.512159</td>\n",
       "      <td>-1.311555</td>\n",
       "      <td>1.577832</td>\n",
       "      <td>1.347018</td>\n",
       "      <td>-2.527398</td>\n",
       "      <td>-2.527398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.527952</td>\n",
       "      <td>-0.112899</td>\n",
       "      <td>0.928374</td>\n",
       "      <td>-1.309474</td>\n",
       "      <td>0.492559</td>\n",
       "      <td>1.286378</td>\n",
       "      <td>1.015379</td>\n",
       "      <td>0.911701</td>\n",
       "      <td>1.434507</td>\n",
       "      <td>22.992952</td>\n",
       "      <td>22.992951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.271889</td>\n",
       "      <td>-0.133770</td>\n",
       "      <td>-0.421640</td>\n",
       "      <td>0.473150</td>\n",
       "      <td>1.231743</td>\n",
       "      <td>-1.535941</td>\n",
       "      <td>-0.056989</td>\n",
       "      <td>1.397812</td>\n",
       "      <td>0.676987</td>\n",
       "      <td>-0.937162</td>\n",
       "      <td>-0.937162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>-0.558915</td>\n",
       "      <td>-1.246648</td>\n",
       "      <td>-1.575125</td>\n",
       "      <td>-0.447675</td>\n",
       "      <td>-0.159731</td>\n",
       "      <td>-1.543643</td>\n",
       "      <td>1.090579</td>\n",
       "      <td>-1.297650</td>\n",
       "      <td>-1.330159</td>\n",
       "      <td>-39.396136</td>\n",
       "      <td>-39.396135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>-0.225284</td>\n",
       "      <td>-1.602015</td>\n",
       "      <td>-1.390170</td>\n",
       "      <td>-1.180679</td>\n",
       "      <td>-0.735611</td>\n",
       "      <td>1.499119</td>\n",
       "      <td>-1.383011</td>\n",
       "      <td>1.016380</td>\n",
       "      <td>-0.585550</td>\n",
       "      <td>-18.212559</td>\n",
       "      <td>-18.212558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>-1.015290</td>\n",
       "      <td>1.392502</td>\n",
       "      <td>-0.077298</td>\n",
       "      <td>1.191526</td>\n",
       "      <td>1.436008</td>\n",
       "      <td>0.265548</td>\n",
       "      <td>0.362297</td>\n",
       "      <td>1.344546</td>\n",
       "      <td>-0.296115</td>\n",
       "      <td>20.263729</td>\n",
       "      <td>20.263728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>-0.967199</td>\n",
       "      <td>-0.094655</td>\n",
       "      <td>1.435992</td>\n",
       "      <td>-1.125157</td>\n",
       "      <td>-0.669034</td>\n",
       "      <td>-0.616910</td>\n",
       "      <td>-0.507009</td>\n",
       "      <td>1.064935</td>\n",
       "      <td>1.385597</td>\n",
       "      <td>3.675849</td>\n",
       "      <td>3.675848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.984809</td>\n",
       "      <td>-0.432724</td>\n",
       "      <td>-0.856172</td>\n",
       "      <td>0.540196</td>\n",
       "      <td>0.925937</td>\n",
       "      <td>-1.253310</td>\n",
       "      <td>-1.205870</td>\n",
       "      <td>-0.819572</td>\n",
       "      <td>0.305050</td>\n",
       "      <td>-10.767648</td>\n",
       "      <td>-10.767648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.413509</td>\n",
       "      <td>1.058919</td>\n",
       "      <td>-0.922843</td>\n",
       "      <td>-0.509983</td>\n",
       "      <td>1.599284</td>\n",
       "      <td>-0.405366</td>\n",
       "      <td>-0.189286</td>\n",
       "      <td>-1.149717</td>\n",
       "      <td>-1.497353</td>\n",
       "      <td>-2.128586</td>\n",
       "      <td>-2.128586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>-1.679555</td>\n",
       "      <td>-0.694407</td>\n",
       "      <td>1.406804</td>\n",
       "      <td>-1.424938</td>\n",
       "      <td>-1.406629</td>\n",
       "      <td>1.187624</td>\n",
       "      <td>-1.147714</td>\n",
       "      <td>-1.007959</td>\n",
       "      <td>1.117841</td>\n",
       "      <td>0.635216</td>\n",
       "      <td>0.635216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.507115</td>\n",
       "      <td>0.768514</td>\n",
       "      <td>-1.467289</td>\n",
       "      <td>0.849162</td>\n",
       "      <td>-1.296199</td>\n",
       "      <td>-0.565294</td>\n",
       "      <td>-0.016895</td>\n",
       "      <td>-1.076433</td>\n",
       "      <td>1.141586</td>\n",
       "      <td>-10.139689</td>\n",
       "      <td>-10.139689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1.596938</td>\n",
       "      <td>1.190550</td>\n",
       "      <td>1.258782</td>\n",
       "      <td>-1.609760</td>\n",
       "      <td>-1.396872</td>\n",
       "      <td>-1.279890</td>\n",
       "      <td>1.158624</td>\n",
       "      <td>0.183398</td>\n",
       "      <td>1.528090</td>\n",
       "      <td>6.013564</td>\n",
       "      <td>6.013563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.685067</td>\n",
       "      <td>1.467706</td>\n",
       "      <td>-1.041145</td>\n",
       "      <td>1.256522</td>\n",
       "      <td>-0.186027</td>\n",
       "      <td>-1.432319</td>\n",
       "      <td>-0.296070</td>\n",
       "      <td>-0.967086</td>\n",
       "      <td>1.132984</td>\n",
       "      <td>-1.996754</td>\n",
       "      <td>-1.996753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>-0.716093</td>\n",
       "      <td>-0.297883</td>\n",
       "      <td>-0.065816</td>\n",
       "      <td>-0.333801</td>\n",
       "      <td>0.459976</td>\n",
       "      <td>-1.461304</td>\n",
       "      <td>-1.769459</td>\n",
       "      <td>-1.594027</td>\n",
       "      <td>-0.347879</td>\n",
       "      <td>-18.959873</td>\n",
       "      <td>-18.959872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.586481</td>\n",
       "      <td>0.410997</td>\n",
       "      <td>-0.816413</td>\n",
       "      <td>0.671319</td>\n",
       "      <td>-1.476762</td>\n",
       "      <td>-1.076221</td>\n",
       "      <td>-0.615050</td>\n",
       "      <td>0.999238</td>\n",
       "      <td>0.090364</td>\n",
       "      <td>-13.234462</td>\n",
       "      <td>-13.234462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-1.277597</td>\n",
       "      <td>-0.801247</td>\n",
       "      <td>0.704444</td>\n",
       "      <td>-0.456304</td>\n",
       "      <td>1.361129</td>\n",
       "      <td>-1.347793</td>\n",
       "      <td>-1.564253</td>\n",
       "      <td>-0.274022</td>\n",
       "      <td>-0.141847</td>\n",
       "      <td>-9.109070</td>\n",
       "      <td>-9.109070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-1.021648</td>\n",
       "      <td>0.743806</td>\n",
       "      <td>1.184939</td>\n",
       "      <td>1.687454</td>\n",
       "      <td>0.334471</td>\n",
       "      <td>1.277799</td>\n",
       "      <td>-1.451201</td>\n",
       "      <td>0.367901</td>\n",
       "      <td>0.715431</td>\n",
       "      <td>28.539751</td>\n",
       "      <td>28.539751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.856707</td>\n",
       "      <td>-1.114305</td>\n",
       "      <td>1.575026</td>\n",
       "      <td>-0.557563</td>\n",
       "      <td>0.969788</td>\n",
       "      <td>0.848473</td>\n",
       "      <td>0.669069</td>\n",
       "      <td>0.981742</td>\n",
       "      <td>-0.798167</td>\n",
       "      <td>14.608217</td>\n",
       "      <td>14.608216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-1.292252</td>\n",
       "      <td>0.908604</td>\n",
       "      <td>-0.079072</td>\n",
       "      <td>0.935524</td>\n",
       "      <td>-0.583653</td>\n",
       "      <td>0.072284</td>\n",
       "      <td>1.410656</td>\n",
       "      <td>0.831134</td>\n",
       "      <td>-0.989897</td>\n",
       "      <td>1.821248</td>\n",
       "      <td>1.821248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0   0.463340  0.605805  0.022953 -1.143801  0.235715  0.644254  1.276217   \n",
       "1   1.640901 -0.544786 -1.709071  0.282615 -0.169942 -0.611693 -0.131537   \n",
       "2   0.970912  0.187193  0.221589 -1.385247 -0.244116  0.242864  1.323801   \n",
       "3   0.102926 -0.695344 -0.291127 -0.486043  0.042624  0.714035  1.176323   \n",
       "4  -0.091495 -0.200845  1.579307 -1.185153 -1.450891  1.515346  0.734429   \n",
       "5  -1.617708 -1.643191 -1.058965 -0.515723  0.322674 -0.080178  0.069471   \n",
       "6  -0.003564  1.601169  0.676719  1.484848  1.340410 -0.745118 -0.771643   \n",
       "7   0.260826 -0.173039 -1.437188  1.023740 -1.353533 -0.130372  0.651006   \n",
       "8  -0.691403  1.387041 -0.710818  0.725680  1.220434  1.577667  1.119616   \n",
       "9  -0.054124 -1.452695  1.669528  0.316676 -0.136198 -1.354090  1.262854   \n",
       "10 -0.666406  1.252924 -0.879921 -1.353745 -0.597139  1.195820 -0.679192   \n",
       "11 -0.009733 -1.446147 -0.385777 -1.138016 -1.131140 -0.749925 -1.342200   \n",
       "12 -1.465297  0.668361 -1.104108 -0.448108 -1.012594 -1.534951 -0.925220   \n",
       "13 -0.377694 -0.235807  0.799166  0.762433  0.704368 -0.393625  0.754585   \n",
       "14 -0.206496  0.780511 -0.554355  0.356267 -1.360204 -1.289249 -0.166742   \n",
       "15 -1.675017 -0.744504  0.656593 -1.288110 -1.167937  0.148573  1.499930   \n",
       "16 -1.249255 -0.917527 -0.613900  1.481831  0.500128  1.432433  1.490268   \n",
       "17  0.268131  1.321735 -1.378878 -1.505212 -1.218942  1.428645  0.679257   \n",
       "18  0.574802  0.737586 -0.987255  1.229598  1.292035 -1.482112  1.058227   \n",
       "19  0.903783  0.470976 -0.297671 -1.595859  0.168141 -1.615096  0.764214   \n",
       "20 -0.370815 -0.385737  0.784692  1.413219  0.728598 -0.256319 -0.111970   \n",
       "21 -0.286458 -0.578795  1.791680  1.221010  1.572366  0.243872  1.351531   \n",
       "22 -0.807184 -0.808333  1.475963 -1.368672  1.385543  0.713994  0.273988   \n",
       "23 -0.201713 -0.711380 -1.036933 -0.666560 -1.317363  0.562426 -1.104238   \n",
       "24  0.894562 -0.311911 -0.574409 -0.927094  1.307377  0.923455  0.485042   \n",
       "25  0.474020  0.130047  1.270395 -1.628624 -1.157094 -0.323110  0.703023   \n",
       "26 -0.365633  0.207009 -0.352072  0.808244 -1.109727 -1.243610  1.379550   \n",
       "27  1.280746  1.288927 -0.794331 -1.150486 -0.899902  0.230090  0.766790   \n",
       "28 -0.203091 -0.322998 -1.172566  1.199950 -0.242681  1.376896 -0.763822   \n",
       "29  0.077467  0.177870  0.975567  1.547870  0.486238  0.667674 -0.270191   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "70 -1.358744 -1.261230 -0.479547 -0.559569  1.096572  0.567517  1.061808   \n",
       "71  1.682297 -0.026850 -0.100488  0.927531 -1.023564  1.019254  0.881029   \n",
       "72  1.077443 -1.325434  1.583091  0.060382  0.253489  0.225815  1.145960   \n",
       "73  0.320053 -1.280072 -0.329306 -0.547173  0.582897 -1.051472 -1.372399   \n",
       "74 -0.794759  1.292839  0.699215  0.756494  0.744749  1.417172 -1.784227   \n",
       "75 -0.170850 -1.416001 -1.474817  1.010615  1.346831 -0.891382 -0.282473   \n",
       "76  1.605818 -1.612791 -0.341177 -0.095874  0.569930 -0.535698  0.001425   \n",
       "77  1.119874 -0.858375 -0.400813  0.434959  1.444287 -0.843131 -0.166217   \n",
       "78 -1.219898  0.354955 -0.495114 -1.281919  0.117205 -0.633999  0.442413   \n",
       "79 -1.551333 -1.672694 -0.350680 -1.466739 -1.182608  0.056949  0.274751   \n",
       "80 -1.179041 -0.707760  1.255114  0.567860  0.889624  1.459596 -0.542862   \n",
       "81 -1.310935  0.099731 -0.392134  0.581845 -0.630315 -0.512159 -1.311555   \n",
       "82  0.527952 -0.112899  0.928374 -1.309474  0.492559  1.286378  1.015379   \n",
       "83  0.271889 -0.133770 -0.421640  0.473150  1.231743 -1.535941 -0.056989   \n",
       "84 -0.558915 -1.246648 -1.575125 -0.447675 -0.159731 -1.543643  1.090579   \n",
       "85 -0.225284 -1.602015 -1.390170 -1.180679 -0.735611  1.499119 -1.383011   \n",
       "86 -1.015290  1.392502 -0.077298  1.191526  1.436008  0.265548  0.362297   \n",
       "87 -0.967199 -0.094655  1.435992 -1.125157 -0.669034 -0.616910 -0.507009   \n",
       "88  0.984809 -0.432724 -0.856172  0.540196  0.925937 -1.253310 -1.205870   \n",
       "89  1.413509  1.058919 -0.922843 -0.509983  1.599284 -0.405366 -0.189286   \n",
       "90 -1.679555 -0.694407  1.406804 -1.424938 -1.406629  1.187624 -1.147714   \n",
       "91  0.507115  0.768514 -1.467289  0.849162 -1.296199 -0.565294 -0.016895   \n",
       "92  1.596938  1.190550  1.258782 -1.609760 -1.396872 -1.279890  1.158624   \n",
       "93  0.685067  1.467706 -1.041145  1.256522 -0.186027 -1.432319 -0.296070   \n",
       "94 -0.716093 -0.297883 -0.065816 -0.333801  0.459976 -1.461304 -1.769459   \n",
       "95  0.586481  0.410997 -0.816413  0.671319 -1.476762 -1.076221 -0.615050   \n",
       "96 -1.277597 -0.801247  0.704444 -0.456304  1.361129 -1.347793 -1.564253   \n",
       "97 -1.021648  0.743806  1.184939  1.687454  0.334471  1.277799 -1.451201   \n",
       "98  0.856707 -1.114305  1.575026 -0.557563  0.969788  0.848473  0.669069   \n",
       "99 -1.292252  0.908604 -0.079072  0.935524 -0.583653  0.072284  1.410656   \n",
       "\n",
       "           7         8    y_truth     y_pred  \n",
       "0  -1.254529 -1.377642   0.235711   0.235711  \n",
       "1  -1.024020 -0.953006 -22.225730 -22.225729  \n",
       "2   1.097155  0.517549   6.528140   6.528139  \n",
       "3  -0.320013  0.941989   2.053385   2.053385  \n",
       "4  -0.988471 -1.072902   3.930470   3.930470  \n",
       "5  -0.409094  1.209383 -16.913551 -16.913551  \n",
       "6   1.473302 -0.689814  19.693391  19.693391  \n",
       "7  -1.704830  1.556819 -11.717325 -11.717324  \n",
       "8  -1.504898  1.521724  25.483311  25.483310  \n",
       "9  -1.366396 -1.273295 -12.910427 -12.910426  \n",
       "10  1.290141 -0.240465   2.288324   2.288324  \n",
       "11 -0.191087 -0.244946 -28.420167 -28.420166  \n",
       "12  0.198157  1.473457 -18.436214 -18.436214  \n",
       "13 -0.692469  0.655534   8.469471   8.469470  \n",
       "14  0.717288 -1.034897 -17.239130 -17.239129  \n",
       "15  1.634422 -0.220005  -7.996156  -7.996156  \n",
       "16 -0.278244 -1.652839  -1.005381  -1.005381  \n",
       "17  1.572812 -1.272461  -3.060268  -3.060268  \n",
       "18 -0.602254  1.412911   3.943882   3.943882  \n",
       "19  1.536560 -1.267286 -13.934625 -13.934624  \n",
       "20 -0.633002  0.531441   9.272996   9.272996  \n",
       "21  1.470753  1.040448  30.182867  30.182866  \n",
       "22 -0.123665  1.209105  16.550112  16.550111  \n",
       "23  0.151767  0.148525 -16.786356 -16.786356  \n",
       "24 -0.460393  0.390451   7.242644   7.242644  \n",
       "25  0.832643 -1.690845  -6.707091  -6.707091  \n",
       "26  1.383166 -0.764365 -12.694476 -12.694475  \n",
       "27 -1.078344 -0.832184  -6.324229  -6.324228  \n",
       "28 -1.669100  0.846853   0.757974   0.757974  \n",
       "29 -0.545513 -1.415189  12.777665  12.777666  \n",
       "..       ...       ...        ...        ...  \n",
       "70  0.471975 -1.394917  -9.438826  -9.438826  \n",
       "71  0.323192 -0.617103   6.358629   6.358629  \n",
       "72  0.206747  0.348076  11.264976  11.264976  \n",
       "73 -1.215559 -1.343193 -24.609097 -24.609096  \n",
       "74  0.678001 -1.703016  19.735162  19.735162  \n",
       "75  0.925488 -0.258033 -15.098730 -15.098730  \n",
       "76  0.180778 -1.430995 -15.402208 -15.402207  \n",
       "77  1.584794  1.575020   6.211651   6.211650  \n",
       "78 -0.601785  0.205218 -11.413871 -11.413871  \n",
       "79  1.058400 -1.416634 -28.783186 -28.783186  \n",
       "80 -1.534913  1.311404  20.182949  20.182948  \n",
       "81  1.577832  1.347018  -2.527398  -2.527398  \n",
       "82  0.911701  1.434507  22.992952  22.992951  \n",
       "83  1.397812  0.676987  -0.937162  -0.937162  \n",
       "84 -1.297650 -1.330159 -39.396136 -39.396135  \n",
       "85  1.016380 -0.585550 -18.212559 -18.212558  \n",
       "86  1.344546 -0.296115  20.263729  20.263728  \n",
       "87  1.064935  1.385597   3.675849   3.675848  \n",
       "88 -0.819572  0.305050 -10.767648 -10.767648  \n",
       "89 -1.149717 -1.497353  -2.128586  -2.128586  \n",
       "90 -1.007959  1.117841   0.635216   0.635216  \n",
       "91 -1.076433  1.141586 -10.139689 -10.139689  \n",
       "92  0.183398  1.528090   6.013564   6.013563  \n",
       "93 -0.967086  1.132984  -1.996754  -1.996753  \n",
       "94 -1.594027 -0.347879 -18.959873 -18.959872  \n",
       "95  0.999238  0.090364 -13.234462 -13.234462  \n",
       "96 -0.274022 -0.141847  -9.109070  -9.109070  \n",
       "97  0.367901  0.715431  28.539751  28.539751  \n",
       "98  0.981742 -0.798167  14.608217  14.608216  \n",
       "99  0.831134 -0.989897   1.821248   1.821248  \n",
       "\n",
       "[100 rows x 11 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_star = cross_validator.lambda_star\n",
    "print('with regularization: ', lambda_star)\n",
    "final_model = ManyTargetsModel(custom_loss_function, regularization=lambda_star)\n",
    "final_model.fit(X, Y)\n",
    "final_model.estimated_weights\n",
    "y_pred = final_model.predict(X)\n",
    "train_data = pd.DataFrame(X)\n",
    "train_data['y_truth'] = Y\n",
    "train_data['y_pred'] = y_pred\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(custom_loss_function(np.matmul(X,final_model.estimated_weights), Y),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.random.random((10,9))\n",
    "test_result = final_model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.376174</td>\n",
       "      <td>0.098168</td>\n",
       "      <td>0.749752</td>\n",
       "      <td>0.773878</td>\n",
       "      <td>0.931769</td>\n",
       "      <td>0.436162</td>\n",
       "      <td>0.867291</td>\n",
       "      <td>0.341407</td>\n",
       "      <td>0.799800</td>\n",
       "      <td>21.372531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.123409</td>\n",
       "      <td>0.236409</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.325244</td>\n",
       "      <td>0.330626</td>\n",
       "      <td>0.585656</td>\n",
       "      <td>0.994374</td>\n",
       "      <td>0.614705</td>\n",
       "      <td>0.881945</td>\n",
       "      <td>14.200184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.623453</td>\n",
       "      <td>0.997036</td>\n",
       "      <td>0.714577</td>\n",
       "      <td>0.016341</td>\n",
       "      <td>0.789110</td>\n",
       "      <td>0.004986</td>\n",
       "      <td>0.528634</td>\n",
       "      <td>0.660836</td>\n",
       "      <td>0.923247</td>\n",
       "      <td>21.803928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.591259</td>\n",
       "      <td>0.221597</td>\n",
       "      <td>0.386482</td>\n",
       "      <td>0.116665</td>\n",
       "      <td>0.031293</td>\n",
       "      <td>0.629575</td>\n",
       "      <td>0.829792</td>\n",
       "      <td>0.663270</td>\n",
       "      <td>0.074777</td>\n",
       "      <td>12.586394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.899467</td>\n",
       "      <td>0.464640</td>\n",
       "      <td>0.694096</td>\n",
       "      <td>0.527885</td>\n",
       "      <td>0.327508</td>\n",
       "      <td>0.384824</td>\n",
       "      <td>0.628720</td>\n",
       "      <td>0.918305</td>\n",
       "      <td>0.382895</td>\n",
       "      <td>19.357311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.855617</td>\n",
       "      <td>0.564766</td>\n",
       "      <td>0.865399</td>\n",
       "      <td>0.239339</td>\n",
       "      <td>0.591516</td>\n",
       "      <td>0.268935</td>\n",
       "      <td>0.268407</td>\n",
       "      <td>0.119821</td>\n",
       "      <td>0.129832</td>\n",
       "      <td>17.743136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.787594</td>\n",
       "      <td>0.144437</td>\n",
       "      <td>0.402183</td>\n",
       "      <td>0.465212</td>\n",
       "      <td>0.112250</td>\n",
       "      <td>0.669448</td>\n",
       "      <td>0.364317</td>\n",
       "      <td>0.725385</td>\n",
       "      <td>0.076757</td>\n",
       "      <td>14.022224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.646415</td>\n",
       "      <td>0.073898</td>\n",
       "      <td>0.016171</td>\n",
       "      <td>0.446606</td>\n",
       "      <td>0.654247</td>\n",
       "      <td>0.874867</td>\n",
       "      <td>0.026137</td>\n",
       "      <td>0.803386</td>\n",
       "      <td>0.521216</td>\n",
       "      <td>16.302307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.254905</td>\n",
       "      <td>0.640424</td>\n",
       "      <td>0.522446</td>\n",
       "      <td>0.381834</td>\n",
       "      <td>0.699115</td>\n",
       "      <td>0.312262</td>\n",
       "      <td>0.408142</td>\n",
       "      <td>0.118649</td>\n",
       "      <td>0.604684</td>\n",
       "      <td>17.900562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.273383</td>\n",
       "      <td>0.887628</td>\n",
       "      <td>0.399006</td>\n",
       "      <td>0.141607</td>\n",
       "      <td>0.019918</td>\n",
       "      <td>0.215078</td>\n",
       "      <td>0.032770</td>\n",
       "      <td>0.120582</td>\n",
       "      <td>0.433281</td>\n",
       "      <td>12.702592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.376174  0.098168  0.749752  0.773878  0.931769  0.436162  0.867291   \n",
       "1  0.123409  0.236409  0.007843  0.325244  0.330626  0.585656  0.994374   \n",
       "2  0.623453  0.997036  0.714577  0.016341  0.789110  0.004986  0.528634   \n",
       "3  0.591259  0.221597  0.386482  0.116665  0.031293  0.629575  0.829792   \n",
       "4  0.899467  0.464640  0.694096  0.527885  0.327508  0.384824  0.628720   \n",
       "5  0.855617  0.564766  0.865399  0.239339  0.591516  0.268935  0.268407   \n",
       "6  0.787594  0.144437  0.402183  0.465212  0.112250  0.669448  0.364317   \n",
       "7  0.646415  0.073898  0.016171  0.446606  0.654247  0.874867  0.026137   \n",
       "8  0.254905  0.640424  0.522446  0.381834  0.699115  0.312262  0.408142   \n",
       "9  0.273383  0.887628  0.399006  0.141607  0.019918  0.215078  0.032770   \n",
       "\n",
       "          7         8     y_pred  \n",
       "0  0.341407  0.799800  21.372531  \n",
       "1  0.614705  0.881945  14.200184  \n",
       "2  0.660836  0.923247  21.803928  \n",
       "3  0.663270  0.074777  12.586394  \n",
       "4  0.918305  0.382895  19.357311  \n",
       "5  0.119821  0.129832  17.743136  \n",
       "6  0.725385  0.076757  14.022224  \n",
       "7  0.803386  0.521216  16.302307  \n",
       "8  0.118649  0.604684  17.900562  \n",
       "9  0.120582  0.433281  12.702592  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(test_data)\n",
    "data['y_pred'] = test_result\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599453"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import make_scorer\n",
    "def my_custom_loss_func(ground_truth, predictions):\n",
    "    diff = np.abs(ground_truth - predictions).max()\n",
    "    return np.log(1 + diff)\n",
    "\n",
    "# loss_func will negate the return value of my_custom_loss_func,\n",
    "#  which will be np.log(2), 0.693, given the values for ground_truth\n",
    "#  and predictions defined below.\n",
    "loss  = make_scorer(my_custom_loss_func, greater_is_better=False)\n",
    "score = make_scorer(my_custom_loss_func, greater_is_better=True)\n",
    "ground_truth = [[1, 1],[2,1]]\n",
    "predictions  = [0, 1]\n",
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier(strategy='most_frequent', random_state=0)\n",
    "clf = clf.fit(ground_truth, predictions)\n",
    "loss(clf,ground_truth, predictions) \n",
    "\n",
    "score(clf,ground_truth, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['malignant' 'benign']\n",
      "Class label =  0\n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "[1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01\n",
      " 1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02\n",
      " 6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01\n",
      " 1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01\n",
      " 4.601e-01 1.189e-01]\n",
      "[1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n",
      " 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n",
      " 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0\n",
      " 1 1 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0\n",
      " 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0\n",
      " 0 1 1]\n",
      "0.9414893617021277\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Organize our data\n",
    "label_names = data['target_names']\n",
    "labels = data['target']\n",
    "feature_names = data['feature_names']\n",
    "features = data['data']\n",
    "\n",
    "# Look at our data\n",
    "print(label_names)\n",
    "print('Class label = ', labels[0])\n",
    "print(feature_names)\n",
    "print(features[0])\n",
    "\n",
    "# Split our data\n",
    "train, test, train_labels, test_labels = train_test_split(features,\n",
    "                                                          labels,\n",
    "                                                          test_size=0.33,\n",
    "                                                          random_state=42)\n",
    "\n",
    "# Initialize our classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train our classifier\n",
    "model = gnb.fit(train, train_labels)\n",
    "\n",
    "# Make predictions\n",
    "preds = gnb.predict(test)\n",
    "print(preds)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(accuracy_score(test_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove zeros from set...\n",
      "Remove zeros from set...\n",
      "[[1, 2], [0, 0], [2, 6, 7, 0]]\n",
      "[2 4 7]\n",
      "Idx to delete  [0, 2]\n",
      "[array([1, 2]), array([], dtype=int64), array([2, 6, 7])]\n",
      "[2 4 7]\n",
      "[True, False, True]\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "y_true = [[1,2],[0,0],[2,6,7,0]]\n",
    "y_pred = np.array([2,4,7])\n",
    "\n",
    "\n",
    "#print(y_pred[y_pred != 0])\n",
    "y_true_excluding_zeros = [np.array(v)[np.array(v)!=0] for v in y_true]\n",
    "\n",
    "y_idx = []\n",
    "for idx, v in enumerate(y_true):\n",
    "    if v.__contains__(2):\n",
    "        print(\"Remove zeros from set...\")\n",
    "        y_idx.append(idx)\n",
    "\n",
    "print(y_true)\n",
    "print(y_pred)\n",
    "print('Idx to delete ', y_idx)\n",
    "y_true = y_true_excluding_zeros\n",
    "\n",
    "#y_true = np.delete(y_true,y_idx,axis=0)\n",
    "#y_pred = np.delete(y_pred,y_idx)\n",
    "print(y_true)\n",
    "print(y_pred)\n",
    "matched_index = [t.__contains__(p) for (t,p) in zip(y_true, y_pred)]\n",
    "print(matched_index)\n",
    "print(sum(matched_index)/len(matched_index))\n",
    "\n",
    "\n",
    "#matched_values = [reduce(np.intersect1d, (p, a)) for (p,a) in zip(y_true, y_pred)]\n",
    "#print(matched_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.scorer import make_scorer\n",
    "def multi_targets_scorer_function(y_true, y_pred):\n",
    "    y_true_excluding_zeros = [np.array(v)[np.array(v)!=0] for v in y_true]\n",
    "    matched_index = [t.__contains__(p) for (t,p) in zip(y_true_excluding_zeros, y_pred)]\n",
    "    return sum(matched_index)/len(y_true_excluding_zeros)\n",
    "\n",
    "multi_targets_scorer = make_scorer(multi_targets_scorer_function, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cores:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n",
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  return _inspect.getargspec(target)\n",
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:4900: ResourceWarning: unclosed file <_io.TextIOWrapper name='/Users/uqapp/.keras/keras.json' mode='r' encoding='UTF-8'>\n",
      "  _config = json.load(open(_config_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#!conda install -n mldds -c anaconda joblib\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "import multiprocessing\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "print(\"Cores: \", num_cores)\n",
    "\n",
    "import time\n",
    "import keras\n",
    "# import tensorflow as tf\n",
    "# config = tf.ConfigProto( device_count = {'GPU': 0 , 'CPU': num_cores} )\n",
    "# sess = tf.Session(config=config) \n",
    "# keras.backend.set_session(sess)\n",
    "\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from MyTotoResearchv4 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install autograd\n",
    "#!conda install -c omnia autograd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-69db1a1f6c87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_matched\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_loss_given_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'grad' is not defined"
     ]
    }
   ],
   "source": [
    "def wTx(w, x):\n",
    "    return np.dot(x, w)\n",
    "\n",
    "def sigmoid_range(z,bottom,top):\n",
    "#    return 1./(1+np.exp(-z))\n",
    "    return bottom + (top - bottom) / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_range_inverse(y, bottom,top):\n",
    "    return np.log((y - bottom) / (top - y))\n",
    "\n",
    "def custom_predictions(w, x):\n",
    "    predictions = sigmoid_range(wTx(w, x),1,49)\n",
    "    return predictions\n",
    "#     global i\n",
    "#     if ( i < 10 ):\n",
    "#         print(X)\n",
    "#     print(predictions)\n",
    "#     return predictions.clip(eps, 1-eps)\n",
    "\n",
    "def custom_loss(y, y_predicted):\n",
    "#    return -(y*np.log(y_predicted) - (1-y)*np.log(1-y_predicted)**2).mean()\n",
    "\n",
    "    return -(y*np.log(y_predicted) - (1-y)*np.log(1-y_predicted)**2).mean()\n",
    "\n",
    "i = 0\n",
    "def custom_loss_given_weights(w):\n",
    "#     global i\n",
    "#     if ( i < 10 ):\n",
    "#         i = i + 1\n",
    "#         print(X)\n",
    "    y_predicted = custom_predictions(w, X)\n",
    "    y_matched = y_predicted[np.abs(y_predicted-y).argmin()]\n",
    "    return custom_loss(y_matched, y_predicted)\n",
    "    \n",
    "gradient = grad(custom_loss_given_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from autograd import grad\n",
    "import autograd.numpy as np\n",
    "\n",
    "\n",
    "def getAllData(df):\n",
    "    drop_cols = ['T', 'D', 'N1','N2','N3','N4','N5','N6','N7','L','M','S','R','E','A','V' ,'J','U']\n",
    "    X = df.drop(drop_cols, axis=1)\n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wTx(w, x):\n",
    "    return np.dot(x, w)\n",
    "\n",
    "def sigmoid_range(z,bottom,top):\n",
    "#    return 1./(1+np.exp(-z))\n",
    "    return bottom + (top - bottom) / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_range_inverse(y, bottom,top):\n",
    "    return np.log((y - bottom) / (top - y))\n",
    "\n",
    "def custom_predictions(w, x):\n",
    "    predictions = sigmoid_range(wTx(w, x),1,49)\n",
    "    return predictions\n",
    "#     global i\n",
    "#     if ( i < 10 ):\n",
    "#         print(X)\n",
    "#     print(predictions)\n",
    "#     return predictions.clip(eps, 1-eps)\n",
    "\n",
    "def custom_loss(y, y_predicted):\n",
    "    return -(y*np.log(y_predicted) - (1-y)*np.log(1-y_predicted)**2).mean()\n",
    "\n",
    "i = 0\n",
    "def custom_loss_given_weights(w):\n",
    "#     global i\n",
    "#     if ( i < 10 ):\n",
    "#         i = i + 1\n",
    "#         print(X)\n",
    "    y_predicted = custom_predictions(w, X)\n",
    "    y_matched = y_predicted[np.abs(y_predicted-y).argmin()]\n",
    "    return custom_loss(y_matched, y_predicted)\n",
    "    \n",
    "gradient = grad(custom_loss_given_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [ 0.3213,  0.4856,  0.2995,  2.5044],\n",
    "    [ 0.3005,  0.4757,  0.2974,  2.4691],\n",
    "    [ 0.5638,  0.8005,  0.3381,  2.3102],\n",
    "    [ 0.5281,  0.6542,  0.3129,  2.1298],\n",
    "    [ 0.3221,  0.5126,  0.3085,  2.6147],\n",
    "    [ 0.3055,  0.4885,  0.289 ,  2.4957],\n",
    "    [ 0.3276,  0.5185,  0.3218,  2.6013],\n",
    "    [ 0.5313,  0.7028,  0.3266,  2.1543],\n",
    "    [ 0.4728,  0.6399,  0.3062,  2.0597],\n",
    "    [ 0.3221,  0.5126,  0.3085,  2.6147]\n",
    "])\n",
    "y = np.array([1., 1., 0., 0., 1., 1., 1., 1., 0., 0.])\n",
    "\n",
    "weights = np.zeros(X.shape[1])\n",
    "eps = 1e-15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [(y_hat, custom_loss(False, y_hat)) for y_hat in np.linspace(0, 1, 101)],\n",
    "    columns=['y_hat', 'loss']\n",
    ").plot(x='y_hat', title='y_hat vs. Loss for y=0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [(y_hat, custom_loss(True, y_hat)) for y_hat in np.linspace(0, 1, 101)],\n",
    "    columns=['y_hat', 'loss']\n",
    ").plot(x='y_hat', title='y_hat vs. Loss for y=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    if i % 100 == 0:\n",
    "        print('Iteration %-4d | Loss: %.4f' % (i, custom_loss_given_weights(weights)))\n",
    "    weights -= gradient(weights) * .05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_prediction(mrt, model, f, scaler=None, name='unnamed'):\n",
    "    def getAllData(df):\n",
    "        drop_cols = ['T', 'L','M','S','R','E','A','V' ,'J','U']\n",
    "        X = df.drop(drop_cols, axis=1)\n",
    "#        print(df.head())\n",
    "        use_cols = ['Ph','il','age','dist','adia','sundist','sunadia']\n",
    "        X = df[use_cols]\n",
    "        return X\n",
    "\n",
    "    test_data = mtr.get_test_data()\n",
    "    X = mtr.modified_dataset(getAllData(test_data)) #\n",
    "#    X = getAdjustedDataF(test_data,f)\n",
    "\n",
    "\n",
    "    if ( scaler == None ):\n",
    "        Z = X\n",
    "    else:\n",
    "        scaler.fit(X)\n",
    "        Z = scaler.transform(X)\n",
    "\n",
    "    predictions = model.predict(Z)\n",
    "\n",
    "    dfResult= pd.DataFrame(predictions, columns=['N1', 'N2', 'N3', 'N4', 'N5','N6', 'N7'])\n",
    "#    mtr.print_predictions(dfResult)\n",
    "\n",
    "    global df_predictions\n",
    "    global prev_r\n",
    "    r = mtr.getAccuracyCount(np.array(dfResult)) ;\n",
    "#    if ( r > prev_r ):\n",
    "#        df_predictions = []\n",
    "    df_predictions.append(dfResult)\n",
    "    g_all_pred.update({name : dfResult})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from keras.models import Input, Model\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, History\n",
    "import json as simplejson\n",
    "from keras import regularizers\n",
    "from sklearn import preprocessing\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, RandomForestClassifier, ExtraTreesRegressor, ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor, SGDClassifier, LogisticRegression, PassiveAggressiveClassifier, Perceptron, RidgeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression, Lasso, ElasticNet, Ridge, RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, SVR, LinearSVC\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "seed = 42\n",
    "\n",
    "mtr = MyTotoResearch(algo_no=1)\n",
    "lresult, df = mtr.load_totodata()\n",
    "\n",
    "df_predictions = []\n",
    "\n",
    "\n",
    "all_models = []\n",
    "\n",
    "#all_models.append(('SVCpoly01', SVC(kernel='poly', coef0=0.05, probability=True, degree=2, random_state=seed)))\n",
    "#all_models.append(('SVCrbf010', SVC(kernel='rbf', coef0=0.75, probability=True, degree=2, random_state=seed)))\n",
    "# all_models.append(('SVCrbf011', SVC(kernel='rbf', coef0=0.5, probability=True, degree=2, random_state=seed)))\n",
    "# all_models.append(('SVCrbf012', SVC(kernel='rbf', coef0=0.25, probability=True, degree=2, random_state=seed)))\n",
    "\n",
    "# all_models.append(('SVCrbf0103', SVC(kernel='rbf', coef0=0.75, probability=True, degree=3, random_state=seed)))\n",
    "# all_models.append(('SVCrbf0113', SVC(kernel='rbf', coef0=0.5, probability=True, degree=3, random_state=seed)))\n",
    "# all_models.append(('SVCrbf0123', SVC(kernel='rbf', coef0=0.25, probability=True, degree=3, random_state=seed)))\n",
    "\n",
    "\n",
    "#all_models.append(('SVCrbf020', SVC(kernel='sigmoid', coef0=0.75, probability=True, degree=2, random_state=seed)))\n",
    "# all_models.append(('SVCrbf021', SVC(kernel='sigmoid', coef0=0.5, probability=True, degree=2, random_state=seed)))\n",
    "# all_models.append(('SVCrbf022', SVC(kernel='sigmoid', coef0=0.25, probability=True, degree=2, random_state=seed)))\n",
    "\n",
    "# all_models.append(('SVCrbf0203', SVC(kernel='sigmoid', coef0=0.75, probability=True, degree=3, random_state=seed)))\n",
    "# all_models.append(('SVCrbf0213', SVC(kernel='sigmoid', coef0=0.5, probability=True, degree=3, random_state=seed)))\n",
    "# all_models.append(('SVCrbf0223', SVC(kernel='sigmoid', coef0=0.25, probability=True, degree=3, random_state=seed)))\n",
    "\n",
    "\n",
    "# all_models.append(('SVCrbf030', SVC(kernel='linear', coef0=0.75, probability=True, degree=2, random_state=seed)))\n",
    "# all_models.append(('SVCrbf031', SVC(kernel='linear', coef0=0.5, probability=True, degree=2, random_state=seed)))\n",
    "# all_models.append(('SVCrbf032', SVC(kernel='linear', coef0=0.25, probability=True, degree=2, random_state=seed)))\n",
    "\n",
    "# all_models.append(('SVCrbf0303', SVC(kernel='linear', coef0=0.75, probability=True, degree=3, random_state=seed)))\n",
    "# all_models.append(('SVCrbf0313', SVC(kernel='linear', coef0=0.5, probability=True, degree=3, random_state=seed)))\n",
    "# all_models.append(('SVCrbf0323', SVC(kernel='linear', coef0=0.25, probability=True, degree=3, random_state=seed)))\n",
    "\n",
    "\n",
    "\n",
    "# all_models.append(('LR', (LogisticRegression(random_state=seed))))\n",
    "\n",
    "#all_models.append(('KNNC', KNeighborsClassifier()))\n",
    "#all_models.append(('KNNR', KNeighborsRegressor()))\n",
    "#all_models.append(('RC', RidgeClassifier(random_state=seed)))\n",
    "# all_models.append(('LR', LogisticRegression(random_state=seed)))\n",
    "# all_models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "# all_models.append(('DTR', DecisionTreeRegressor()))\n",
    "# all_models.append(('ETR', ExtraTreesRegressor(n_estimators=5)))\n",
    "#all_models.append(('ETC', ExtraTreesClassifier(n_estimators=5)))\n",
    "# all_models.append(('EN', ElasticNet()))\n",
    "#all_models.append(('CART', DecisionTreeClassifier()))\n",
    "# all_models.append(('NB', GaussianNB()))\n",
    "# all_models.append(('Lasso', Lasso()))\n",
    "all_models.append(('GBR', GradientBoostingRegressor()))\n",
    "#all_models.append(('RFR5', RandomForestClassifier(n_estimators=5, n_jobs=5, random_state=seed)))\n",
    "# all_models.append(('RFR5', RandomForestClassifier(n_estimators=5, n_jobs=5, random_state=seed)))\n",
    "# all_models.append(('RFR3', RandomForestRegressor(n_estimators=3, n_jobs=5, random_state=seed)))\n",
    "# all_models.append(('SGDR', SGDRegressor(random_state=seed)))\n",
    "#all_models.append(('AdaB', AdaBoostClassifier(RandomForestClassifier(n_estimators=3))))\n",
    "#all_models.append(('MLPC', MLPClassifier(hidden_layer_sizes=(500,500,500), max_iter=2000, alpha=0.001, activation='tanh', learning_rate='adaptive', solver='sgd', verbose=0,  random_state=42,tol=0.000000001)))\n",
    "\n",
    "#92.45 accuracy\n",
    "#all_models.append(('MLPC', MLPClassifier(hidden_layer_sizes=(490,490,490,490,490,490,490), max_iter=500000, alpha=0.001, activation='relu', learning_rate='adaptive', solver='adam', verbose=10,  random_state=42,tol=0.000000001)))\n",
    "\n",
    "\n",
    "all_models.append(('MLPC', MLPClassifier(hidden_layer_sizes=(780,490,780,490,780,490,280), max_iter=500000, alpha=0.001, activation='relu', learning_rate='adaptive', solver='adam', verbose=10,  random_state=42,tol=0.000000001)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each model in turn\n",
    "from sklearn import model_selection\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "\n",
    "g_all_pred = {}\n",
    "\n",
    "X = mtr.modified_dataset(getAllData(df)) #\n",
    "f = 1.0 #365/27.58\n",
    "#    X = getAdjustedDataF(df,f)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X)\n",
    "Z = scaler.transform(X)\n",
    "\n",
    "for name, model in all_models:\n",
    "    \n",
    "    \n",
    "#    scaler = None\n",
    "#    Z = X\n",
    "\n",
    "#     kfold = model_selection.KFold(n_splits=3, random_state=seed)\n",
    "#     cv_results = model_selection.cross_val_score(model, Z, mtr.getTarget(3), cv=kfold, scoring=scoring)\n",
    "#     results.append(cv_results)\n",
    "#     names.append(name)\n",
    "#     msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "#     print(msg)\n",
    "    \n",
    "    oClassifier = MultiOutputClassifier(model, n_jobs=7)\n",
    "    oClassifier.fit(Z, mtr.getTargets()) \n",
    "    print(oClassifier)\n",
    "    s = oClassifier.score(Z, mtr.getTargets())\n",
    "    if(oClassifier.score(Z, mtr.getTargets()) == 1.0):\n",
    "        print( name, ' ', str(f), ' ', str(s))\n",
    "    store_prediction(mtr, oClassifier, f, scaler=scaler, name=name)\n",
    "    start = time.clock()\n",
    "    print(str(f), \" Time taken: \", (time.clock() - start),  \" \")\n",
    "\n",
    "# for n in range(len(df_predictions)):\n",
    "#     print( mtr.getAccuracyCount(np.array(df_predictions[n])))\n",
    "#     mtr.print_predictions(df_predictions[n])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# boxplot algorithm comparison\n",
    "# fig = plt.figure()\n",
    "# fig.suptitle('Algorithm Comparison')\n",
    "# ax = fig.add_subplot(111)\n",
    "# plt.boxplot(results)\n",
    "# ax.set_xticklabels(names)\n",
    "# plt.show()\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_prediction(arr, initial_pred=[]):\n",
    "    global s\n",
    "    if ( isinstance(arr, list) ):\n",
    "        for a in arr:\n",
    "            combine_prediction(a, initial_pred)\n",
    "        return \n",
    "    if ( len(s) > 1 ):\n",
    "        s += '_'\n",
    "    s += arr\n",
    "    initial_pred.append(g_all_pred[arr])\n",
    "    return \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from itertools import combinations\n",
    "import operator \n",
    "from itertools import islice\n",
    "\n",
    "name_ = []\n",
    "\n",
    "lst = [name for name, model in all_models]\n",
    "iBestIndex = -1\n",
    "iBestN = []\n",
    "#print(\"List \", lst)\n",
    "top_n = 12\n",
    "\n",
    "\n",
    "dict_accuracy = {}\n",
    "for z in range(5, 0,-1):\n",
    "    a = [list(x) for x in itertools.combinations(lst, z) if len(x) > 1 ] \n",
    "#    print(a)\n",
    "\n",
    "    for xx in a:\n",
    "        test_pred = []\n",
    "        s = ''\n",
    "        combine_prediction(xx, test_pred)\n",
    "#        print(s)\n",
    "\n",
    "        #print(len(test_pred))\n",
    "\n",
    "        all_pred = [] ;\n",
    "        for i in range(len(test_pred)):\n",
    "            if ( i == 0 ):\n",
    "                all_pred = test_pred[i]\n",
    "            else:\n",
    "                all_pred = np.column_stack((all_pred, test_pred[i]) )\n",
    "\n",
    "        top_seven = []\n",
    "        for i in range(len(all_pred)):\n",
    "            unique, counts = np.unique(all_pred[i], return_counts=True)\n",
    "            x = dict(zip(unique, counts))\n",
    "            sorted_x = sorted(x.items(), key=operator.itemgetter(1), reverse=True) # sorted by value\n",
    "            l = list(islice([int(x) for x,y in sorted_x],top_n))\n",
    "            while ( len(l) < top_n ):\n",
    "                l.append(-1)\n",
    "\n",
    "            top_seven.append(l)\n",
    "            \n",
    "\n",
    "#        print(len(top_seven))\n",
    "#         if(len(top_seven[0]) < top_n ):\n",
    "#             print(\"*** Caught \", )\n",
    "        columns = ['N'+str(i+1) for i in range(len(top_seven[0]))]\n",
    "#        print(columns)\n",
    "        df_top_seven = pd.DataFrame(top_seven, columns=columns)\n",
    "        r = mtr.getAccuracyCount(np.array(df_top_seven)) ;\n",
    "        matched, weighted_match = mtr.print_weighted_numbers(df_top_seven.values)\n",
    "        r = sum(weighted_match)\n",
    "\n",
    "        dict_accuracy.update({s: r})\n",
    "\n",
    "t_accuracy = sorted(dict_accuracy.items(),key=operator.itemgetter(1), reverse=True)\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched, weighted_match = mtr.print_weighted_numbers(df_top_seven.values)\n",
    "print(matched)\n",
    "print(weighted_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 7\n",
    "print(t_accuracy[:n])\n",
    "\n",
    "a = [x[0].split('_') for x in t_accuracy[:n] ] \n",
    "print(a)\n",
    "for xx in a:\n",
    "    test_pred = []\n",
    "    s = ''\n",
    "    combine_prediction(xx, test_pred)\n",
    "    all_pred = [] ;\n",
    "    for i in range(len(test_pred)):\n",
    "        if ( i == 0 ):\n",
    "            all_pred = test_pred[i]\n",
    "        else:\n",
    "            all_pred = np.column_stack((all_pred, test_pred[i]) )\n",
    "\n",
    "    top_seven = []\n",
    "    for i in range(len(all_pred)):\n",
    "        unique, counts = np.unique(all_pred[i], return_counts=True)\n",
    "        x = dict(zip(unique, counts))\n",
    "        sorted_x = sorted(x.items(), key=operator.itemgetter(1), reverse=True) # sorted by value\n",
    "        l = list(islice([int(x) for x,y in sorted_x],top_n))\n",
    "        while ( len(l) < top_n ):\n",
    "          l.append(-1)\n",
    "        top_seven.append(l)\n",
    "\n",
    "\n",
    "    columns = ['N'+str(i+1) for i in range(len(top_seven[0]))]\n",
    "    df_top_seven = pd.DataFrame(top_seven, columns=columns)\n",
    "    r = mtr.getAccuracyCount(np.array(df_top_seven)) ;\n",
    "    print ( \"Accuracy: \",  r)\n",
    "    dict_accuracy.update({s: r})\n",
    "    mtr.plot_matched_counts(df_top_seven.values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Nov 26\n",
    "# 16 22 28 31 38 46 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep track of all results\n",
    "#df_predictions = []\n",
    "\n",
    "#print(df_predictions)\n",
    "#mtr = MyTotoResearch(algo_no=1)\n",
    "def getAllData(df):\n",
    "#     drop_cols = ['T', 'L','M','S','R','E','A','V' ,'J','U','K']\n",
    "#     X = df.drop(drop_cols, axis=1)\n",
    "\n",
    "    use_cols = ['Ph','il','age','dist','adia','sundist','sunadia']\n",
    "    X = df[use_cols]\n",
    "    return X\n",
    "\n",
    "lresult, df = mtr.load_totodata()\n",
    "\n",
    "test_data = mtr.get_test_data()\n",
    "X = mtr.modified_dataset(getAllData(test_data)) #\n",
    "\n",
    "print(len(df_predictions))\n",
    "for n in range(len(df_predictions)):\n",
    "    print( mtr.getAccuracyCount(np.array(df_predictions[n])))\n",
    "    mtr.print_predictions(df_predictions[n])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

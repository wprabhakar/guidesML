{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      A gift that fits conveniently into your travel...\n",
       "1      Christmas shopping made easier with THEFACESHO...\n",
       "2      All you want for Christmas is THEFACESHOP x li...\n",
       "3      #WHATSNEW: Play up your look with the all-new ...\n",
       "4      This is what you have all been waiting for, TH...\n",
       "5      Keep your skin healthy and radiant in less tha...\n",
       "6      #WHATSNEW: Fresh, unique and versatile. Its cr...\n",
       "7      THEFACESHOP Singapore is officially on Lazada ...\n",
       "8      We‚Äôre 1 day away from the opening launch of TH...\n",
       "9      Your favourite THEFACESHOP Singapore products ...\n",
       "10     üõçÔ∏è Psss..guess what's brewing at Lazada?\\n\\nSt...\n",
       "11     [CONTEST] Tis' the season of gifting and we‚Äôre...\n",
       "12     Black Friday Member's Exclusive Double Points ...\n",
       "13     Leave her smiling from ear to ear with THEFACE...\n",
       "14     As the saying goes, 'not to prime is a crime'....\n",
       "15     It‚Äôs Christmas, and every family member deserv...\n",
       "16     Finding the perfect gift for co-workers is no ...\n",
       "17     #WHATSNEW: With its non-drying texture and lon...\n",
       "18     This could be the Christmas wish list he has f...\n",
       "19     #WHATSNEW: Bring the sweetest treat to your va...\n",
       "20     Our thoughtfully curated list of gift ideas ma...\n",
       "21     #WHATSNEW: For lips as smooth, soft and delici...\n",
       "22     Celebrate this holiday season by gifting bundl...\n",
       "23     üì¢ PSA: Calling out to all THEFACESHOP members!...\n",
       "24     #WHATSNEW: With 15 delicately curated cool and...\n",
       "25     Emulsion vs Cream! You may use them both, but ...\n",
       "26     Wake up with beautiful, luminous fresh skin wi...\n",
       "27     Last Chance to earn up to 8X points* on your p...\n",
       "28     Infused with Safflower, Ginseng and Goji Berry...\n",
       "29     Turn back the clock with YEHWADAM Hwansaenggo ...\n",
       "                             ...                        \n",
       "652    [GONG XI FA CAI] Dear Valued Customers, kindly...\n",
       "653    [CHEERS TO HEALTHY SKIN] this Lunar New Year w...\n",
       "654    [MOISTURE, MATT OR GLOSSY? üíÑ] Don't forget to ...\n",
       "655    [LUNAR NEW YEAR LUCKY 8 SPECIAL] Join us as we...\n",
       "656    [SURE WIN LUNAR NEW YEAR LUCKY DIP] \\n\\nSpend ...\n",
       "657    [GRAB LIMITED EDITION MYSTERY FORTUNE BAGS] At...\n",
       "658    [SURE WIN LUNAR NEW YEAR LUCKY DIP] \\n\\nSpend ...\n",
       "659    [LUNAR NEW YEAR LUCKY 8 SPECIALS: DAY 4] Lucky...\n",
       "660    [LUNAR NEW YEAR LUCKY 8 SPECIALS: DAY 2] Lucky...\n",
       "661    [LUNAR NEW YEAR LUCKY 8 SPECIALS: DAY 2] Lucky...\n",
       "662    [LUNAR NEW YEAR LUCKY 8 SPECIALS: DAY 1] Lucky...\n",
       "663    [LUNAR NEW YEAR LUCKY 8 SPECIALS TEASER] Hoora...\n",
       "664    [BLOOM WHERE YOU ARE PLANTED üå∏] #YEHWADAM Revi...\n",
       "665    [DISNEY PRINCESS TONE UP CUSHION ‚ú®] \\nHappy Mo...\n",
       "666    [BECAUSE WE BELIEVE IN...] Repairing & hydrati...\n",
       "667    [WIN THEFACESHOP NEW MOISTURE TOUCH LIPSTICK -...\n",
       "668    [ONE TOUCH TO PERFECTION] Choose from 20 #gorg...\n",
       "669    [FIND OUT YOUR SKIN TYPE] Dry? Normal? Combina...\n",
       "670                                                  NaN\n",
       "671    [CHINESE NEW YEAR - LUCKY 8 BUNDLES] Happy Mid...\n",
       "672    Wondering which lip color is the perfect fit f...\n",
       "673    [DR. BELMEUR CUSHIONS] Drop by our stores to t...\n",
       "674    [DR. BELMEUR IN Nanyang Polytechnic OPEN HOUSE...\n",
       "675    [GIFT WITH PURCHASE OF NEW LIPSTICK üíÑ] Receive...\n",
       "676    [CNY LUCKY 8 - EYESHADOW BUNDLE üêî] Get your fa...\n",
       "677    [DR. BELMEUR IN NYP OPEN HOUSE '17] It's time ...\n",
       "678    [HAPPY WEDNESDAY] Don't forget to relax and re...\n",
       "679    [THEFACESHOP | Disney Princess üíï] The beauty s...\n",
       "680                                                  NaN\n",
       "681    [YEAR END SPECIALS] Hooray! It's the last day ...\n",
       "Name: message, Length: 682, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create URL to JSON file (alternatively this can be a filepath)\n",
    "url = 'posts.json'\n",
    "\n",
    "# Load the first sheet of the JSON file into a data frame\n",
    "df = pd.read_json(url, orient='columns')\n",
    "\n",
    "# View the first ten rows\n",
    "df.head(10)\n",
    "\n",
    "df.message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-bf7ffca5355a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbrown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Install nltk\n",
    "#!conda install -c anaconda nltk\n",
    "#import nltk\n",
    "#nltk.download ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'work', 'and', 'no', 'play', 'makes', 'jack', 'a', 'dull', 'boy', 'all', 'work', 'and', 'no', 'play']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    " \n",
    "data = \"All work and no play makes jack a dull boy, all work and no play\"\n",
    "\n",
    "#Remove punuations\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "data = tokenizer.tokenize(data)\n",
    "#print(word_tokenize(data))\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today the Netherlands celebrates King's Day. To honor this tradition, the Dutch embassy in San Francisco invited me to\n",
      "('Netherlands', 'NNP')\n",
      "('King', 'NNP')\n",
      "('Day', 'NNP')\n",
      "('Dutch', 'NNPS')\n",
      "('San', 'NNP')\n",
      "('Francisco', 'NNP')\n",
      "Tokenized Sentence: ['Today', 'the', 'Netherlands', 'celebrates', 'King', \"'s\", 'Day', '.', 'To', 'honor', 'this', 'tradition', ',', 'the', 'Dutch', 'embassy', 'in', 'San', 'Francisco', 'invited', 'me', 'to']\n",
      "Filterd Sentence: ['Today', 'Netherlands', 'celebrates', 'King', \"'s\", 'Day', '.', 'To', 'honor', 'tradition', ',', 'Dutch', 'embassy', 'San', 'Francisco', 'invited']\n",
      "Today the Netherlands celebrates King's Day. To honor this tradition, the Dutch embassy in San Francisco invited me to\n"
     ]
    }
   ],
   "source": [
    "document = 'Today the Netherlands celebrates King\\'s Day. To honor this tradition, the Dutch embassy in San Francisco invited me to'\n",
    "data = tokenizer.tokenize(document)\n",
    "print(document)\n",
    "#data = nltk.pos_tag(nltk.word_tokenize(document))\n",
    "#sentences = nltk.sent_tokenize(document)   \n",
    " \n",
    "# data = []\n",
    "# for sent in sentences:\n",
    "#     data = data + nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "\n",
    " \n",
    "data = nltk.pos_tag(data)\n",
    "for word in data: \n",
    "    if 'NNP' in word[1]: \n",
    "        print(word)\n",
    "\n",
    "# tokens=nltk.word_tokenize(document)\n",
    "# print(nltk.pos_tag(tokens))\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "#print(stop_words)\n",
    "\n",
    "#Remove stop_words\n",
    "filtered_sent=[]\n",
    "for w in tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sent.append(w)\n",
    "print(\"Tokenized Sentence:\",tokens)\n",
    "print(\"Filterd Sentence:\",filtered_sent)\n",
    "print(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: ['Today', 'Netherlands', 'celebrates', 'King', \"'s\", 'Day', '.', 'To', 'honor', 'tradition', ',', 'Dutch', 'embassy', 'San', 'Francisco', 'invited']\n",
      "Stemmed Sentence: ['today', 'netherland', 'celebr', 'king', \"'s\", 'day', '.', 'To', 'honor', 'tradit', ',', 'dutch', 'embassi', 'san', 'francisco', 'invit']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_words=[]\n",
    "for w in filtered_sent:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "\n",
    "print(\"Filtered Sentence:\",filtered_sent)\n",
    "print(\"Stemmed Sentence:\",stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'NN': 9,\n",
       "         'VBP': 1,\n",
       "         'POS': 1,\n",
       "         '.': 1,\n",
       "         'TO': 1,\n",
       "         'VB': 1,\n",
       "         ',': 1,\n",
       "         'JJ': 1})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(tag for word,tag in tagged)\n",
    "counts\n",
    "#Counter({'DT': 2, 'NN': 2, 'VB': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pythonspot-9329.kxcdn.com/wp-content/uploads/2016/08/nltk-speech-codes.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment Analysis Example\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import names\n",
    " \n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "positive_vocab = [ 'awesome', 'outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great', ':)' ]\n",
    "negative_vocab = [ 'bad', 'terrible','useless', 'hate', ':(' ]\n",
    "neutral_vocab = [ 'movie','the','sound','was','is','actors','did','know','words','not' ]\n",
    " \n",
    "positive_features = [(word_feats(pos), 'pos') for pos in positive_vocab]\n",
    "negative_features = [(word_feats(neg), 'neg') for neg in negative_vocab]\n",
    "neutral_features = [(word_feats(neu), 'neu') for neu in neutral_vocab]\n",
    " \n",
    "train_set = negative_features + positive_features + neutral_features\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(train_set) \n",
    " \n",
    "# Predict\n",
    "neg = 0\n",
    "pos = 0\n",
    "sentence = \"Awesome movie, I liked it\"\n",
    "sentence = sentence.lower()\n",
    "words = sentence.split(' ')\n",
    "for word in words:\n",
    "    classResult = classifier.classify( word_feats(word))\n",
    "    if classResult == 'neg':\n",
    "        neg = neg + 1\n",
    "    if classResult == 'pos':\n",
    "        pos = pos + 1\n",
    " \n",
    "print('Positive: ' + str(float(pos)/len(words)))\n",
    "print('Negative: ' + str(float(neg)/len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

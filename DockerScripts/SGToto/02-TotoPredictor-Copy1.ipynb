{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Where can we get the data ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 9)\n"
     ]
    }
   ],
   "source": [
    "# Generate predictors\n",
    "X_raw = np.random.random(100*9)\n",
    "X_raw = np.reshape(X_raw, (100, 9))\n",
    "\n",
    "# Standardize the predictors\n",
    "scaler = StandardScaler().fit(X_raw)\n",
    "X = scaler.transform(X_raw)\n",
    "\n",
    "# Add an intercept column to the model.  Essentially add 1.\n",
    "#X = np.abs(np.concatenate((np.ones((X.shape[0],1)), X), axis=1))\n",
    "\n",
    "print(X.shape)\n",
    "#print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3.22837193 -13.28519907 -12.04249453   3.25367343 -13.73964687\n",
      "  10.62541322 -10.27630005  -2.13634762 -23.37443697   1.44027384\n",
      " -14.54257482  -4.31415827   3.61238716 -24.30138388  -2.6051898\n",
      "  13.12320224   9.47060367   6.89827267  26.87800061  15.20467588\n",
      "  11.45982132 -15.76661224  -2.33532527 -12.31019993 -13.10080956\n",
      "  20.76860392 -13.23811085   2.93862903  17.46179181   9.73393013\n",
      "  -1.9473759  -12.05232674 -11.9552322   19.68929573  12.19413759\n",
      "  -4.00540253   9.72235637  -6.4919078   -1.0462246    8.3476282\n",
      "  -7.55838013   2.88029136 -20.33610587  -7.79637239   8.11505389\n",
      " -21.51576329 -31.4239922  -10.09427758   2.60203556   7.12002922\n",
      " -38.21378718  17.44682659 -12.05881451  22.01628766  22.20822399\n",
      " -20.60085557  14.44873096  10.18232719  14.87996322  -3.13307234\n",
      "  29.81996721   3.10815652  -2.328849     3.5401399  -17.64742408\n",
      "  -7.16543961  -6.67958679  -7.91741451   2.95967729  11.51399966\n",
      "  -2.16546046   5.06157388  15.18635625  -0.10514701  -0.61872241\n",
      "  21.64545008   5.22068088  28.52119192  22.31842565  -3.41387493\n",
      "  -2.59139902 -21.12787194  -0.18529703 -20.97942986  -0.26944616\n",
      "  -7.47757272  -1.71155102   0.90852275   3.52548848  -6.59636494\n",
      "   6.76826666  30.19908904   4.00937458 -14.91409069 -11.54597962\n",
      "   6.0505988   -4.13574846  12.18751956 -31.32592334  18.00595665]\n"
     ]
    }
   ],
   "source": [
    "# We have the data.  How about the target ?\n",
    "# Lets generate target\n",
    "\n",
    "# Define coefficients\n",
    "some_weights = np.array([2,6,7,3,5,7,1,2,4])\n",
    "# Y = Xb\n",
    "Y_truth = np.matmul(X,some_weights)\n",
    "\n",
    "print(Y_truth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_pred, y_truth, w=None):\n",
    "    y_true = np.array(y_truth)\n",
    "    y_pred = np.array(y_pred)\n",
    "    if np.any(y_true==0):\n",
    "        print(\"Remove zeros from set...\")\n",
    "        idx = np.where(y_true==0)\n",
    "        y_true = np.delete(y_true, idx)\n",
    "        y_pred = np.delete(y_pred, idx)\n",
    "        if type(w) != type(None):\n",
    "            w = np.array(w)\n",
    "            w = np.delete(w, idx)     \n",
    "    if type(w) == type(None):\n",
    "        return(np.mean(np.abs((y_true - y_pred) / y_true)) * 100)\n",
    "    return min(100,100/sum(w)*np.dot(w, (np.abs((y_true - y_pred) / y_true))))\n",
    "    \n",
    "custom_loss_function = mean_absolute_percentage_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.99999991 6.00000006 6.99999995 3.00000006 5.00000002 7.\n",
      " 0.99999996 1.99999999 3.99999999]\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def objective_function(w, X, Y):\n",
    "    return custom_loss_function(np.matmul(X,w), Y)\n",
    "\n",
    "Y = Y_truth\n",
    "# add some noise\n",
    "#Y = Y_truth*np.exp(np.random.normal(loc=0.0, scale=0.2, size=100))\n",
    "\n",
    "#print(Y)\n",
    "\n",
    "# provide a starting weights at which to initialize\n",
    "# the parameter search space\n",
    "initial_weights = np.array([1]*X.shape[1])\n",
    "#initial_weights = np.array([np.mean(X)]*X.shape[1])\n",
    "\n",
    "result = minimize(objective_function, initial_weights, args=(X,Y),\n",
    "                  method='BFGS', options={'maxiter': 1000})\n",
    "\n",
    "# The optimal values for the input parameters are stored\n",
    "# in result.x\n",
    "estimated_weights = result.x\n",
    "\n",
    "print(estimated_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>initial_weights</th>\n",
       "      <th>estimated_weights</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   initial_weights  estimated_weights  error\n",
       "0                2                2.0    0.0\n",
       "1                6                6.0   -0.0\n",
       "2                7                7.0    0.0\n",
       "3                3                3.0   -0.0\n",
       "4                5                5.0   -0.0\n",
       "5                7                7.0    0.0\n",
       "6                1                1.0    0.0\n",
       "7                2                2.0    0.0\n",
       "8                4                4.0    0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    \"initial_weights\": some_weights, \n",
    "    \"estimated_weights\": estimated_weights,\n",
    "    \"error\": np.array(np.around(some_weights-estimated_weights,2))\n",
    "})\n",
    "\n",
    "#[[\"y_truth\", \"y_pred\", \"error\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(custom_loss_function(np.matmul(X,estimated_weights), Y),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManyTargetsModel:\n",
    "    \"\"\"\n",
    "    Linear model: Y = XB, fit by minimizing the provided loss_function\n",
    "    with L2 regularization\n",
    "    \"\"\"\n",
    "    def __init__(self, loss_function, regularization=0.00012):\n",
    "        self.regularization = regularization\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        prediction = np.matmul(X, self.beta)\n",
    "        return(prediction)\n",
    "\n",
    "#     def score(self, X, y_true):\n",
    "#         return(sum(self.predict(X)))\n",
    "\n",
    "\n",
    "    def model_error(self):\n",
    "        error = self.loss_function(\n",
    "            self.predict(self.X), self.Y, w=self.sample_weights\n",
    "        )\n",
    "        return(error)\n",
    "    \n",
    "    def l2_regularized_loss(self, beta):\n",
    "        self.beta = beta\n",
    "        m = len(self.X)\n",
    "#        return (self.model_error())\n",
    "        return(self.model_error()/m + \\\n",
    "                sum(((self.regularization)/2*m)*(np.array(self.beta)**2)))\n",
    "    \n",
    "    def fit(self, X, Y, maxiter=250, sample_weights=None, initial_weights=None):        \n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "        self.beta = None  #latest weights\n",
    "        self.sample_weights = sample_weights\n",
    "        self.estimated_weights = initial_weights\n",
    "        \n",
    "        # Initialize estimated_weights\n",
    "        if type(self.estimated_weights)==type(None):\n",
    "            # set estimated_weights = 1 for every feature\n",
    "            self.estimated_weights = np.array([1]*self.X.shape[1])\n",
    "            \n",
    "        res = minimize(self.l2_regularized_loss, self.estimated_weights\n",
    "                       #, args=(X,Y),\n",
    "                       ,method='BFGS', options={'maxiter': maxiter})\n",
    "        self.beta = res.x\n",
    "        self.estimated_weights = self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.99999864, 5.99999705, 6.99999644, 2.99999825, 4.99999785,\n",
       "       6.99999648, 0.9999998 , 1.99999899, 3.99999836])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ManyTargetsModel(mean_absolute_percentage_error, regularization=0.000012)\n",
    "model.fit( X, Y)\n",
    "model.estimated_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>initial_weights</th>\n",
       "      <th>estimated_weights</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1.999999</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>5.999997</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>6.999996</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.999998</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4.999998</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>6.999996</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>1.999999</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>3.999998</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   initial_weights  estimated_weights     error\n",
       "0                2           1.999999  0.000001\n",
       "1                6           5.999997  0.000003\n",
       "2                7           6.999996  0.000004\n",
       "3                3           2.999998  0.000002\n",
       "4                5           4.999998  0.000002\n",
       "5                7           6.999996  0.000004\n",
       "6                1           1.000000  0.000000\n",
       "7                2           1.999999  0.000001\n",
       "8                4           3.999998  0.000002"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    \"initial_weights\": some_weights, \n",
    "    \"estimated_weights\": model.estimated_weights,\n",
    "    \"error\": np.array(np.around(some_weights-model.estimated_weights,6))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted Y vs. observed Y\n",
    "#plt.scatter(model.predict(X), Y)\n",
    "round(custom_loss_function(np.matmul(X,model.estimated_weights), Y),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Used to cross-validate models and identify optimal lambda\n",
    "class CustomCrossValidator:\n",
    "    \n",
    "    \"\"\"\n",
    "    Cross validates arbitrary model using MAPE criterion on\n",
    "    list of lambdas.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def cross_validate(self, ModelClass, X, Y, lambdas, \n",
    "                        loss_function, \n",
    "                        sample_weights=None,\n",
    "                        num_folds=10):\n",
    "        \"\"\"\n",
    "        lambdas: set of regularization parameters to try\n",
    "        num_folds: number of folds to cross-validate against\n",
    "        \"\"\"\n",
    "        \n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.ModelClass = ModelClass\n",
    "        self.loss_function = loss_function\n",
    "        self.sample_weights = sample_weights\n",
    "    \n",
    "\n",
    "        self.lambdas = lambdas\n",
    "        self.cv_scores = []\n",
    "        X = self.X\n",
    "        Y = self.Y \n",
    "        \n",
    "        # Beta values are not likely to differ dramatically\n",
    "        # between differnt folds. Keeping track of the estimated\n",
    "        # beta coefficients and passing them as starting values\n",
    "        # to the .fit() operator on our model class can significantly\n",
    "        # lower the time it takes for the minimize() function to run\n",
    "        beta_init = None\n",
    "        \n",
    "        for lam in self.lambdas:\n",
    "            print(\"Lambda: {}\".format(lam))\n",
    "            \n",
    "            # Split data into training/holdout sets\n",
    "            kf = KFold(n_splits=num_folds, shuffle=True)\n",
    "            kf.get_n_splits(X)\n",
    "            \n",
    "            # Keep track of the error for each holdout fold\n",
    "            k_fold_scores = []\n",
    "            \n",
    "            # Iterate over folds, using k-1 folds for training\n",
    "            # and the k-th fold for validation\n",
    "            f = 1\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                # Training data\n",
    "                CV_X = X[train_index,:]\n",
    "                CV_Y = Y[train_index]\n",
    "                CV_weights = None\n",
    "                if type(self.sample_weights) != type(None):\n",
    "                    CV_weights = self.sample_weights[train_index]\n",
    "                \n",
    "                # Holdout data\n",
    "                holdout_X = X[test_index,:]\n",
    "                holdout_Y = Y[test_index]\n",
    "                holdout_weights = None\n",
    "                if type(self.sample_weights) != type(None):\n",
    "                    holdout_weights = self.sample_weights[test_index]\n",
    "                \n",
    "                # Fit model to training sample\n",
    "                lambda_fold_model = self.ModelClass(self.loss_function, regularization=lam)\n",
    "                lambda_fold_model.fit(CV_X, CV_Y, sample_weights=CV_weights, initial_weights=beta_init)\n",
    "                \n",
    "                # Extract beta values to pass as beta_init \n",
    "                # to speed up estimation of the next fold\n",
    "                beta_init = lambda_fold_model.beta\n",
    "                \n",
    "                # Calculate holdout error\n",
    "                fold_preds = lambda_fold_model.predict(holdout_X)\n",
    "                fold_mape = self.loss_function(holdout_Y, fold_preds, w=holdout_weights)\n",
    "                k_fold_scores.append(fold_mape)\n",
    "                print(\"Fold: {}. Error: {}\".format( f, fold_mape))\n",
    "                f += 1\n",
    "            \n",
    "            # Error associated with each lambda is the average\n",
    "            # of the errors across the k folds\n",
    "            lambda_scores = np.mean(k_fold_scores)\n",
    "            print(\"** AVERAGE: {}\".format(lambda_scores))\n",
    "            self.cv_scores.append(lambda_scores)\n",
    "        \n",
    "        # Optimal lambda is that which minimizes the cross-validation error\n",
    "        self.lambda_star_index = np.argmin(self.cv_scores)\n",
    "        self.lambda_star = self.lambdas[self.lambda_star_index]\n",
    "        print(\"\\n\\n**BEST LAMBDA: {}**\".format(self.lambda_star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 1\n",
      "Fold: 1. Error: 336858.6669361465\n",
      "Fold: 2. Error: 373631.2280348293\n",
      "Fold: 3. Error: 1650006.1947128961\n",
      "Fold: 4. Error: 1536954.6373374881\n",
      "Fold: 5. Error: 996228.6772652713\n",
      "** AVERAGE: 978735.8808573263\n",
      "Lambda: 0.1\n",
      "Fold: 1. Error: 120947.39792130326\n",
      "Fold: 2. Error: 51381.13491326714\n",
      "Fold: 3. Error: 36458.61144592528\n",
      "Fold: 4. Error: 44678.51463257665\n",
      "Fold: 5. Error: 172117.82334977048\n",
      "** AVERAGE: 85116.69645256856\n",
      "Lambda: 0.01\n",
      "Fold: 1. Error: 14096.839790096441\n",
      "Fold: 2. Error: 12315.339566307\n",
      "Fold: 3. Error: 52721.62694026774\n",
      "Fold: 4. Error: 10260.738284255334\n",
      "Fold: 5. Error: 13897.174068118533\n",
      "** AVERAGE: 20658.34372980901\n",
      "Lambda: 0.001\n",
      "Fold: 1. Error: 4566.477059083119\n",
      "Fold: 2. Error: 1212.0620956125717\n",
      "Fold: 3. Error: 4832.549165204718\n",
      "Fold: 4. Error: 8375.716414623304\n",
      "Fold: 5. Error: 6776.8599809071175\n",
      "** AVERAGE: 5152.732943086166\n",
      "Lambda: 0.0001\n",
      "Fold: 1. Error: 35.731281298015475\n",
      "Fold: 2. Error: 86.25535128920625\n",
      "Fold: 3. Error: 38.84530753007775\n",
      "Fold: 4. Error: 37.99503240804689\n",
      "Fold: 5. Error: 35.05450175325458\n",
      "** AVERAGE: 46.77629485572019\n",
      "Lambda: 1e-05\n",
      "Fold: 1. Error: 40.63225238141478\n",
      "Fold: 2. Error: 34.734335838754006\n",
      "Fold: 3. Error: 37.11421319347684\n",
      "Fold: 4. Error: 39.793106840422936\n",
      "Fold: 5. Error: 36.56430602592601\n",
      "** AVERAGE: 37.767642855998915\n",
      "Lambda: 1e-06\n",
      "Fold: 1. Error: 2.5354998850971073e-06\n",
      "Fold: 2. Error: 2.690636552477118e-06\n",
      "Fold: 3. Error: 3.87394840410045e-06\n",
      "Fold: 4. Error: 2.0272258818058083e-06\n",
      "Fold: 5. Error: 2.1254194692410637e-06\n",
      "** AVERAGE: 2.6505460385443095e-06\n",
      "\n",
      "\n",
      "**BEST LAMBDA: 1e-06**\n"
     ]
    }
   ],
   "source": [
    "# specify lambdas values to search\n",
    "lambdas = [1, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001]\n",
    "\n",
    "cross_validator = CustomCrossValidator()\n",
    "cross_validator.cross_validate(ManyTargetsModel, X, Y,  lambdas, custom_loss_function,  num_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with regularization:  1e-06\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>y_truth</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.507579</td>\n",
       "      <td>-0.563400</td>\n",
       "      <td>0.562276</td>\n",
       "      <td>-1.332146</td>\n",
       "      <td>1.050077</td>\n",
       "      <td>-0.462330</td>\n",
       "      <td>-1.296277</td>\n",
       "      <td>-0.530634</td>\n",
       "      <td>0.999397</td>\n",
       "      <td>3.228372</td>\n",
       "      <td>3.228372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.691418</td>\n",
       "      <td>-1.092415</td>\n",
       "      <td>1.645547</td>\n",
       "      <td>0.116214</td>\n",
       "      <td>-1.293266</td>\n",
       "      <td>-0.630865</td>\n",
       "      <td>-0.540870</td>\n",
       "      <td>0.685703</td>\n",
       "      <td>-1.290875</td>\n",
       "      <td>-13.285199</td>\n",
       "      <td>-13.285200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.534903</td>\n",
       "      <td>-0.941776</td>\n",
       "      <td>-0.516393</td>\n",
       "      <td>-0.662857</td>\n",
       "      <td>-0.619402</td>\n",
       "      <td>-1.427847</td>\n",
       "      <td>1.506040</td>\n",
       "      <td>0.650407</td>\n",
       "      <td>1.606690</td>\n",
       "      <td>-12.042495</td>\n",
       "      <td>-12.042495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.489120</td>\n",
       "      <td>-0.057204</td>\n",
       "      <td>1.180967</td>\n",
       "      <td>1.490712</td>\n",
       "      <td>1.513530</td>\n",
       "      <td>-1.624721</td>\n",
       "      <td>-1.450672</td>\n",
       "      <td>0.805173</td>\n",
       "      <td>-1.618630</td>\n",
       "      <td>3.253673</td>\n",
       "      <td>3.253674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.216013</td>\n",
       "      <td>0.609950</td>\n",
       "      <td>-0.881171</td>\n",
       "      <td>-1.621498</td>\n",
       "      <td>-0.214712</td>\n",
       "      <td>0.924015</td>\n",
       "      <td>-1.567411</td>\n",
       "      <td>-0.730678</td>\n",
       "      <td>-1.575103</td>\n",
       "      <td>-13.739647</td>\n",
       "      <td>-13.739647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.317969</td>\n",
       "      <td>1.177619</td>\n",
       "      <td>1.751039</td>\n",
       "      <td>-0.159548</td>\n",
       "      <td>0.355435</td>\n",
       "      <td>-0.866265</td>\n",
       "      <td>0.005105</td>\n",
       "      <td>-0.055901</td>\n",
       "      <td>-0.797403</td>\n",
       "      <td>10.625413</td>\n",
       "      <td>10.625413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.667827</td>\n",
       "      <td>-0.597424</td>\n",
       "      <td>0.336092</td>\n",
       "      <td>0.023883</td>\n",
       "      <td>-1.019887</td>\n",
       "      <td>-1.158811</td>\n",
       "      <td>-0.978959</td>\n",
       "      <td>0.644807</td>\n",
       "      <td>1.280015</td>\n",
       "      <td>-10.276300</td>\n",
       "      <td>-10.276301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.093962</td>\n",
       "      <td>0.855995</td>\n",
       "      <td>-1.441042</td>\n",
       "      <td>0.546202</td>\n",
       "      <td>-1.096045</td>\n",
       "      <td>0.998796</td>\n",
       "      <td>-0.887760</td>\n",
       "      <td>-0.833294</td>\n",
       "      <td>0.007861</td>\n",
       "      <td>-2.136348</td>\n",
       "      <td>-2.136348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.147885</td>\n",
       "      <td>-0.102030</td>\n",
       "      <td>-1.711481</td>\n",
       "      <td>0.757459</td>\n",
       "      <td>0.293677</td>\n",
       "      <td>-1.325290</td>\n",
       "      <td>0.609164</td>\n",
       "      <td>-1.115484</td>\n",
       "      <td>-0.979897</td>\n",
       "      <td>-23.374437</td>\n",
       "      <td>-23.374438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.212376</td>\n",
       "      <td>1.297297</td>\n",
       "      <td>-0.711397</td>\n",
       "      <td>1.496720</td>\n",
       "      <td>-0.461206</td>\n",
       "      <td>-1.283986</td>\n",
       "      <td>-0.620666</td>\n",
       "      <td>1.444033</td>\n",
       "      <td>1.399348</td>\n",
       "      <td>1.440274</td>\n",
       "      <td>1.440273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.120168</td>\n",
       "      <td>-0.798034</td>\n",
       "      <td>-0.698064</td>\n",
       "      <td>-0.131630</td>\n",
       "      <td>0.870847</td>\n",
       "      <td>-1.246654</td>\n",
       "      <td>-1.601331</td>\n",
       "      <td>-0.180733</td>\n",
       "      <td>1.025611</td>\n",
       "      <td>-14.542575</td>\n",
       "      <td>-14.542576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1.088539</td>\n",
       "      <td>-0.788254</td>\n",
       "      <td>0.992074</td>\n",
       "      <td>0.397843</td>\n",
       "      <td>-0.054696</td>\n",
       "      <td>-1.334192</td>\n",
       "      <td>-0.051516</td>\n",
       "      <td>0.367258</td>\n",
       "      <td>0.846054</td>\n",
       "      <td>-4.314158</td>\n",
       "      <td>-4.314159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.459945</td>\n",
       "      <td>1.599933</td>\n",
       "      <td>-1.339692</td>\n",
       "      <td>0.789026</td>\n",
       "      <td>0.619296</td>\n",
       "      <td>-0.186033</td>\n",
       "      <td>1.341051</td>\n",
       "      <td>0.204035</td>\n",
       "      <td>0.100020</td>\n",
       "      <td>3.612387</td>\n",
       "      <td>3.612387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.388259</td>\n",
       "      <td>-1.446906</td>\n",
       "      <td>0.988091</td>\n",
       "      <td>-1.215553</td>\n",
       "      <td>-1.611967</td>\n",
       "      <td>-1.027301</td>\n",
       "      <td>-1.391575</td>\n",
       "      <td>0.292325</td>\n",
       "      <td>-0.513886</td>\n",
       "      <td>-24.301384</td>\n",
       "      <td>-24.301385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.795923</td>\n",
       "      <td>1.159641</td>\n",
       "      <td>0.478476</td>\n",
       "      <td>1.497858</td>\n",
       "      <td>-1.172989</td>\n",
       "      <td>-0.655175</td>\n",
       "      <td>0.204002</td>\n",
       "      <td>0.614949</td>\n",
       "      <td>-1.699207</td>\n",
       "      <td>-2.605190</td>\n",
       "      <td>-2.605190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.692741</td>\n",
       "      <td>1.065067</td>\n",
       "      <td>1.227567</td>\n",
       "      <td>0.901992</td>\n",
       "      <td>-1.147615</td>\n",
       "      <td>-0.148743</td>\n",
       "      <td>1.581019</td>\n",
       "      <td>-0.463831</td>\n",
       "      <td>-0.456427</td>\n",
       "      <td>13.123202</td>\n",
       "      <td>13.123203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.471408</td>\n",
       "      <td>0.131728</td>\n",
       "      <td>-0.060300</td>\n",
       "      <td>0.811863</td>\n",
       "      <td>1.125443</td>\n",
       "      <td>0.830276</td>\n",
       "      <td>-0.562109</td>\n",
       "      <td>-0.422493</td>\n",
       "      <td>-1.077031</td>\n",
       "      <td>9.470604</td>\n",
       "      <td>9.470604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.489501</td>\n",
       "      <td>-0.323686</td>\n",
       "      <td>-0.581351</td>\n",
       "      <td>-1.292685</td>\n",
       "      <td>0.610696</td>\n",
       "      <td>1.415118</td>\n",
       "      <td>-0.738773</td>\n",
       "      <td>0.069306</td>\n",
       "      <td>0.862439</td>\n",
       "      <td>6.898273</td>\n",
       "      <td>6.898273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.080739</td>\n",
       "      <td>1.251375</td>\n",
       "      <td>-0.170816</td>\n",
       "      <td>-0.034188</td>\n",
       "      <td>1.403991</td>\n",
       "      <td>0.670841</td>\n",
       "      <td>1.175820</td>\n",
       "      <td>0.284706</td>\n",
       "      <td>1.261368</td>\n",
       "      <td>26.878001</td>\n",
       "      <td>26.878002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.782441</td>\n",
       "      <td>-0.835077</td>\n",
       "      <td>0.328894</td>\n",
       "      <td>1.700289</td>\n",
       "      <td>-0.193583</td>\n",
       "      <td>1.480727</td>\n",
       "      <td>-0.240514</td>\n",
       "      <td>0.925768</td>\n",
       "      <td>0.842175</td>\n",
       "      <td>15.204676</td>\n",
       "      <td>15.204676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.067161</td>\n",
       "      <td>-0.198523</td>\n",
       "      <td>1.381362</td>\n",
       "      <td>0.123919</td>\n",
       "      <td>0.277036</td>\n",
       "      <td>-0.148787</td>\n",
       "      <td>1.660277</td>\n",
       "      <td>0.798713</td>\n",
       "      <td>-0.214345</td>\n",
       "      <td>11.459821</td>\n",
       "      <td>11.459822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-1.182331</td>\n",
       "      <td>-0.221221</td>\n",
       "      <td>-0.414183</td>\n",
       "      <td>-0.492007</td>\n",
       "      <td>-0.113478</td>\n",
       "      <td>-0.477210</td>\n",
       "      <td>-0.407451</td>\n",
       "      <td>0.361489</td>\n",
       "      <td>-1.026746</td>\n",
       "      <td>-15.766612</td>\n",
       "      <td>-15.766613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.215758</td>\n",
       "      <td>0.359574</td>\n",
       "      <td>-0.433139</td>\n",
       "      <td>-1.017972</td>\n",
       "      <td>-1.360996</td>\n",
       "      <td>1.095050</td>\n",
       "      <td>0.430310</td>\n",
       "      <td>-1.711745</td>\n",
       "      <td>0.823603</td>\n",
       "      <td>-2.335325</td>\n",
       "      <td>-2.335326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.600306</td>\n",
       "      <td>-1.094103</td>\n",
       "      <td>-1.051048</td>\n",
       "      <td>-1.247837</td>\n",
       "      <td>1.439596</td>\n",
       "      <td>-0.454037</td>\n",
       "      <td>0.664764</td>\n",
       "      <td>0.529200</td>\n",
       "      <td>0.203248</td>\n",
       "      <td>-12.310200</td>\n",
       "      <td>-12.310200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.494573</td>\n",
       "      <td>1.322509</td>\n",
       "      <td>-0.116617</td>\n",
       "      <td>-0.963281</td>\n",
       "      <td>-1.713432</td>\n",
       "      <td>-1.501945</td>\n",
       "      <td>-0.115030</td>\n",
       "      <td>0.800571</td>\n",
       "      <td>0.313525</td>\n",
       "      <td>-13.100810</td>\n",
       "      <td>-13.100810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.412703</td>\n",
       "      <td>-0.824271</td>\n",
       "      <td>1.720470</td>\n",
       "      <td>1.638013</td>\n",
       "      <td>0.598430</td>\n",
       "      <td>1.304542</td>\n",
       "      <td>-0.541551</td>\n",
       "      <td>1.190768</td>\n",
       "      <td>-1.508108</td>\n",
       "      <td>20.768604</td>\n",
       "      <td>20.768605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1.177722</td>\n",
       "      <td>0.609108</td>\n",
       "      <td>-1.204753</td>\n",
       "      <td>-1.277441</td>\n",
       "      <td>-1.443997</td>\n",
       "      <td>1.222813</td>\n",
       "      <td>-0.472514</td>\n",
       "      <td>0.188130</td>\n",
       "      <td>-0.878793</td>\n",
       "      <td>-13.238111</td>\n",
       "      <td>-13.238111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.671384</td>\n",
       "      <td>-0.303265</td>\n",
       "      <td>1.326540</td>\n",
       "      <td>0.470663</td>\n",
       "      <td>-1.263572</td>\n",
       "      <td>0.326099</td>\n",
       "      <td>-0.560163</td>\n",
       "      <td>1.370548</td>\n",
       "      <td>-1.357022</td>\n",
       "      <td>2.938629</td>\n",
       "      <td>2.938629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.445057</td>\n",
       "      <td>-0.197486</td>\n",
       "      <td>-0.298208</td>\n",
       "      <td>-0.444341</td>\n",
       "      <td>1.367048</td>\n",
       "      <td>1.449482</td>\n",
       "      <td>0.358791</td>\n",
       "      <td>0.451148</td>\n",
       "      <td>0.733594</td>\n",
       "      <td>17.461792</td>\n",
       "      <td>17.461793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.675634</td>\n",
       "      <td>0.769161</td>\n",
       "      <td>-1.392347</td>\n",
       "      <td>-0.419206</td>\n",
       "      <td>1.063023</td>\n",
       "      <td>0.681035</td>\n",
       "      <td>1.360530</td>\n",
       "      <td>-1.229381</td>\n",
       "      <td>1.446903</td>\n",
       "      <td>9.733930</td>\n",
       "      <td>9.733931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1.043403</td>\n",
       "      <td>-1.365863</td>\n",
       "      <td>-1.372621</td>\n",
       "      <td>0.352810</td>\n",
       "      <td>0.909160</td>\n",
       "      <td>0.305495</td>\n",
       "      <td>-1.144606</td>\n",
       "      <td>1.367512</td>\n",
       "      <td>1.054536</td>\n",
       "      <td>-2.165460</td>\n",
       "      <td>-2.165460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1.468835</td>\n",
       "      <td>-1.373475</td>\n",
       "      <td>0.337053</td>\n",
       "      <td>0.378435</td>\n",
       "      <td>1.024131</td>\n",
       "      <td>-0.793935</td>\n",
       "      <td>0.323053</td>\n",
       "      <td>0.201136</td>\n",
       "      <td>1.645410</td>\n",
       "      <td>5.061574</td>\n",
       "      <td>5.061574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.259870</td>\n",
       "      <td>-0.007479</td>\n",
       "      <td>0.917635</td>\n",
       "      <td>-1.123974</td>\n",
       "      <td>-0.382370</td>\n",
       "      <td>1.473714</td>\n",
       "      <td>0.664386</td>\n",
       "      <td>0.261321</td>\n",
       "      <td>0.517198</td>\n",
       "      <td>15.186356</td>\n",
       "      <td>15.186357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>-1.617863</td>\n",
       "      <td>-1.257668</td>\n",
       "      <td>0.357775</td>\n",
       "      <td>0.592762</td>\n",
       "      <td>-0.693092</td>\n",
       "      <td>1.180321</td>\n",
       "      <td>-0.235642</td>\n",
       "      <td>1.170991</td>\n",
       "      <td>-0.127312</td>\n",
       "      <td>-0.105147</td>\n",
       "      <td>-0.105147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1.501840</td>\n",
       "      <td>-0.793812</td>\n",
       "      <td>-0.559829</td>\n",
       "      <td>-0.515484</td>\n",
       "      <td>-1.150349</td>\n",
       "      <td>1.006399</td>\n",
       "      <td>1.165122</td>\n",
       "      <td>0.952453</td>\n",
       "      <td>0.560662</td>\n",
       "      <td>-0.618722</td>\n",
       "      <td>-0.618722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>-1.184754</td>\n",
       "      <td>1.600200</td>\n",
       "      <td>1.777135</td>\n",
       "      <td>-1.137943</td>\n",
       "      <td>1.685633</td>\n",
       "      <td>-0.396103</td>\n",
       "      <td>-0.075974</td>\n",
       "      <td>1.476635</td>\n",
       "      <td>-0.786276</td>\n",
       "      <td>21.645450</td>\n",
       "      <td>21.645451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.168724</td>\n",
       "      <td>-1.084327</td>\n",
       "      <td>-0.450027</td>\n",
       "      <td>-0.056936</td>\n",
       "      <td>0.489832</td>\n",
       "      <td>0.872784</td>\n",
       "      <td>1.407473</td>\n",
       "      <td>-0.493842</td>\n",
       "      <td>1.432939</td>\n",
       "      <td>5.220681</td>\n",
       "      <td>5.220681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.113145</td>\n",
       "      <td>0.193581</td>\n",
       "      <td>1.212916</td>\n",
       "      <td>1.644137</td>\n",
       "      <td>-0.316583</td>\n",
       "      <td>1.334510</td>\n",
       "      <td>1.333003</td>\n",
       "      <td>1.081577</td>\n",
       "      <td>0.613947</td>\n",
       "      <td>28.521192</td>\n",
       "      <td>28.521193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>-1.662389</td>\n",
       "      <td>1.428342</td>\n",
       "      <td>0.935777</td>\n",
       "      <td>-1.286285</td>\n",
       "      <td>1.013095</td>\n",
       "      <td>1.007054</td>\n",
       "      <td>1.026677</td>\n",
       "      <td>0.287650</td>\n",
       "      <td>0.166184</td>\n",
       "      <td>22.318426</td>\n",
       "      <td>22.318427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>-0.547370</td>\n",
       "      <td>1.652340</td>\n",
       "      <td>0.267138</td>\n",
       "      <td>-1.179139</td>\n",
       "      <td>-1.006453</td>\n",
       "      <td>-0.787706</td>\n",
       "      <td>1.293719</td>\n",
       "      <td>-1.423677</td>\n",
       "      <td>0.383530</td>\n",
       "      <td>-3.413875</td>\n",
       "      <td>-3.413875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>-0.474846</td>\n",
       "      <td>-0.265553</td>\n",
       "      <td>0.879794</td>\n",
       "      <td>-1.601382</td>\n",
       "      <td>0.537718</td>\n",
       "      <td>0.221740</td>\n",
       "      <td>-0.752122</td>\n",
       "      <td>0.032860</td>\n",
       "      <td>-1.239293</td>\n",
       "      <td>-2.591399</td>\n",
       "      <td>-2.591399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>-0.667749</td>\n",
       "      <td>-0.405547</td>\n",
       "      <td>-0.731258</td>\n",
       "      <td>0.143400</td>\n",
       "      <td>-1.005243</td>\n",
       "      <td>-0.624594</td>\n",
       "      <td>-1.567544</td>\n",
       "      <td>1.057897</td>\n",
       "      <td>-0.955091</td>\n",
       "      <td>-21.127872</td>\n",
       "      <td>-21.127873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>-0.692552</td>\n",
       "      <td>1.299913</td>\n",
       "      <td>-0.225420</td>\n",
       "      <td>-1.207146</td>\n",
       "      <td>-1.698881</td>\n",
       "      <td>0.972943</td>\n",
       "      <td>0.138543</td>\n",
       "      <td>1.203606</td>\n",
       "      <td>-0.565560</td>\n",
       "      <td>-0.185297</td>\n",
       "      <td>-0.185297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>-1.003586</td>\n",
       "      <td>-0.337045</td>\n",
       "      <td>0.288406</td>\n",
       "      <td>0.439528</td>\n",
       "      <td>-1.105946</td>\n",
       "      <td>-1.377526</td>\n",
       "      <td>-0.833862</td>\n",
       "      <td>1.008648</td>\n",
       "      <td>-1.574607</td>\n",
       "      <td>-20.979430</td>\n",
       "      <td>-20.979431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>-0.486504</td>\n",
       "      <td>0.850218</td>\n",
       "      <td>0.672447</td>\n",
       "      <td>0.194526</td>\n",
       "      <td>-0.812083</td>\n",
       "      <td>-0.927369</td>\n",
       "      <td>1.069031</td>\n",
       "      <td>1.467546</td>\n",
       "      <td>-0.785145</td>\n",
       "      <td>-0.269446</td>\n",
       "      <td>-0.269446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>-0.787942</td>\n",
       "      <td>1.592197</td>\n",
       "      <td>-1.540385</td>\n",
       "      <td>0.639053</td>\n",
       "      <td>0.921415</td>\n",
       "      <td>-1.587679</td>\n",
       "      <td>-0.436150</td>\n",
       "      <td>-0.520107</td>\n",
       "      <td>0.348427</td>\n",
       "      <td>-7.477573</td>\n",
       "      <td>-7.477573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.047600</td>\n",
       "      <td>0.753313</td>\n",
       "      <td>-0.773498</td>\n",
       "      <td>1.861828</td>\n",
       "      <td>-1.362336</td>\n",
       "      <td>0.454267</td>\n",
       "      <td>-1.040387</td>\n",
       "      <td>-0.356724</td>\n",
       "      <td>-0.277994</td>\n",
       "      <td>-1.711551</td>\n",
       "      <td>-1.711551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>-0.316711</td>\n",
       "      <td>0.257017</td>\n",
       "      <td>0.059407</td>\n",
       "      <td>0.379587</td>\n",
       "      <td>-0.626652</td>\n",
       "      <td>-1.206190</td>\n",
       "      <td>0.137181</td>\n",
       "      <td>1.663277</td>\n",
       "      <td>1.639521</td>\n",
       "      <td>0.908523</td>\n",
       "      <td>0.908522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.367914</td>\n",
       "      <td>1.437960</td>\n",
       "      <td>-0.196100</td>\n",
       "      <td>0.376835</td>\n",
       "      <td>1.005571</td>\n",
       "      <td>-1.486241</td>\n",
       "      <td>-0.766558</td>\n",
       "      <td>-1.506509</td>\n",
       "      <td>0.889875</td>\n",
       "      <td>3.525488</td>\n",
       "      <td>3.525488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>-0.796926</td>\n",
       "      <td>-0.907121</td>\n",
       "      <td>-0.047123</td>\n",
       "      <td>0.916734</td>\n",
       "      <td>-0.223235</td>\n",
       "      <td>-0.145526</td>\n",
       "      <td>0.863929</td>\n",
       "      <td>-1.571957</td>\n",
       "      <td>0.608678</td>\n",
       "      <td>-6.596365</td>\n",
       "      <td>-6.596365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>-0.925880</td>\n",
       "      <td>0.939792</td>\n",
       "      <td>-0.206395</td>\n",
       "      <td>-0.526468</td>\n",
       "      <td>1.626930</td>\n",
       "      <td>0.410526</td>\n",
       "      <td>0.388395</td>\n",
       "      <td>-0.587623</td>\n",
       "      <td>-1.054011</td>\n",
       "      <td>6.768267</td>\n",
       "      <td>6.768267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>-1.190868</td>\n",
       "      <td>1.641381</td>\n",
       "      <td>1.675556</td>\n",
       "      <td>-0.904113</td>\n",
       "      <td>1.640461</td>\n",
       "      <td>1.223992</td>\n",
       "      <td>1.307353</td>\n",
       "      <td>0.543188</td>\n",
       "      <td>-1.361998</td>\n",
       "      <td>30.199089</td>\n",
       "      <td>30.199091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.511631</td>\n",
       "      <td>-0.710669</td>\n",
       "      <td>-0.906767</td>\n",
       "      <td>1.561931</td>\n",
       "      <td>-0.520870</td>\n",
       "      <td>0.754824</td>\n",
       "      <td>0.799915</td>\n",
       "      <td>0.599537</td>\n",
       "      <td>1.058324</td>\n",
       "      <td>4.009375</td>\n",
       "      <td>4.009375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>-1.001954</td>\n",
       "      <td>-0.835137</td>\n",
       "      <td>-0.229547</td>\n",
       "      <td>0.547285</td>\n",
       "      <td>0.236499</td>\n",
       "      <td>-0.585836</td>\n",
       "      <td>-1.197340</td>\n",
       "      <td>-0.869889</td>\n",
       "      <td>-0.519728</td>\n",
       "      <td>-14.914091</td>\n",
       "      <td>-14.914091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.675575</td>\n",
       "      <td>-1.089444</td>\n",
       "      <td>0.680448</td>\n",
       "      <td>-1.098028</td>\n",
       "      <td>0.100916</td>\n",
       "      <td>-0.297551</td>\n",
       "      <td>-1.369120</td>\n",
       "      <td>-1.405439</td>\n",
       "      <td>-1.017812</td>\n",
       "      <td>-11.545980</td>\n",
       "      <td>-11.545980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.122300</td>\n",
       "      <td>-0.798027</td>\n",
       "      <td>1.513162</td>\n",
       "      <td>-0.076851</td>\n",
       "      <td>-0.956313</td>\n",
       "      <td>-0.161344</td>\n",
       "      <td>-0.282009</td>\n",
       "      <td>-0.452065</td>\n",
       "      <td>1.332423</td>\n",
       "      <td>6.050599</td>\n",
       "      <td>6.050599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.412504</td>\n",
       "      <td>1.447831</td>\n",
       "      <td>0.228027</td>\n",
       "      <td>-0.881102</td>\n",
       "      <td>-1.606497</td>\n",
       "      <td>-0.028546</td>\n",
       "      <td>0.723239</td>\n",
       "      <td>-1.457871</td>\n",
       "      <td>-0.543955</td>\n",
       "      <td>-4.135748</td>\n",
       "      <td>-4.135749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.577469</td>\n",
       "      <td>-0.981247</td>\n",
       "      <td>1.166527</td>\n",
       "      <td>0.457101</td>\n",
       "      <td>-0.443984</td>\n",
       "      <td>1.584019</td>\n",
       "      <td>-0.481968</td>\n",
       "      <td>0.077078</td>\n",
       "      <td>-0.789335</td>\n",
       "      <td>12.187520</td>\n",
       "      <td>12.187520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-1.581068</td>\n",
       "      <td>-0.030973</td>\n",
       "      <td>-1.485300</td>\n",
       "      <td>-1.432372</td>\n",
       "      <td>1.233094</td>\n",
       "      <td>-1.551342</td>\n",
       "      <td>-0.070133</td>\n",
       "      <td>-1.225179</td>\n",
       "      <td>-1.517330</td>\n",
       "      <td>-31.325923</td>\n",
       "      <td>-31.325924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-0.054256</td>\n",
       "      <td>0.441986</td>\n",
       "      <td>-0.668982</td>\n",
       "      <td>1.309750</td>\n",
       "      <td>0.090323</td>\n",
       "      <td>1.347129</td>\n",
       "      <td>-0.721201</td>\n",
       "      <td>0.279655</td>\n",
       "      <td>1.624137</td>\n",
       "      <td>18.005957</td>\n",
       "      <td>18.005957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0   1.507579 -0.563400  0.562276 -1.332146  1.050077 -0.462330 -1.296277   \n",
       "1  -1.691418 -1.092415  1.645547  0.116214 -1.293266 -0.630865 -0.540870   \n",
       "2   1.534903 -0.941776 -0.516393 -0.662857 -0.619402 -1.427847  1.506040   \n",
       "3   0.489120 -0.057204  1.180967  1.490712  1.513530 -1.624721 -1.450672   \n",
       "4  -1.216013  0.609950 -0.881171 -1.621498 -0.214712  0.924015 -1.567411   \n",
       "5  -0.317969  1.177619  1.751039 -0.159548  0.355435 -0.866265  0.005105   \n",
       "6  -0.667827 -0.597424  0.336092  0.023883 -1.019887 -1.158811 -0.978959   \n",
       "7   1.093962  0.855995 -1.441042  0.546202 -1.096045  0.998796 -0.887760   \n",
       "8   0.147885 -0.102030 -1.711481  0.757459  0.293677 -1.325290  0.609164   \n",
       "9  -1.212376  1.297297 -0.711397  1.496720 -0.461206 -1.283986 -0.620666   \n",
       "10 -1.120168 -0.798034 -0.698064 -0.131630  0.870847 -1.246654 -1.601331   \n",
       "11 -1.088539 -0.788254  0.992074  0.397843 -0.054696 -1.334192 -0.051516   \n",
       "12 -1.459945  1.599933 -1.339692  0.789026  0.619296 -0.186033  1.341051   \n",
       "13 -0.388259 -1.446906  0.988091 -1.215553 -1.611967 -1.027301 -1.391575   \n",
       "14 -0.795923  1.159641  0.478476  1.497858 -1.172989 -0.655175  0.204002   \n",
       "15  1.692741  1.065067  1.227567  0.901992 -1.147615 -0.148743  1.581019   \n",
       "16  0.471408  0.131728 -0.060300  0.811863  1.125443  0.830276 -0.562109   \n",
       "17  0.489501 -0.323686 -0.581351 -1.292685  0.610696  1.415118 -0.738773   \n",
       "18  1.080739  1.251375 -0.170816 -0.034188  1.403991  0.670841  1.175820   \n",
       "19 -0.782441 -0.835077  0.328894  1.700289 -0.193583  1.480727 -0.240514   \n",
       "20 -0.067161 -0.198523  1.381362  0.123919  0.277036 -0.148787  1.660277   \n",
       "21 -1.182331 -0.221221 -0.414183 -0.492007 -0.113478 -0.477210 -0.407451   \n",
       "22  0.215758  0.359574 -0.433139 -1.017972 -1.360996  1.095050  0.430310   \n",
       "23 -0.600306 -1.094103 -1.051048 -1.247837  1.439596 -0.454037  0.664764   \n",
       "24 -0.494573  1.322509 -0.116617 -0.963281 -1.713432 -1.501945 -0.115030   \n",
       "25  0.412703 -0.824271  1.720470  1.638013  0.598430  1.304542 -0.541551   \n",
       "26 -1.177722  0.609108 -1.204753 -1.277441 -1.443997  1.222813 -0.472514   \n",
       "27  0.671384 -0.303265  1.326540  0.470663 -1.263572  0.326099 -0.560163   \n",
       "28  0.445057 -0.197486 -0.298208 -0.444341  1.367048  1.449482  0.358791   \n",
       "29  0.675634  0.769161 -1.392347 -0.419206  1.063023  0.681035  1.360530   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "70  1.043403 -1.365863 -1.372621  0.352810  0.909160  0.305495 -1.144606   \n",
       "71  1.468835 -1.373475  0.337053  0.378435  1.024131 -0.793935  0.323053   \n",
       "72  0.259870 -0.007479  0.917635 -1.123974 -0.382370  1.473714  0.664386   \n",
       "73 -1.617863 -1.257668  0.357775  0.592762 -0.693092  1.180321 -0.235642   \n",
       "74  1.501840 -0.793812 -0.559829 -0.515484 -1.150349  1.006399  1.165122   \n",
       "75 -1.184754  1.600200  1.777135 -1.137943  1.685633 -0.396103 -0.075974   \n",
       "76  0.168724 -1.084327 -0.450027 -0.056936  0.489832  0.872784  1.407473   \n",
       "77  0.113145  0.193581  1.212916  1.644137 -0.316583  1.334510  1.333003   \n",
       "78 -1.662389  1.428342  0.935777 -1.286285  1.013095  1.007054  1.026677   \n",
       "79 -0.547370  1.652340  0.267138 -1.179139 -1.006453 -0.787706  1.293719   \n",
       "80 -0.474846 -0.265553  0.879794 -1.601382  0.537718  0.221740 -0.752122   \n",
       "81 -0.667749 -0.405547 -0.731258  0.143400 -1.005243 -0.624594 -1.567544   \n",
       "82 -0.692552  1.299913 -0.225420 -1.207146 -1.698881  0.972943  0.138543   \n",
       "83 -1.003586 -0.337045  0.288406  0.439528 -1.105946 -1.377526 -0.833862   \n",
       "84 -0.486504  0.850218  0.672447  0.194526 -0.812083 -0.927369  1.069031   \n",
       "85 -0.787942  1.592197 -1.540385  0.639053  0.921415 -1.587679 -0.436150   \n",
       "86  0.047600  0.753313 -0.773498  1.861828 -1.362336  0.454267 -1.040387   \n",
       "87 -0.316711  0.257017  0.059407  0.379587 -0.626652 -1.206190  0.137181   \n",
       "88  0.367914  1.437960 -0.196100  0.376835  1.005571 -1.486241 -0.766558   \n",
       "89 -0.796926 -0.907121 -0.047123  0.916734 -0.223235 -0.145526  0.863929   \n",
       "90 -0.925880  0.939792 -0.206395 -0.526468  1.626930  0.410526  0.388395   \n",
       "91 -1.190868  1.641381  1.675556 -0.904113  1.640461  1.223992  1.307353   \n",
       "92  0.511631 -0.710669 -0.906767  1.561931 -0.520870  0.754824  0.799915   \n",
       "93 -1.001954 -0.835137 -0.229547  0.547285  0.236499 -0.585836 -1.197340   \n",
       "94  1.675575 -1.089444  0.680448 -1.098028  0.100916 -0.297551 -1.369120   \n",
       "95  1.122300 -0.798027  1.513162 -0.076851 -0.956313 -0.161344 -0.282009   \n",
       "96  0.412504  1.447831  0.228027 -0.881102 -1.606497 -0.028546  0.723239   \n",
       "97  1.577469 -0.981247  1.166527  0.457101 -0.443984  1.584019 -0.481968   \n",
       "98 -1.581068 -0.030973 -1.485300 -1.432372  1.233094 -1.551342 -0.070133   \n",
       "99 -0.054256  0.441986 -0.668982  1.309750  0.090323  1.347129 -0.721201   \n",
       "\n",
       "           7         8    y_truth     y_pred  \n",
       "0  -0.530634  0.999397   3.228372   3.228372  \n",
       "1   0.685703 -1.290875 -13.285199 -13.285200  \n",
       "2   0.650407  1.606690 -12.042495 -12.042495  \n",
       "3   0.805173 -1.618630   3.253673   3.253674  \n",
       "4  -0.730678 -1.575103 -13.739647 -13.739647  \n",
       "5  -0.055901 -0.797403  10.625413  10.625413  \n",
       "6   0.644807  1.280015 -10.276300 -10.276301  \n",
       "7  -0.833294  0.007861  -2.136348  -2.136348  \n",
       "8  -1.115484 -0.979897 -23.374437 -23.374438  \n",
       "9   1.444033  1.399348   1.440274   1.440273  \n",
       "10 -0.180733  1.025611 -14.542575 -14.542576  \n",
       "11  0.367258  0.846054  -4.314158  -4.314159  \n",
       "12  0.204035  0.100020   3.612387   3.612387  \n",
       "13  0.292325 -0.513886 -24.301384 -24.301385  \n",
       "14  0.614949 -1.699207  -2.605190  -2.605190  \n",
       "15 -0.463831 -0.456427  13.123202  13.123203  \n",
       "16 -0.422493 -1.077031   9.470604   9.470604  \n",
       "17  0.069306  0.862439   6.898273   6.898273  \n",
       "18  0.284706  1.261368  26.878001  26.878002  \n",
       "19  0.925768  0.842175  15.204676  15.204676  \n",
       "20  0.798713 -0.214345  11.459821  11.459822  \n",
       "21  0.361489 -1.026746 -15.766612 -15.766613  \n",
       "22 -1.711745  0.823603  -2.335325  -2.335326  \n",
       "23  0.529200  0.203248 -12.310200 -12.310200  \n",
       "24  0.800571  0.313525 -13.100810 -13.100810  \n",
       "25  1.190768 -1.508108  20.768604  20.768605  \n",
       "26  0.188130 -0.878793 -13.238111 -13.238111  \n",
       "27  1.370548 -1.357022   2.938629   2.938629  \n",
       "28  0.451148  0.733594  17.461792  17.461793  \n",
       "29 -1.229381  1.446903   9.733930   9.733931  \n",
       "..       ...       ...        ...        ...  \n",
       "70  1.367512  1.054536  -2.165460  -2.165460  \n",
       "71  0.201136  1.645410   5.061574   5.061574  \n",
       "72  0.261321  0.517198  15.186356  15.186357  \n",
       "73  1.170991 -0.127312  -0.105147  -0.105147  \n",
       "74  0.952453  0.560662  -0.618722  -0.618722  \n",
       "75  1.476635 -0.786276  21.645450  21.645451  \n",
       "76 -0.493842  1.432939   5.220681   5.220681  \n",
       "77  1.081577  0.613947  28.521192  28.521193  \n",
       "78  0.287650  0.166184  22.318426  22.318427  \n",
       "79 -1.423677  0.383530  -3.413875  -3.413875  \n",
       "80  0.032860 -1.239293  -2.591399  -2.591399  \n",
       "81  1.057897 -0.955091 -21.127872 -21.127873  \n",
       "82  1.203606 -0.565560  -0.185297  -0.185297  \n",
       "83  1.008648 -1.574607 -20.979430 -20.979431  \n",
       "84  1.467546 -0.785145  -0.269446  -0.269446  \n",
       "85 -0.520107  0.348427  -7.477573  -7.477573  \n",
       "86 -0.356724 -0.277994  -1.711551  -1.711551  \n",
       "87  1.663277  1.639521   0.908523   0.908522  \n",
       "88 -1.506509  0.889875   3.525488   3.525488  \n",
       "89 -1.571957  0.608678  -6.596365  -6.596365  \n",
       "90 -0.587623 -1.054011   6.768267   6.768267  \n",
       "91  0.543188 -1.361998  30.199089  30.199091  \n",
       "92  0.599537  1.058324   4.009375   4.009375  \n",
       "93 -0.869889 -0.519728 -14.914091 -14.914091  \n",
       "94 -1.405439 -1.017812 -11.545980 -11.545980  \n",
       "95 -0.452065  1.332423   6.050599   6.050599  \n",
       "96 -1.457871 -0.543955  -4.135748  -4.135749  \n",
       "97  0.077078 -0.789335  12.187520  12.187520  \n",
       "98 -1.225179 -1.517330 -31.325923 -31.325924  \n",
       "99  0.279655  1.624137  18.005957  18.005957  \n",
       "\n",
       "[100 rows x 11 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_star = cross_validator.lambda_star\n",
    "print('with regularization: ', lambda_star)\n",
    "final_model = ManyTargetsModel(custom_loss_function, regularization=lambda_star)\n",
    "final_model.fit(X, Y)\n",
    "final_model.estimated_weights\n",
    "y_pred = final_model.predict(X)\n",
    "train_data = pd.DataFrame(X)\n",
    "train_data['y_truth'] = Y\n",
    "train_data['y_pred'] = y_pred\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(custom_loss_function(np.matmul(X,final_model.estimated_weights), Y),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.random.random((10,9))\n",
    "test_result = final_model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.602090</td>\n",
       "      <td>0.812987</td>\n",
       "      <td>0.846952</td>\n",
       "      <td>0.857944</td>\n",
       "      <td>0.214467</td>\n",
       "      <td>0.141252</td>\n",
       "      <td>0.302948</td>\n",
       "      <td>0.897758</td>\n",
       "      <td>0.797035</td>\n",
       "      <td>21.932302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.624330</td>\n",
       "      <td>0.850129</td>\n",
       "      <td>0.332856</td>\n",
       "      <td>0.265681</td>\n",
       "      <td>0.334010</td>\n",
       "      <td>0.640477</td>\n",
       "      <td>0.829841</td>\n",
       "      <td>0.048767</td>\n",
       "      <td>0.982292</td>\n",
       "      <td>20.486398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.805075</td>\n",
       "      <td>0.324719</td>\n",
       "      <td>0.812155</td>\n",
       "      <td>0.780388</td>\n",
       "      <td>0.794551</td>\n",
       "      <td>0.120760</td>\n",
       "      <td>0.726676</td>\n",
       "      <td>0.207888</td>\n",
       "      <td>0.719619</td>\n",
       "      <td>20.423726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.610952</td>\n",
       "      <td>0.475557</td>\n",
       "      <td>0.880631</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.134853</td>\n",
       "      <td>0.345883</td>\n",
       "      <td>0.659849</td>\n",
       "      <td>0.435708</td>\n",
       "      <td>0.051033</td>\n",
       "      <td>16.172549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.287632</td>\n",
       "      <td>0.150329</td>\n",
       "      <td>0.781215</td>\n",
       "      <td>0.571158</td>\n",
       "      <td>0.578206</td>\n",
       "      <td>0.126166</td>\n",
       "      <td>0.697782</td>\n",
       "      <td>0.367559</td>\n",
       "      <td>0.676292</td>\n",
       "      <td>16.571479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.555329</td>\n",
       "      <td>0.247484</td>\n",
       "      <td>0.776044</td>\n",
       "      <td>0.373284</td>\n",
       "      <td>0.840344</td>\n",
       "      <td>0.380229</td>\n",
       "      <td>0.958696</td>\n",
       "      <td>0.498656</td>\n",
       "      <td>0.223507</td>\n",
       "      <td>18.861082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.928780</td>\n",
       "      <td>0.693301</td>\n",
       "      <td>0.145977</td>\n",
       "      <td>0.212827</td>\n",
       "      <td>0.524590</td>\n",
       "      <td>0.026931</td>\n",
       "      <td>0.352008</td>\n",
       "      <td>0.391696</td>\n",
       "      <td>0.954128</td>\n",
       "      <td>15.441055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.194920</td>\n",
       "      <td>0.582053</td>\n",
       "      <td>0.691961</td>\n",
       "      <td>0.336600</td>\n",
       "      <td>0.038425</td>\n",
       "      <td>0.476979</td>\n",
       "      <td>0.483514</td>\n",
       "      <td>0.141998</td>\n",
       "      <td>0.501276</td>\n",
       "      <td>16.039267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.231129</td>\n",
       "      <td>0.198635</td>\n",
       "      <td>0.422906</td>\n",
       "      <td>0.407853</td>\n",
       "      <td>0.423211</td>\n",
       "      <td>0.035530</td>\n",
       "      <td>0.174594</td>\n",
       "      <td>0.068946</td>\n",
       "      <td>0.309196</td>\n",
       "      <td>9.752001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.276302</td>\n",
       "      <td>0.997535</td>\n",
       "      <td>0.905833</td>\n",
       "      <td>0.946538</td>\n",
       "      <td>0.180229</td>\n",
       "      <td>0.654463</td>\n",
       "      <td>0.229402</td>\n",
       "      <td>0.985819</td>\n",
       "      <td>0.303154</td>\n",
       "      <td>24.614303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.602090  0.812987  0.846952  0.857944  0.214467  0.141252  0.302948   \n",
       "1  0.624330  0.850129  0.332856  0.265681  0.334010  0.640477  0.829841   \n",
       "2  0.805075  0.324719  0.812155  0.780388  0.794551  0.120760  0.726676   \n",
       "3  0.610952  0.475557  0.880631  0.367347  0.134853  0.345883  0.659849   \n",
       "4  0.287632  0.150329  0.781215  0.571158  0.578206  0.126166  0.697782   \n",
       "5  0.555329  0.247484  0.776044  0.373284  0.840344  0.380229  0.958696   \n",
       "6  0.928780  0.693301  0.145977  0.212827  0.524590  0.026931  0.352008   \n",
       "7  0.194920  0.582053  0.691961  0.336600  0.038425  0.476979  0.483514   \n",
       "8  0.231129  0.198635  0.422906  0.407853  0.423211  0.035530  0.174594   \n",
       "9  0.276302  0.997535  0.905833  0.946538  0.180229  0.654463  0.229402   \n",
       "\n",
       "          7         8     y_pred  \n",
       "0  0.897758  0.797035  21.932302  \n",
       "1  0.048767  0.982292  20.486398  \n",
       "2  0.207888  0.719619  20.423726  \n",
       "3  0.435708  0.051033  16.172549  \n",
       "4  0.367559  0.676292  16.571479  \n",
       "5  0.498656  0.223507  18.861082  \n",
       "6  0.391696  0.954128  15.441055  \n",
       "7  0.141998  0.501276  16.039267  \n",
       "8  0.068946  0.309196   9.752001  \n",
       "9  0.985819  0.303154  24.614303  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(test_data)\n",
    "data['y_pred'] = test_result\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599453"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import make_scorer\n",
    "def my_custom_loss_func(ground_truth, predictions):\n",
    "    diff = np.abs(ground_truth - predictions).max()\n",
    "    return np.log(1 + diff)\n",
    "\n",
    "# loss_func will negate the return value of my_custom_loss_func,\n",
    "#  which will be np.log(2), 0.693, given the values for ground_truth\n",
    "#  and predictions defined below.\n",
    "loss  = make_scorer(my_custom_loss_func, greater_is_better=False)\n",
    "score = make_scorer(my_custom_loss_func, greater_is_better=True)\n",
    "ground_truth = [[1, 1],[2,1]]\n",
    "predictions  = [0, 1]\n",
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier(strategy='most_frequent', random_state=0)\n",
    "clf = clf.fit(ground_truth, predictions)\n",
    "loss(clf,ground_truth, predictions) \n",
    "\n",
    "score(clf,ground_truth, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['malignant' 'benign']\n",
      "Class label =  0\n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "[1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01\n",
      " 1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02\n",
      " 6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01\n",
      " 1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01\n",
      " 4.601e-01 1.189e-01]\n",
      "[1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0\n",
      " 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0\n",
      " 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0\n",
      " 1 1 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0\n",
      " 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0\n",
      " 0 1 1]\n",
      "0.9414893617021277\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Organize our data\n",
    "label_names = data['target_names']\n",
    "labels = data['target']\n",
    "feature_names = data['feature_names']\n",
    "features = data['data']\n",
    "\n",
    "# Look at our data\n",
    "print(label_names)\n",
    "print('Class label = ', labels[0])\n",
    "print(feature_names)\n",
    "print(features[0])\n",
    "\n",
    "# Split our data\n",
    "train, test, train_labels, test_labels = train_test_split(features,\n",
    "                                                          labels,\n",
    "                                                          test_size=0.33,\n",
    "                                                          random_state=42)\n",
    "\n",
    "# Initialize our classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train our classifier\n",
    "model = gnb.fit(train, train_labels)\n",
    "\n",
    "# Make predictions\n",
    "preds = gnb.predict(test)\n",
    "print(preds)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(accuracy_score(test_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove zeros from set...\n",
      "Remove zeros from set...\n",
      "[[1, 2], [0, 0], [2, 6, 7, 0]]\n",
      "[2 4 7]\n",
      "Idx to delete  [0, 2]\n",
      "[array([1, 2]), array([], dtype=int64), array([2, 6, 7])]\n",
      "[2 4 7]\n",
      "[True, False, True]\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "y_true = [[1,2],[0,0],[2,6,7,0]]\n",
    "y_pred = np.array([2,4,7])\n",
    "\n",
    "\n",
    "#print(y_pred[y_pred != 0])\n",
    "y_true_excluding_zeros = [np.array(v)[np.array(v)!=0] for v in y_true]\n",
    "\n",
    "y_idx = []\n",
    "for idx, v in enumerate(y_true):\n",
    "    if v.__contains__(2):\n",
    "        print(\"Remove zeros from set...\")\n",
    "        y_idx.append(idx)\n",
    "\n",
    "print(y_true)\n",
    "print(y_pred)\n",
    "print('Idx to delete ', y_idx)\n",
    "y_true = y_true_excluding_zeros\n",
    "\n",
    "#y_true = np.delete(y_true,y_idx,axis=0)\n",
    "#y_pred = np.delete(y_pred,y_idx)\n",
    "print(y_true)\n",
    "print(y_pred)\n",
    "matched_index = [t.__contains__(p) for (t,p) in zip(y_true, y_pred)]\n",
    "print(matched_index)\n",
    "print(sum(matched_index)/len(matched_index))\n",
    "\n",
    "\n",
    "#matched_values = [reduce(np.intersect1d, (p, a)) for (p,a) in zip(y_true, y_pred)]\n",
    "#print(matched_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.scorer import make_scorer\n",
    "def multi_targets_scorer_function(y_true, y_pred):\n",
    "    y_true_excluding_zeros = [np.array(v)[np.array(v)!=0] for v in y_true]\n",
    "    matched_index = [t.__contains__(p) for (t,p) in zip(y_true_excluding_zeros, y_pred)]\n",
    "    return sum(matched_index)/len(y_true_excluding_zeros)\n",
    "\n",
    "multi_targets_scorer = make_scorer(multi_targets_scorer_function, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cores:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n",
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  return _inspect.getargspec(target)\n",
      "/Users/uqapp/anaconda3/envs/mldss/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:4900: ResourceWarning: unclosed file <_io.TextIOWrapper name='/Users/uqapp/.keras/keras.json' mode='r' encoding='UTF-8'>\n",
      "  _config = json.load(open(_config_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#!conda install -n mldds -c anaconda joblib\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "import multiprocessing\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "print(\"Cores: \", num_cores)\n",
    "\n",
    "import time\n",
    "import keras\n",
    "# import tensorflow as tf\n",
    "# config = tf.ConfigProto( device_count = {'GPU': 0 , 'CPU': num_cores} )\n",
    "# sess = tf.Session(config=config) \n",
    "# keras.backend.set_session(sess)\n",
    "\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from MyTotoResearchv4 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install autograd\n",
    "#!conda install -c omnia autograd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-69db1a1f6c87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_matched\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_loss_given_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'grad' is not defined"
     ]
    }
   ],
   "source": [
    "def wTx(w, x):\n",
    "    return np.dot(x, w)\n",
    "\n",
    "def sigmoid_range(z,bottom,top):\n",
    "#    return 1./(1+np.exp(-z))\n",
    "    return bottom + (top - bottom) / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_range_inverse(y, bottom,top):\n",
    "    return np.log((y - bottom) / (top - y))\n",
    "\n",
    "def custom_predictions(w, x):\n",
    "    predictions = sigmoid_range(wTx(w, x),1,49)\n",
    "    return predictions\n",
    "#     global i\n",
    "#     if ( i < 10 ):\n",
    "#         print(X)\n",
    "#     print(predictions)\n",
    "#     return predictions.clip(eps, 1-eps)\n",
    "\n",
    "def custom_loss(y, y_predicted):\n",
    "#    return -(y*np.log(y_predicted) - (1-y)*np.log(1-y_predicted)**2).mean()\n",
    "\n",
    "    return -(y*np.log(y_predicted) - (1-y)*np.log(1-y_predicted)**2).mean()\n",
    "\n",
    "i = 0\n",
    "def custom_loss_given_weights(w):\n",
    "#     global i\n",
    "#     if ( i < 10 ):\n",
    "#         i = i + 1\n",
    "#         print(X)\n",
    "    y_predicted = custom_predictions(w, X)\n",
    "    y_matched = y_predicted[np.abs(y_predicted-y).argmin()]\n",
    "    return custom_loss(y_matched, y_predicted)\n",
    "    \n",
    "gradient = grad(custom_loss_given_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from autograd import grad\n",
    "import autograd.numpy as np\n",
    "\n",
    "\n",
    "def getAllData(df):\n",
    "    drop_cols = ['T', 'D', 'N1','N2','N3','N4','N5','N6','N7','L','M','S','R','E','A','V' ,'J','U']\n",
    "    X = df.drop(drop_cols, axis=1)\n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wTx(w, x):\n",
    "    return np.dot(x, w)\n",
    "\n",
    "def sigmoid_range(z,bottom,top):\n",
    "#    return 1./(1+np.exp(-z))\n",
    "    return bottom + (top - bottom) / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_range_inverse(y, bottom,top):\n",
    "    return np.log((y - bottom) / (top - y))\n",
    "\n",
    "def custom_predictions(w, x):\n",
    "    predictions = sigmoid_range(wTx(w, x),1,49)\n",
    "    return predictions\n",
    "#     global i\n",
    "#     if ( i < 10 ):\n",
    "#         print(X)\n",
    "#     print(predictions)\n",
    "#     return predictions.clip(eps, 1-eps)\n",
    "\n",
    "def custom_loss(y, y_predicted):\n",
    "    return -(y*np.log(y_predicted) - (1-y)*np.log(1-y_predicted)**2).mean()\n",
    "\n",
    "i = 0\n",
    "def custom_loss_given_weights(w):\n",
    "#     global i\n",
    "#     if ( i < 10 ):\n",
    "#         i = i + 1\n",
    "#         print(X)\n",
    "    y_predicted = custom_predictions(w, X)\n",
    "    y_matched = y_predicted[np.abs(y_predicted-y).argmin()]\n",
    "    return custom_loss(y_matched, y_predicted)\n",
    "    \n",
    "gradient = grad(custom_loss_given_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [ 0.3213,  0.4856,  0.2995,  2.5044],\n",
    "    [ 0.3005,  0.4757,  0.2974,  2.4691],\n",
    "    [ 0.5638,  0.8005,  0.3381,  2.3102],\n",
    "    [ 0.5281,  0.6542,  0.3129,  2.1298],\n",
    "    [ 0.3221,  0.5126,  0.3085,  2.6147],\n",
    "    [ 0.3055,  0.4885,  0.289 ,  2.4957],\n",
    "    [ 0.3276,  0.5185,  0.3218,  2.6013],\n",
    "    [ 0.5313,  0.7028,  0.3266,  2.1543],\n",
    "    [ 0.4728,  0.6399,  0.3062,  2.0597],\n",
    "    [ 0.3221,  0.5126,  0.3085,  2.6147]\n",
    "])\n",
    "y = np.array([1., 1., 0., 0., 1., 1., 1., 1., 0., 0.])\n",
    "\n",
    "weights = np.zeros(X.shape[1])\n",
    "eps = 1e-15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [(y_hat, custom_loss(False, y_hat)) for y_hat in np.linspace(0, 1, 101)],\n",
    "    columns=['y_hat', 'loss']\n",
    ").plot(x='y_hat', title='y_hat vs. Loss for y=0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [(y_hat, custom_loss(True, y_hat)) for y_hat in np.linspace(0, 1, 101)],\n",
    "    columns=['y_hat', 'loss']\n",
    ").plot(x='y_hat', title='y_hat vs. Loss for y=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    if i % 100 == 0:\n",
    "        print('Iteration %-4d | Loss: %.4f' % (i, custom_loss_given_weights(weights)))\n",
    "    weights -= gradient(weights) * .05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_prediction(mrt, model, f, scaler=None, name='unnamed'):\n",
    "    def getAllData(df):\n",
    "        drop_cols = ['T', 'L','M','S','R','E','A','V' ,'J','U']\n",
    "        X = df.drop(drop_cols, axis=1)\n",
    "#        print(df.head())\n",
    "        use_cols = ['Ph','il','age','dist','adia','sundist','sunadia']\n",
    "        X = df[use_cols]\n",
    "        return X\n",
    "\n",
    "    test_data = mtr.get_test_data()\n",
    "    X = mtr.modified_dataset(getAllData(test_data)) #\n",
    "#    X = getAdjustedDataF(test_data,f)\n",
    "\n",
    "\n",
    "    if ( scaler == None ):\n",
    "        Z = X\n",
    "    else:\n",
    "        scaler.fit(X)\n",
    "        Z = scaler.transform(X)\n",
    "\n",
    "    predictions = model.predict(Z)\n",
    "\n",
    "    dfResult= pd.DataFrame(predictions, columns=['N1', 'N2', 'N3', 'N4', 'N5','N6', 'N7'])\n",
    "#    mtr.print_predictions(dfResult)\n",
    "\n",
    "    global df_predictions\n",
    "    global prev_r\n",
    "    r = mtr.getAccuracyCount(np.array(dfResult)) ;\n",
    "#    if ( r > prev_r ):\n",
    "#        df_predictions = []\n",
    "    df_predictions.append(dfResult)\n",
    "    g_all_pred.update({name : dfResult})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from keras.models import Input, Model\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, History\n",
    "import json as simplejson\n",
    "from keras import regularizers\n",
    "from sklearn import preprocessing\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, RandomForestClassifier, ExtraTreesRegressor, ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor, SGDClassifier, LogisticRegression, PassiveAggressiveClassifier, Perceptron, RidgeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression, Lasso, ElasticNet, Ridge, RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, SVR, LinearSVC\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "seed = 42\n",
    "\n",
    "mtr = MyTotoResearch(algo_no=1)\n",
    "lresult, df = mtr.load_totodata()\n",
    "\n",
    "df_predictions = []\n",
    "\n",
    "\n",
    "all_models = []\n",
    "\n",
    "#all_models.append(('SVCpoly01', SVC(kernel='poly', coef0=0.05, probability=True, degree=2, random_state=seed)))\n",
    "#all_models.append(('SVCrbf010', SVC(kernel='rbf', coef0=0.75, probability=True, degree=2, random_state=seed)))\n",
    "# all_models.append(('SVCrbf011', SVC(kernel='rbf', coef0=0.5, probability=True, degree=2, random_state=seed)))\n",
    "# all_models.append(('SVCrbf012', SVC(kernel='rbf', coef0=0.25, probability=True, degree=2, random_state=seed)))\n",
    "\n",
    "# all_models.append(('SVCrbf0103', SVC(kernel='rbf', coef0=0.75, probability=True, degree=3, random_state=seed)))\n",
    "# all_models.append(('SVCrbf0113', SVC(kernel='rbf', coef0=0.5, probability=True, degree=3, random_state=seed)))\n",
    "# all_models.append(('SVCrbf0123', SVC(kernel='rbf', coef0=0.25, probability=True, degree=3, random_state=seed)))\n",
    "\n",
    "\n",
    "#all_models.append(('SVCrbf020', SVC(kernel='sigmoid', coef0=0.75, probability=True, degree=2, random_state=seed)))\n",
    "# all_models.append(('SVCrbf021', SVC(kernel='sigmoid', coef0=0.5, probability=True, degree=2, random_state=seed)))\n",
    "# all_models.append(('SVCrbf022', SVC(kernel='sigmoid', coef0=0.25, probability=True, degree=2, random_state=seed)))\n",
    "\n",
    "# all_models.append(('SVCrbf0203', SVC(kernel='sigmoid', coef0=0.75, probability=True, degree=3, random_state=seed)))\n",
    "# all_models.append(('SVCrbf0213', SVC(kernel='sigmoid', coef0=0.5, probability=True, degree=3, random_state=seed)))\n",
    "# all_models.append(('SVCrbf0223', SVC(kernel='sigmoid', coef0=0.25, probability=True, degree=3, random_state=seed)))\n",
    "\n",
    "\n",
    "# all_models.append(('SVCrbf030', SVC(kernel='linear', coef0=0.75, probability=True, degree=2, random_state=seed)))\n",
    "# all_models.append(('SVCrbf031', SVC(kernel='linear', coef0=0.5, probability=True, degree=2, random_state=seed)))\n",
    "# all_models.append(('SVCrbf032', SVC(kernel='linear', coef0=0.25, probability=True, degree=2, random_state=seed)))\n",
    "\n",
    "# all_models.append(('SVCrbf0303', SVC(kernel='linear', coef0=0.75, probability=True, degree=3, random_state=seed)))\n",
    "# all_models.append(('SVCrbf0313', SVC(kernel='linear', coef0=0.5, probability=True, degree=3, random_state=seed)))\n",
    "# all_models.append(('SVCrbf0323', SVC(kernel='linear', coef0=0.25, probability=True, degree=3, random_state=seed)))\n",
    "\n",
    "\n",
    "\n",
    "# all_models.append(('LR', (LogisticRegression(random_state=seed))))\n",
    "\n",
    "#all_models.append(('KNNC', KNeighborsClassifier()))\n",
    "#all_models.append(('KNNR', KNeighborsRegressor()))\n",
    "#all_models.append(('RC', RidgeClassifier(random_state=seed)))\n",
    "# all_models.append(('LR', LogisticRegression(random_state=seed)))\n",
    "# all_models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "# all_models.append(('DTR', DecisionTreeRegressor()))\n",
    "# all_models.append(('ETR', ExtraTreesRegressor(n_estimators=5)))\n",
    "#all_models.append(('ETC', ExtraTreesClassifier(n_estimators=5)))\n",
    "# all_models.append(('EN', ElasticNet()))\n",
    "#all_models.append(('CART', DecisionTreeClassifier()))\n",
    "# all_models.append(('NB', GaussianNB()))\n",
    "# all_models.append(('Lasso', Lasso()))\n",
    "all_models.append(('GBR', GradientBoostingRegressor()))\n",
    "#all_models.append(('RFR5', RandomForestClassifier(n_estimators=5, n_jobs=5, random_state=seed)))\n",
    "# all_models.append(('RFR5', RandomForestClassifier(n_estimators=5, n_jobs=5, random_state=seed)))\n",
    "# all_models.append(('RFR3', RandomForestRegressor(n_estimators=3, n_jobs=5, random_state=seed)))\n",
    "# all_models.append(('SGDR', SGDRegressor(random_state=seed)))\n",
    "#all_models.append(('AdaB', AdaBoostClassifier(RandomForestClassifier(n_estimators=3))))\n",
    "#all_models.append(('MLPC', MLPClassifier(hidden_layer_sizes=(500,500,500), max_iter=2000, alpha=0.001, activation='tanh', learning_rate='adaptive', solver='sgd', verbose=0,  random_state=42,tol=0.000000001)))\n",
    "\n",
    "#92.45 accuracy\n",
    "#all_models.append(('MLPC', MLPClassifier(hidden_layer_sizes=(490,490,490,490,490,490,490), max_iter=500000, alpha=0.001, activation='relu', learning_rate='adaptive', solver='adam', verbose=10,  random_state=42,tol=0.000000001)))\n",
    "\n",
    "\n",
    "all_models.append(('MLPC', MLPClassifier(hidden_layer_sizes=(780,490,780,490,780,490,280), max_iter=500000, alpha=0.001, activation='relu', learning_rate='adaptive', solver='adam', verbose=10,  random_state=42,tol=0.000000001)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each model in turn\n",
    "from sklearn import model_selection\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "\n",
    "g_all_pred = {}\n",
    "\n",
    "X = mtr.modified_dataset(getAllData(df)) #\n",
    "f = 1.0 #365/27.58\n",
    "#    X = getAdjustedDataF(df,f)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X)\n",
    "Z = scaler.transform(X)\n",
    "\n",
    "for name, model in all_models:\n",
    "    \n",
    "    \n",
    "#    scaler = None\n",
    "#    Z = X\n",
    "\n",
    "#     kfold = model_selection.KFold(n_splits=3, random_state=seed)\n",
    "#     cv_results = model_selection.cross_val_score(model, Z, mtr.getTarget(3), cv=kfold, scoring=scoring)\n",
    "#     results.append(cv_results)\n",
    "#     names.append(name)\n",
    "#     msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "#     print(msg)\n",
    "    \n",
    "    oClassifier = MultiOutputClassifier(model, n_jobs=7)\n",
    "    oClassifier.fit(Z, mtr.getTargets()) \n",
    "    print(oClassifier)\n",
    "    s = oClassifier.score(Z, mtr.getTargets())\n",
    "    if(oClassifier.score(Z, mtr.getTargets()) == 1.0):\n",
    "        print( name, ' ', str(f), ' ', str(s))\n",
    "    store_prediction(mtr, oClassifier, f, scaler=scaler, name=name)\n",
    "    start = time.clock()\n",
    "    print(str(f), \" Time taken: \", (time.clock() - start),  \" \")\n",
    "\n",
    "# for n in range(len(df_predictions)):\n",
    "#     print( mtr.getAccuracyCount(np.array(df_predictions[n])))\n",
    "#     mtr.print_predictions(df_predictions[n])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# boxplot algorithm comparison\n",
    "# fig = plt.figure()\n",
    "# fig.suptitle('Algorithm Comparison')\n",
    "# ax = fig.add_subplot(111)\n",
    "# plt.boxplot(results)\n",
    "# ax.set_xticklabels(names)\n",
    "# plt.show()\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_prediction(arr, initial_pred=[]):\n",
    "    global s\n",
    "    if ( isinstance(arr, list) ):\n",
    "        for a in arr:\n",
    "            combine_prediction(a, initial_pred)\n",
    "        return \n",
    "    if ( len(s) > 1 ):\n",
    "        s += '_'\n",
    "    s += arr\n",
    "    initial_pred.append(g_all_pred[arr])\n",
    "    return \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from itertools import combinations\n",
    "import operator \n",
    "from itertools import islice\n",
    "\n",
    "name_ = []\n",
    "\n",
    "lst = [name for name, model in all_models]\n",
    "iBestIndex = -1\n",
    "iBestN = []\n",
    "#print(\"List \", lst)\n",
    "top_n = 12\n",
    "\n",
    "\n",
    "dict_accuracy = {}\n",
    "for z in range(5, 0,-1):\n",
    "    a = [list(x) for x in itertools.combinations(lst, z) if len(x) > 1 ] \n",
    "#    print(a)\n",
    "\n",
    "    for xx in a:\n",
    "        test_pred = []\n",
    "        s = ''\n",
    "        combine_prediction(xx, test_pred)\n",
    "#        print(s)\n",
    "\n",
    "        #print(len(test_pred))\n",
    "\n",
    "        all_pred = [] ;\n",
    "        for i in range(len(test_pred)):\n",
    "            if ( i == 0 ):\n",
    "                all_pred = test_pred[i]\n",
    "            else:\n",
    "                all_pred = np.column_stack((all_pred, test_pred[i]) )\n",
    "\n",
    "        top_seven = []\n",
    "        for i in range(len(all_pred)):\n",
    "            unique, counts = np.unique(all_pred[i], return_counts=True)\n",
    "            x = dict(zip(unique, counts))\n",
    "            sorted_x = sorted(x.items(), key=operator.itemgetter(1), reverse=True) # sorted by value\n",
    "            l = list(islice([int(x) for x,y in sorted_x],top_n))\n",
    "            while ( len(l) < top_n ):\n",
    "                l.append(-1)\n",
    "\n",
    "            top_seven.append(l)\n",
    "            \n",
    "\n",
    "#        print(len(top_seven))\n",
    "#         if(len(top_seven[0]) < top_n ):\n",
    "#             print(\"*** Caught \", )\n",
    "        columns = ['N'+str(i+1) for i in range(len(top_seven[0]))]\n",
    "#        print(columns)\n",
    "        df_top_seven = pd.DataFrame(top_seven, columns=columns)\n",
    "        r = mtr.getAccuracyCount(np.array(df_top_seven)) ;\n",
    "        matched, weighted_match = mtr.print_weighted_numbers(df_top_seven.values)\n",
    "        r = sum(weighted_match)\n",
    "\n",
    "        dict_accuracy.update({s: r})\n",
    "\n",
    "t_accuracy = sorted(dict_accuracy.items(),key=operator.itemgetter(1), reverse=True)\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched, weighted_match = mtr.print_weighted_numbers(df_top_seven.values)\n",
    "print(matched)\n",
    "print(weighted_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 7\n",
    "print(t_accuracy[:n])\n",
    "\n",
    "a = [x[0].split('_') for x in t_accuracy[:n] ] \n",
    "print(a)\n",
    "for xx in a:\n",
    "    test_pred = []\n",
    "    s = ''\n",
    "    combine_prediction(xx, test_pred)\n",
    "    all_pred = [] ;\n",
    "    for i in range(len(test_pred)):\n",
    "        if ( i == 0 ):\n",
    "            all_pred = test_pred[i]\n",
    "        else:\n",
    "            all_pred = np.column_stack((all_pred, test_pred[i]) )\n",
    "\n",
    "    top_seven = []\n",
    "    for i in range(len(all_pred)):\n",
    "        unique, counts = np.unique(all_pred[i], return_counts=True)\n",
    "        x = dict(zip(unique, counts))\n",
    "        sorted_x = sorted(x.items(), key=operator.itemgetter(1), reverse=True) # sorted by value\n",
    "        l = list(islice([int(x) for x,y in sorted_x],top_n))\n",
    "        while ( len(l) < top_n ):\n",
    "          l.append(-1)\n",
    "        top_seven.append(l)\n",
    "\n",
    "\n",
    "    columns = ['N'+str(i+1) for i in range(len(top_seven[0]))]\n",
    "    df_top_seven = pd.DataFrame(top_seven, columns=columns)\n",
    "    r = mtr.getAccuracyCount(np.array(df_top_seven)) ;\n",
    "    print ( \"Accuracy: \",  r)\n",
    "    dict_accuracy.update({s: r})\n",
    "    mtr.plot_matched_counts(df_top_seven.values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Nov 26\n",
    "# 16 22 28 31 38 46 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep track of all results\n",
    "#df_predictions = []\n",
    "\n",
    "#print(df_predictions)\n",
    "#mtr = MyTotoResearch(algo_no=1)\n",
    "def getAllData(df):\n",
    "#     drop_cols = ['T', 'L','M','S','R','E','A','V' ,'J','U','K']\n",
    "#     X = df.drop(drop_cols, axis=1)\n",
    "\n",
    "    use_cols = ['Ph','il','age','dist','adia','sundist','sunadia']\n",
    "    X = df[use_cols]\n",
    "    return X\n",
    "\n",
    "lresult, df = mtr.load_totodata()\n",
    "\n",
    "test_data = mtr.get_test_data()\n",
    "X = mtr.modified_dataset(getAllData(test_data)) #\n",
    "\n",
    "print(len(df_predictions))\n",
    "for n in range(len(df_predictions)):\n",
    "    print( mtr.getAccuracyCount(np.array(df_predictions[n])))\n",
    "    mtr.print_predictions(df_predictions[n])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
